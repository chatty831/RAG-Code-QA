{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np \n",
    "import re\n",
    "import pandas as pd \n",
    "import json \n",
    "import pickle\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://huggingface.co/docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://huggingface.co/docs/hub/index',\n",
       " 'https://huggingface.co/docs/hub/repositories',\n",
       " 'https://huggingface.co/docs/hub/repositories-getting-started',\n",
       " 'https://huggingface.co/docs/hub/repositories-settings',\n",
       " 'https://huggingface.co/docs/hub/repositories-pull-requests-discussions',\n",
       " 'https://huggingface.co/docs/hub/notifications',\n",
       " 'https://huggingface.co/docs/hub/collections',\n",
       " 'https://huggingface.co/docs/hub/webhooks',\n",
       " 'https://huggingface.co/docs/hub/repositories-recommendations',\n",
       " 'https://huggingface.co/docs/hub/repositories-next-steps',\n",
       " 'https://huggingface.co/docs/hub/repositories-licenses',\n",
       " 'https://huggingface.co/docs/hub/models',\n",
       " 'https://huggingface.co/docs/hub/models-the-hub',\n",
       " 'https://huggingface.co/docs/hub/model-cards',\n",
       " 'https://huggingface.co/docs/hub/models-gated',\n",
       " 'https://huggingface.co/docs/hub/models-uploading',\n",
       " 'https://huggingface.co/docs/hub/models-downloading',\n",
       " 'https://huggingface.co/docs/hub/models-libraries',\n",
       " 'https://huggingface.co/docs/hub/models-widgets',\n",
       " 'https://huggingface.co/docs/hub/models-inference',\n",
       " 'https://huggingface.co/docs/hub/models-download-stats',\n",
       " 'https://huggingface.co/docs/hub/models-faq',\n",
       " 'https://huggingface.co/docs/hub/models-advanced',\n",
       " 'https://huggingface.co/docs/hub/datasets',\n",
       " 'https://huggingface.co/docs/hub/datasets-overview',\n",
       " 'https://huggingface.co/docs/hub/datasets-cards',\n",
       " 'https://huggingface.co/docs/hub/datasets-gated',\n",
       " 'https://huggingface.co/docs/hub/datasets-adding',\n",
       " 'https://huggingface.co/docs/hub/datasets-downloading',\n",
       " 'https://huggingface.co/docs/hub/datasets-libraries',\n",
       " 'https://huggingface.co/docs/hub/datasets-viewer',\n",
       " 'https://huggingface.co/docs/hub/datasets-download-stats',\n",
       " 'https://huggingface.co/docs/hub/datasets-data-files-configuration',\n",
       " 'https://huggingface.co/docs/hub/spaces',\n",
       " 'https://huggingface.co/docs/hub/spaces-overview',\n",
       " 'https://huggingface.co/docs/hub/spaces-gpus',\n",
       " 'https://huggingface.co/docs/hub/spaces-storage',\n",
       " 'https://huggingface.co/docs/hub/spaces-sdks-gradio',\n",
       " 'https://huggingface.co/docs/hub/spaces-sdks-streamlit',\n",
       " 'https://huggingface.co/docs/hub/spaces-sdks-static',\n",
       " 'https://huggingface.co/docs/hub/spaces-sdks-docker',\n",
       " 'https://huggingface.co/docs/hub/spaces-embed',\n",
       " 'https://huggingface.co/docs/hub/spaces-run-with-docker',\n",
       " 'https://huggingface.co/docs/hub/spaces-config-reference',\n",
       " 'https://huggingface.co/docs/hub/spaces-oauth',\n",
       " 'https://huggingface.co/docs/hub/spaces-changelog',\n",
       " 'https://huggingface.co/docs/hub/spaces-advanced',\n",
       " 'https://huggingface.co/docs/hub/other',\n",
       " 'https://huggingface.co/docs/hub/organizations',\n",
       " 'https://huggingface.co/docs/hub/billing',\n",
       " 'https://huggingface.co/docs/hub/security',\n",
       " 'https://huggingface.co/docs/hub/moderation',\n",
       " 'https://huggingface.co/docs/hub/paper-pages',\n",
       " 'https://huggingface.co/docs/hub/search',\n",
       " 'https://huggingface.co/docs/hub/doi',\n",
       " 'https://huggingface.co/docs/hub/api',\n",
       " 'https://huggingface.co/docs/hub/oauth',\n",
       " 'https://huggingface.co/docs/transformers/index',\n",
       " 'https://huggingface.co/docs/transformers/quicktour',\n",
       " 'https://huggingface.co/docs/transformers/installation',\n",
       " 'https://huggingface.co/docs/transformers/pipeline_tutorial',\n",
       " 'https://huggingface.co/docs/transformers/autoclass_tutorial',\n",
       " 'https://huggingface.co/docs/transformers/preprocessing',\n",
       " 'https://huggingface.co/docs/transformers/training',\n",
       " 'https://huggingface.co/docs/transformers/run_scripts',\n",
       " 'https://huggingface.co/docs/transformers/accelerate',\n",
       " 'https://huggingface.co/docs/transformers/peft',\n",
       " 'https://huggingface.co/docs/transformers/model_sharing',\n",
       " 'https://huggingface.co/docs/transformers/transformers_agents',\n",
       " 'https://huggingface.co/docs/transformers/llm_tutorial',\n",
       " 'https://huggingface.co/docs/transformers/fast_tokenizers',\n",
       " 'https://huggingface.co/docs/transformers/multilingual',\n",
       " 'https://huggingface.co/docs/transformers/create_a_model',\n",
       " 'https://huggingface.co/docs/transformers/custom_models',\n",
       " 'https://huggingface.co/docs/transformers/chat_templating',\n",
       " 'https://huggingface.co/docs/transformers/trainer',\n",
       " 'https://huggingface.co/docs/transformers/sagemaker',\n",
       " 'https://huggingface.co/docs/transformers/serialization',\n",
       " 'https://huggingface.co/docs/transformers/tflite',\n",
       " 'https://huggingface.co/docs/transformers/torchscript',\n",
       " 'https://huggingface.co/docs/transformers/benchmarks',\n",
       " 'https://huggingface.co/docs/transformers/notebooks',\n",
       " 'https://huggingface.co/docs/transformers/community',\n",
       " 'https://huggingface.co/docs/transformers/custom_tools',\n",
       " 'https://huggingface.co/docs/transformers/troubleshooting',\n",
       " 'https://huggingface.co/docs/transformers/performance',\n",
       " 'https://huggingface.co/docs/transformers/quantization',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_gpu_one',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_gpu_many',\n",
       " 'https://huggingface.co/docs/transformers/fsdp',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_cpu',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_cpu_many',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_tpu_tf',\n",
       " 'https://huggingface.co/docs/transformers/perf_train_special',\n",
       " 'https://huggingface.co/docs/transformers/perf_hardware',\n",
       " 'https://huggingface.co/docs/transformers/hpo_train',\n",
       " 'https://huggingface.co/docs/transformers/perf_infer_cpu',\n",
       " 'https://huggingface.co/docs/transformers/perf_infer_gpu_one',\n",
       " 'https://huggingface.co/docs/transformers/big_models',\n",
       " 'https://huggingface.co/docs/transformers/debugging',\n",
       " 'https://huggingface.co/docs/transformers/tf_xla',\n",
       " 'https://huggingface.co/docs/transformers/perf_torch_compile',\n",
       " 'https://huggingface.co/docs/transformers/contributing',\n",
       " 'https://huggingface.co/docs/transformers/add_new_model',\n",
       " 'https://huggingface.co/docs/transformers/add_tensorflow_model',\n",
       " 'https://huggingface.co/docs/transformers/add_new_pipeline',\n",
       " 'https://huggingface.co/docs/transformers/testing',\n",
       " 'https://huggingface.co/docs/transformers/pr_checks',\n",
       " 'https://huggingface.co/docs/transformers/philosophy',\n",
       " 'https://huggingface.co/docs/transformers/glossary',\n",
       " 'https://huggingface.co/docs/transformers/task_summary',\n",
       " 'https://huggingface.co/docs/transformers/tasks_explained',\n",
       " 'https://huggingface.co/docs/transformers/model_summary',\n",
       " 'https://huggingface.co/docs/transformers/tokenizer_summary',\n",
       " 'https://huggingface.co/docs/transformers/attention',\n",
       " 'https://huggingface.co/docs/transformers/pad_truncation',\n",
       " 'https://huggingface.co/docs/transformers/bertology',\n",
       " 'https://huggingface.co/docs/transformers/perplexity',\n",
       " 'https://huggingface.co/docs/transformers/pipeline_webserver',\n",
       " 'https://huggingface.co/docs/transformers/model_memory_anatomy',\n",
       " 'https://huggingface.co/docs/transformers/llm_tutorial_optimization',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/agent',\n",
       " 'https://huggingface.co/docs/transformers/model_doc/auto',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/backbones',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/callback',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/configuration',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/data_collator',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/keras_callbacks',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/logging',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/model',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/text_generation',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/onnx',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/optimizer_schedules',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/output',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/pipelines',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/processors',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/quantization',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/tokenizer',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/trainer',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/deepspeed',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/feature_extractor',\n",
       " 'https://huggingface.co/docs/transformers/main_classes/image_processor',\n",
       " 'https://huggingface.co/docs/transformers/internal/modeling_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/pipelines_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/tokenization_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/trainer_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/generation_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/image_processing_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/audio_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/file_utils',\n",
       " 'https://huggingface.co/docs/transformers/internal/time_series_utils',\n",
       " 'https://huggingface.co/docs/diffusers/index',\n",
       " 'https://huggingface.co/docs/diffusers/quicktour',\n",
       " 'https://huggingface.co/docs/diffusers/stable_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/installation',\n",
       " 'https://huggingface.co/docs/diffusers/tutorials/tutorial_overview',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline',\n",
       " 'https://huggingface.co/docs/diffusers/tutorials/autopipeline',\n",
       " 'https://huggingface.co/docs/diffusers/tutorials/basic_training',\n",
       " 'https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference',\n",
       " 'https://huggingface.co/docs/diffusers/tutorials/fast_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/loading_overview',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/loading',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/schedulers',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/other-formats',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/push_to_hub',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/unconditional_image_generation',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/img2img',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/inpaint',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/depth2img',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/textual_inversion_inference',\n",
       " 'https://huggingface.co/docs/diffusers/training/distributed_inference',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/reusing_seeds',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/control_brightness',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/weighted_prompts',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/freeu',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/sdxl',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/kandinsky',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/controlnet',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/shap-e',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/diffedit',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/distilled_sd',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/callback',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/reproducibility',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_examples',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/contribute_pipeline',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/svd',\n",
       " 'https://huggingface.co/docs/diffusers/training/overview',\n",
       " 'https://huggingface.co/docs/diffusers/training/create_dataset',\n",
       " 'https://huggingface.co/docs/diffusers/training/adapt_a_model',\n",
       " 'https://huggingface.co/docs/diffusers/training/unconditional_training',\n",
       " 'https://huggingface.co/docs/diffusers/training/text2image',\n",
       " 'https://huggingface.co/docs/diffusers/training/sdxl',\n",
       " 'https://huggingface.co/docs/diffusers/training/kandinsky',\n",
       " 'https://huggingface.co/docs/diffusers/training/wuerstchen',\n",
       " 'https://huggingface.co/docs/diffusers/training/controlnet',\n",
       " 'https://huggingface.co/docs/diffusers/training/t2i_adapters',\n",
       " 'https://huggingface.co/docs/diffusers/training/instructpix2pix',\n",
       " 'https://huggingface.co/docs/diffusers/training/text_inversion',\n",
       " 'https://huggingface.co/docs/diffusers/training/dreambooth',\n",
       " 'https://huggingface.co/docs/diffusers/training/lora',\n",
       " 'https://huggingface.co/docs/diffusers/training/custom_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/training/lcm_distill',\n",
       " 'https://huggingface.co/docs/diffusers/training/ddpo',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/other-modalities',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/opt_overview',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/fp16',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/memory',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/torch2.0',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/xformers',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/tome',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/deepcache',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/onnx',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/open_vino',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/coreml',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/mps',\n",
       " 'https://huggingface.co/docs/diffusers/optimization/habana',\n",
       " 'https://huggingface.co/docs/diffusers/conceptual/philosophy',\n",
       " 'https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation',\n",
       " 'https://huggingface.co/docs/diffusers/conceptual/contribution',\n",
       " 'https://huggingface.co/docs/diffusers/conceptual/ethical_guidelines',\n",
       " 'https://huggingface.co/docs/diffusers/conceptual/evaluation',\n",
       " 'https://huggingface.co/docs/diffusers/api/configuration',\n",
       " 'https://huggingface.co/docs/diffusers/api/logging',\n",
       " 'https://huggingface.co/docs/diffusers/api/outputs',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/ip_adapter',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/lora',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/single_file',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/textual_inversion',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/unet',\n",
       " 'https://huggingface.co/docs/diffusers/api/loaders/peft',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/overview',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/unet',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/unet2d',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/unet2d-cond',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/unet3d-cond',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/unet-motion',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/uvit2d',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/vq',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/autoencoderkl',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/autoencoder_tiny',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/consistency_decoder_vae',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/transformer2d',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/transformer_temporal',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/prior_transformer',\n",
       " 'https://huggingface.co/docs/diffusers/api/models/controlnet',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/overview',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/amused',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/animatediff',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/attend_and_excite',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/audioldm',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/audioldm2',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/auto_pipeline',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/consistency_models',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/controlnet',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/dance_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/ddim',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/ddpm',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/diffedit',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/dit',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/pix2pix',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/kandinsky',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/panorama',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/musicldm',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/paint_by_example',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/pia',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/pixart',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/self_attention_guidance',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/shap_e',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_safe',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/sdxl_turbo',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/k_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/ldm3d_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/gligen',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/text_to_video',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/unclip',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling',\n",
       " 'https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/overview',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/cm_stochastic_iterative',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/consistency_decoder',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/ddim_inverse',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/ddim',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/ddpm',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/deis',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver_inverse',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/dpm_sde',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/singlestep_dpm_solver',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/euler_ancestral',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/euler',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/heun',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/ipndm',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/stochastic_karras_ve',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete_ancestral',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/lcm',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/lms_discrete',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/pndm',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/repaint',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/score_sde_ve',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/score_sde_vp',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/unipc',\n",
       " 'https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion',\n",
       " 'https://huggingface.co/docs/diffusers/api/internal_classes_overview',\n",
       " 'https://huggingface.co/docs/diffusers/api/attnprocessor',\n",
       " 'https://huggingface.co/docs/diffusers/api/activations',\n",
       " 'https://huggingface.co/docs/diffusers/api/normalization',\n",
       " 'https://huggingface.co/docs/diffusers/api/utilities',\n",
       " 'https://huggingface.co/docs/diffusers/api/image_processor',\n",
       " 'https://huggingface.co/docs/datasets/index',\n",
       " 'https://huggingface.co/docs/datasets/quickstart',\n",
       " 'https://huggingface.co/docs/datasets/installation',\n",
       " 'https://huggingface.co/docs/datasets/tutorial',\n",
       " 'https://huggingface.co/docs/datasets/load_hub',\n",
       " 'https://huggingface.co/docs/datasets/access',\n",
       " 'https://huggingface.co/docs/datasets/use_dataset',\n",
       " 'https://huggingface.co/docs/datasets/metrics',\n",
       " 'https://huggingface.co/docs/datasets/create_dataset',\n",
       " 'https://huggingface.co/docs/datasets/upload_dataset',\n",
       " 'https://huggingface.co/docs/datasets/how_to',\n",
       " 'https://huggingface.co/docs/datasets/loading',\n",
       " 'https://huggingface.co/docs/datasets/process',\n",
       " 'https://huggingface.co/docs/datasets/stream',\n",
       " 'https://huggingface.co/docs/datasets/use_with_tensorflow',\n",
       " 'https://huggingface.co/docs/datasets/use_with_pytorch',\n",
       " 'https://huggingface.co/docs/datasets/use_with_jax',\n",
       " 'https://huggingface.co/docs/datasets/use_with_spark',\n",
       " 'https://huggingface.co/docs/datasets/cache',\n",
       " 'https://huggingface.co/docs/datasets/filesystems',\n",
       " 'https://huggingface.co/docs/datasets/faiss_es',\n",
       " 'https://huggingface.co/docs/datasets/how_to_metrics',\n",
       " 'https://huggingface.co/docs/datasets/beam',\n",
       " 'https://huggingface.co/docs/datasets/troubleshoot',\n",
       " 'https://huggingface.co/docs/datasets/audio_load',\n",
       " 'https://huggingface.co/docs/datasets/audio_process',\n",
       " 'https://huggingface.co/docs/datasets/audio_dataset',\n",
       " 'https://huggingface.co/docs/datasets/image_load',\n",
       " 'https://huggingface.co/docs/datasets/image_process',\n",
       " 'https://huggingface.co/docs/datasets/image_dataset',\n",
       " 'https://huggingface.co/docs/datasets/depth_estimation',\n",
       " 'https://huggingface.co/docs/datasets/image_classification',\n",
       " 'https://huggingface.co/docs/datasets/semantic_segmentation',\n",
       " 'https://huggingface.co/docs/datasets/object_detection',\n",
       " 'https://huggingface.co/docs/datasets/nlp_load',\n",
       " 'https://huggingface.co/docs/datasets/nlp_process',\n",
       " 'https://huggingface.co/docs/datasets/tabular_load',\n",
       " 'https://huggingface.co/docs/datasets/share',\n",
       " 'https://huggingface.co/docs/datasets/dataset_card',\n",
       " 'https://huggingface.co/docs/datasets/repository_structure',\n",
       " 'https://huggingface.co/docs/datasets/dataset_script',\n",
       " 'https://huggingface.co/docs/datasets/about_arrow',\n",
       " 'https://huggingface.co/docs/datasets/about_cache',\n",
       " 'https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable',\n",
       " 'https://huggingface.co/docs/datasets/about_dataset_features',\n",
       " 'https://huggingface.co/docs/datasets/about_dataset_load',\n",
       " 'https://huggingface.co/docs/datasets/about_map_batch',\n",
       " 'https://huggingface.co/docs/datasets/about_metrics',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/main_classes',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/builder_classes',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/loading_methods',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/table_classes',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/utilities',\n",
       " 'https://huggingface.co/docs/datasets/package_reference/task_templates',\n",
       " 'https://huggingface.co/docs/huggingface_hub/index',\n",
       " 'https://huggingface.co/docs/huggingface_hub/quick-start',\n",
       " 'https://huggingface.co/docs/huggingface_hub/installation',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/overview',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/download',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/upload',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/cli',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/hf_file_system',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/repository',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/search',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/inference',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/community',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/collections',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/manage-cache',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/model-cards',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/manage-spaces',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/integrations',\n",
       " 'https://huggingface.co/docs/huggingface_hub/guides/webhooks_server',\n",
       " 'https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/overview',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/login',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/repository',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/hf_api',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/file_download',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/mixins',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/inference_client',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/hf_file_system',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/utilities',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/community',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/cache',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/cards',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/space_runtime',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/collections',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/tensorboard',\n",
       " 'https://huggingface.co/docs/huggingface_hub/package_reference/webhooks_server',\n",
       " 'https://huggingface.co/docs/huggingface.js/index',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/README',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/modules',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/classes/HfInference',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/classes/HfInferenceEndpoint',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/classes/InferenceOutputError',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioClassificationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioToAudioOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/AutomaticSpeechRecognitionOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/BaseArgs',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ConversationalOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/DocumentQuestionAnsweringOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageClassificationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageSegmentationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageToTextOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ObjectDetectionOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/Options',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/QuestionAnsweringOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/SummarizationOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TableQuestionAnsweringOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamBestOfSequence',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamDetails',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamPrefillToken',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamToken',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TokenClassificationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/TranslationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/VisualQuestionAnsweringOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotClassificationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotImageClassificationOutputValue',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/README',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/modules',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/classes/HubApiError',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/classes/InvalidApiResponseFormatError',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/AuthInfo',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitDeletedEntry',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitFile',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitParams',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/Credentials',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/DatasetEntry',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/FileDownloadInfoOutput',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/ListFileEntry',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/ModelEntry',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/OAuthResult',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/RepoId',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/SafetensorsIndexJson',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceConfig',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceRequirement',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceRuntime',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/TensorInfo',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIApp',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIOrg',\n",
       " 'https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIUser',\n",
       " 'https://huggingface.co/docs/huggingface.js/agents/README',\n",
       " 'https://huggingface.co/docs/huggingface.js/agents/modules',\n",
       " 'https://huggingface.co/docs/huggingface.js/agents/classes/HfAgent',\n",
       " 'https://huggingface.co/docs/transformers.js/index',\n",
       " 'https://huggingface.co/docs/transformers.js/installation',\n",
       " 'https://huggingface.co/docs/transformers.js/pipelines',\n",
       " 'https://huggingface.co/docs/transformers.js/custom_usage',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/vanilla-js',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/react',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/next',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/browser-extension',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/electron',\n",
       " 'https://huggingface.co/docs/transformers.js/tutorials/node',\n",
       " 'https://huggingface.co/docs/transformers.js/guides/private',\n",
       " 'https://huggingface.co/docs/transformers.js/guides/node-audio-processing',\n",
       " 'https://huggingface.co/docs/transformers.js/api/transformers',\n",
       " 'https://huggingface.co/docs/transformers.js/api/pipelines',\n",
       " 'https://huggingface.co/docs/transformers.js/api/models',\n",
       " 'https://huggingface.co/docs/transformers.js/api/tokenizers',\n",
       " 'https://huggingface.co/docs/transformers.js/api/processors',\n",
       " 'https://huggingface.co/docs/transformers.js/api/configs',\n",
       " 'https://huggingface.co/docs/transformers.js/api/env',\n",
       " 'https://huggingface.co/docs/api-inference/index',\n",
       " 'https://huggingface.co/docs/api-inference/quicktour',\n",
       " 'https://huggingface.co/docs/api-inference/detailed_parameters',\n",
       " 'https://huggingface.co/docs/api-inference/parallelism',\n",
       " 'https://huggingface.co/docs/api-inference/usage',\n",
       " 'https://huggingface.co/docs/api-inference/faq',\n",
       " 'https://huggingface.co/docs/inference-endpoints/index',\n",
       " 'https://huggingface.co/docs/inference-endpoints/security',\n",
       " 'https://huggingface.co/docs/inference-endpoints/supported_tasks',\n",
       " 'https://huggingface.co/docs/inference-endpoints/api_reference',\n",
       " 'https://huggingface.co/docs/inference-endpoints/autoscaling',\n",
       " 'https://huggingface.co/docs/inference-endpoints/pricing',\n",
       " 'https://huggingface.co/docs/inference-endpoints/support',\n",
       " 'https://huggingface.co/docs/inference-endpoints/faq',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/access',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/create_endpoint',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/test_endpoint',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/update_endpoint',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/advanced',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/private_link',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/custom_handler',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/custom_container',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/logs',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/metrics',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/change_organization',\n",
       " 'https://huggingface.co/docs/inference-endpoints/guides/pause_endpoint',\n",
       " 'https://huggingface.co/docs/inference-endpoints/others/runtime',\n",
       " 'https://huggingface.co/docs/inference-endpoints/others/serialization',\n",
       " 'https://huggingface.co/docs/peft/index',\n",
       " 'https://huggingface.co/docs/peft/quicktour',\n",
       " 'https://huggingface.co/docs/peft/install',\n",
       " 'https://huggingface.co/docs/peft/tutorial/peft_model_config',\n",
       " 'https://huggingface.co/docs/peft/tutorial/peft_integrations',\n",
       " 'https://huggingface.co/docs/peft/task_guides/prompt_based_methods',\n",
       " 'https://huggingface.co/docs/peft/task_guides/lora_based_methods',\n",
       " 'https://huggingface.co/docs/peft/task_guides/ia3',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/quantization',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/lora',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/custom_models',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/low_level_api',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/mixed_models',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/contributing',\n",
       " 'https://huggingface.co/docs/peft/developer_guides/troubleshooting',\n",
       " 'https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload',\n",
       " 'https://huggingface.co/docs/peft/accelerate/fsdp',\n",
       " 'https://huggingface.co/docs/peft/conceptual_guides/adapter',\n",
       " 'https://huggingface.co/docs/peft/conceptual_guides/prompting',\n",
       " 'https://huggingface.co/docs/peft/conceptual_guides/ia3',\n",
       " 'https://huggingface.co/docs/peft/package_reference/auto_class',\n",
       " 'https://huggingface.co/docs/peft/package_reference/peft_model',\n",
       " 'https://huggingface.co/docs/peft/package_reference/peft_types',\n",
       " 'https://huggingface.co/docs/peft/package_reference/config',\n",
       " 'https://huggingface.co/docs/peft/package_reference/tuners',\n",
       " 'https://huggingface.co/docs/peft/package_reference/adalora',\n",
       " 'https://huggingface.co/docs/peft/package_reference/ia3',\n",
       " 'https://huggingface.co/docs/peft/package_reference/llama_adapter',\n",
       " 'https://huggingface.co/docs/peft/package_reference/loha',\n",
       " 'https://huggingface.co/docs/peft/package_reference/lokr',\n",
       " 'https://huggingface.co/docs/peft/package_reference/lora',\n",
       " 'https://huggingface.co/docs/peft/package_reference/adapter_utils',\n",
       " 'https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning',\n",
       " 'https://huggingface.co/docs/peft/package_reference/oft',\n",
       " 'https://huggingface.co/docs/peft/package_reference/poly',\n",
       " 'https://huggingface.co/docs/peft/package_reference/p_tuning',\n",
       " 'https://huggingface.co/docs/peft/package_reference/prefix_tuning',\n",
       " 'https://huggingface.co/docs/peft/package_reference/prompt_tuning',\n",
       " 'https://huggingface.co/docs/accelerate/index',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/install',\n",
       " 'https://huggingface.co/docs/accelerate/quicktour',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/overview',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/migration',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/launch',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/notebook',\n",
       " 'https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/explore',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/training_zoo',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/big_modeling',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/quantization',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/distributed_inference',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/local_sgd',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/checkpoint',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/tracking',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/mps',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/low_precision_training',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/deepspeed',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/fsdp',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/megatron_lm',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/sagemaker',\n",
       " 'https://huggingface.co/docs/accelerate/usage_guides/ipex',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/internal_mechanism',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/big_model_inference',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/performance',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/deferring_execution',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/low_precision_training',\n",
       " 'https://huggingface.co/docs/accelerate/concept_guides/training_tpu',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/accelerator',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/state',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/cli',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/torch_wrappers',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/tracking',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/launchers',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/deepspeed',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/logging',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/big_modeling',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/kwargs',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/utilities',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/megatron_lm',\n",
       " 'https://huggingface.co/docs/accelerate/package_reference/fsdp',\n",
       " 'https://huggingface.co/docs/optimum/index',\n",
       " 'https://huggingface.co/docs/optimum/installation',\n",
       " 'https://huggingface.co/docs/optimum/quicktour',\n",
       " 'https://huggingface.co/docs/optimum/notebooks',\n",
       " 'https://huggingface.co/docs/optimum/concept_guides/quantization',\n",
       " 'https://huggingface.co/docs/optimum-neuron/index',\n",
       " 'https://huggingface.co/docs/optimum-neuron/installation',\n",
       " 'https://huggingface.co/docs/optimum-neuron/quickstart',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/overview',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/notebooks',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/stable_diffusion',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_llama_7b',\n",
       " 'https://huggingface.co/docs/optimum-neuron/tutorials/sentence_transformers',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/overview',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/setup_aws_instance',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/sagemaker',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/cache_system',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/fine_tune',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/distributed_training',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/export_model',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/models',\n",
       " 'https://huggingface.co/docs/optimum-neuron/guides/pipelines',\n",
       " 'https://huggingface.co/docs/optimum-neuron/community/contributing',\n",
       " 'https://huggingface.co/docs/optimum-neuron/package_reference/trainer',\n",
       " 'https://huggingface.co/docs/optimum-neuron/package_reference/distributed',\n",
       " 'https://huggingface.co/docs/optimum-neuron/package_reference/export',\n",
       " 'https://huggingface.co/docs/optimum-neuron/package_reference/modeling',\n",
       " 'https://huggingface.co/docs/optimum-neuron/benchmarks/inferentia-llama2',\n",
       " 'https://huggingface.co/docs/tokenizers/index',\n",
       " 'https://huggingface.co/docs/tokenizers/quicktour',\n",
       " 'https://huggingface.co/docs/tokenizers/installation',\n",
       " 'https://huggingface.co/docs/tokenizers/pipeline',\n",
       " 'https://huggingface.co/docs/tokenizers/components',\n",
       " 'https://huggingface.co/docs/tokenizers/training_from_memory',\n",
       " 'https://huggingface.co/docs/tokenizers/api/input-sequences',\n",
       " 'https://huggingface.co/docs/tokenizers/api/encode-inputs',\n",
       " 'https://huggingface.co/docs/tokenizers/api/tokenizer',\n",
       " 'https://huggingface.co/docs/tokenizers/api/encoding',\n",
       " 'https://huggingface.co/docs/tokenizers/api/added-tokens',\n",
       " 'https://huggingface.co/docs/tokenizers/api/models',\n",
       " 'https://huggingface.co/docs/tokenizers/api/normalizers',\n",
       " 'https://huggingface.co/docs/tokenizers/api/pre-tokenizers',\n",
       " 'https://huggingface.co/docs/tokenizers/api/post-processors',\n",
       " 'https://huggingface.co/docs/tokenizers/api/trainers',\n",
       " 'https://huggingface.co/docs/tokenizers/api/decoders',\n",
       " 'https://huggingface.co/docs/tokenizers/api/visualizer',\n",
       " 'https://huggingface.co/docs/evaluate/index',\n",
       " 'https://huggingface.co/docs/evaluate/installation',\n",
       " 'https://huggingface.co/docs/evaluate/a_quick_tour',\n",
       " 'https://huggingface.co/docs/evaluate/choosing_a_metric',\n",
       " 'https://huggingface.co/docs/evaluate/creating_and_sharing',\n",
       " 'https://huggingface.co/docs/evaluate/base_evaluator',\n",
       " 'https://huggingface.co/docs/evaluate/custom_evaluator',\n",
       " 'https://huggingface.co/docs/evaluate/evaluation_suite',\n",
       " 'https://huggingface.co/docs/evaluate/transformers_integrations',\n",
       " 'https://huggingface.co/docs/evaluate/keras_integrations',\n",
       " 'https://huggingface.co/docs/evaluate/sklearn_integrations',\n",
       " 'https://huggingface.co/docs/evaluate/types_of_evaluations',\n",
       " 'https://huggingface.co/docs/evaluate/considerations',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/main_classes',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/loading_methods',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/saving_methods',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/hub_methods',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/evaluator_classes',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/visualization_methods',\n",
       " 'https://huggingface.co/docs/evaluate/package_reference/logging_methods',\n",
       " 'https://huggingface.co/docs/datasets-server/index',\n",
       " 'https://huggingface.co/docs/datasets-server/quick_start',\n",
       " 'https://huggingface.co/docs/datasets-server/analyze_data',\n",
       " 'https://huggingface.co/docs/datasets-server/valid',\n",
       " 'https://huggingface.co/docs/datasets-server/splits',\n",
       " 'https://huggingface.co/docs/datasets-server/info',\n",
       " 'https://huggingface.co/docs/datasets-server/first_rows',\n",
       " 'https://huggingface.co/docs/datasets-server/rows',\n",
       " 'https://huggingface.co/docs/datasets-server/search',\n",
       " 'https://huggingface.co/docs/datasets-server/filter',\n",
       " 'https://huggingface.co/docs/datasets-server/parquet',\n",
       " 'https://huggingface.co/docs/datasets-server/size',\n",
       " 'https://huggingface.co/docs/datasets-server/statistics',\n",
       " 'https://huggingface.co/docs/datasets-server/parquet_process',\n",
       " 'https://huggingface.co/docs/datasets-server/clickhouse',\n",
       " 'https://huggingface.co/docs/datasets-server/duckdb',\n",
       " 'https://huggingface.co/docs/datasets-server/pandas',\n",
       " 'https://huggingface.co/docs/datasets-server/polars',\n",
       " 'https://huggingface.co/docs/datasets-server/configs_and_splits',\n",
       " 'https://huggingface.co/docs/datasets-server/data_types',\n",
       " 'https://huggingface.co/docs/datasets-server/server',\n",
       " 'https://huggingface.co/docs/trl/index',\n",
       " 'https://huggingface.co/docs/trl/quickstart',\n",
       " 'https://huggingface.co/docs/trl/installation',\n",
       " 'https://huggingface.co/docs/trl/how_to_train',\n",
       " 'https://huggingface.co/docs/trl/use_model',\n",
       " 'https://huggingface.co/docs/trl/customization',\n",
       " 'https://huggingface.co/docs/trl/logging',\n",
       " 'https://huggingface.co/docs/trl/models',\n",
       " 'https://huggingface.co/docs/trl/trainer',\n",
       " 'https://huggingface.co/docs/trl/reward_trainer',\n",
       " 'https://huggingface.co/docs/trl/sft_trainer',\n",
       " 'https://huggingface.co/docs/trl/ppo_trainer',\n",
       " 'https://huggingface.co/docs/trl/best_of_n',\n",
       " 'https://huggingface.co/docs/trl/dpo_trainer',\n",
       " 'https://huggingface.co/docs/trl/ddpo_trainer',\n",
       " 'https://huggingface.co/docs/trl/iterative_sft_trainer',\n",
       " 'https://huggingface.co/docs/trl/text_environments',\n",
       " 'https://huggingface.co/docs/trl/example_overview',\n",
       " 'https://huggingface.co/docs/trl/sentiment_tuning',\n",
       " 'https://huggingface.co/docs/trl/lora_tuning_peft',\n",
       " 'https://huggingface.co/docs/trl/detoxifying_a_lm',\n",
       " 'https://huggingface.co/docs/trl/using_llama_models',\n",
       " 'https://huggingface.co/docs/trl/learning_tools',\n",
       " 'https://huggingface.co/docs/trl/multi_adapter_rl',\n",
       " 'https://huggingface.co/docs/sagemaker/index',\n",
       " 'https://huggingface.co/docs/sagemaker/getting-started',\n",
       " 'https://huggingface.co/docs/sagemaker/train',\n",
       " 'https://huggingface.co/docs/sagemaker/inference',\n",
       " 'https://huggingface.co/docs/sagemaker/reference',\n",
       " 'https://huggingface.co/docs/timm/index',\n",
       " 'https://huggingface.co/docs/timm/quickstart',\n",
       " 'https://huggingface.co/docs/timm/installation',\n",
       " 'https://huggingface.co/docs/timm/feature_extraction',\n",
       " 'https://huggingface.co/docs/timm/training_script',\n",
       " 'https://huggingface.co/docs/timm/hf_hub',\n",
       " 'https://huggingface.co/docs/timm/reference/models',\n",
       " 'https://huggingface.co/docs/timm/reference/data',\n",
       " 'https://huggingface.co/docs/timm/reference/optimizers',\n",
       " 'https://huggingface.co/docs/timm/reference/schedulers',\n",
       " 'https://huggingface.co/docs/safetensors/index',\n",
       " 'https://huggingface.co/docs/safetensors/speed',\n",
       " 'https://huggingface.co/docs/safetensors/torch_shared_tensors',\n",
       " 'https://huggingface.co/docs/safetensors/metadata_parsing',\n",
       " 'https://huggingface.co/docs/safetensors/convert-weights',\n",
       " 'https://huggingface.co/docs/safetensors/api/torch',\n",
       " 'https://huggingface.co/docs/safetensors/api/tensorflow',\n",
       " 'https://huggingface.co/docs/safetensors/api/paddle',\n",
       " 'https://huggingface.co/docs/safetensors/api/flax',\n",
       " 'https://huggingface.co/docs/safetensors/api/numpy',\n",
       " 'https://huggingface.co/docs/text-generation-inference/index',\n",
       " 'https://huggingface.co/docs/text-generation-inference/quicktour',\n",
       " 'https://huggingface.co/docs/text-generation-inference/installation',\n",
       " 'https://huggingface.co/docs/text-generation-inference/supported_models',\n",
       " 'https://huggingface.co/docs/text-generation-inference/messages_api',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/gated_model_access',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/using_cli',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher',\n",
       " 'https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/streaming',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/quantization',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/safetensors',\n",
       " 'https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention',\n",
       " 'https://huggingface.co/docs/autotrain/index',\n",
       " 'https://huggingface.co/docs/autotrain/getting_started',\n",
       " 'https://huggingface.co/docs/autotrain/cost',\n",
       " 'https://huggingface.co/docs/autotrain/support',\n",
       " 'https://huggingface.co/docs/autotrain/text_classification',\n",
       " 'https://huggingface.co/docs/autotrain/llm_finetuning',\n",
       " 'https://huggingface.co/docs/autotrain/image_classification',\n",
       " 'https://huggingface.co/docs/autotrain/dreambooth',\n",
       " 'https://huggingface.co/docs/autotrain/seq2seq',\n",
       " 'https://huggingface.co/docs/autotrain/token_classification',\n",
       " 'https://huggingface.co/docs/autotrain/tabular',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/index',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/quick_tour',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/supported_models',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/local_cpu',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/local_metal',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/local_gpu',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/private_models',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/custom_container',\n",
       " 'https://huggingface.co/docs/text-embeddings-inference/cli_arguments',\n",
       " 'https://huggingface.co/docs/competitions/index',\n",
       " 'https://huggingface.co/docs/competitions/pricing',\n",
       " 'https://huggingface.co/docs/competitions/create_competition',\n",
       " 'https://huggingface.co/docs/competitions/competition_repo',\n",
       " 'https://huggingface.co/docs/competitions/competition_space',\n",
       " 'https://huggingface.co/docs/competitions/custom_metric',\n",
       " 'https://huggingface.co/docs/competitions/submit',\n",
       " 'https://huggingface.co/docs/competitions/leaderboard',\n",
       " 'https://huggingface.co/docs/competitions/teams']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"D:\\ML_Projects\\AIPlanet\\Intern\\documentations\\hf_docs\\hf_docs_v3.json\")\n",
    "\n",
    "all_sublinks = df['url'].to_list()\n",
    "all_sublinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Text-to-image',\n",
       " 'The text-to-image script is experimental, and its easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(all_sublinks[200])\n",
    "\n",
    "element = driver.find_element(By.CLASS_NAME, 'from-black.to-gray-900')\n",
    "title = element.text\n",
    "desc = driver.find_element(By.CLASS_NAME, 'prose-doc').find_elements(By.TAG_NAME, 'p')[1]\n",
    "\n",
    "title,desc.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate(all_sublinks):\n",
    "    y = driver.get(x)\n",
    "    driver.implicitly_wait(0.5)\n",
    "    try:\n",
    "        element = driver.find_element(By.CLASS_NAME, 'from-black.to-gray-900')\n",
    "        title = element.text\n",
    "        desc = driver.find_element(By.CLASS_NAME, 'prose-doc').find_elements(By.TAG_NAME, 'p')[1].text\n",
    "    except:\n",
    "        if not title:\n",
    "            title = ''\n",
    "        if not desc:\n",
    "            desc = ''\n",
    "    all_sublinks[i] = [x,title,desc]\n",
    "\n",
    "all_sublinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    token='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'\n",
    "    # \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2'\n",
    ")\n",
    "\n",
    "generate_kwargs = dict(\n",
    "        temperature=0.6,\n",
    "        max_new_tokens=1000,\n",
    "        top_p=0.99,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "        # seed=42,\n",
    "    )\n",
    "\n",
    "formatted_prompt = f'''<s>[INST] This is the list of available documentations \\n\\n{str(all_sublinks)}\\n\\nTell me the url or urls of the most suitable documentation for the given task. Give the URLs in a python list format so i can parse them with code easily. Do not give any explanation or anything, just the urls.\\n Task: Tell me how to create a huggingface dataset and use mapping on it. [/INST]'''\n",
    "\n",
    "response = client.text_generation(formatted_prompt, **generate_kwargs, details=True, return_full_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https://huggingface.co/docs/hub/index',\n",
       "  ' Hugging Face Hub',\n",
       "  'The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? '],\n",
       " ['https://huggingface.co/docs/hub/repositories',\n",
       "  'Repositories',\n",
       "  'Models, Spaces, and Datasets are hosted on the Hugging Face Hub as Git repositories, which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a repo) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team.'],\n",
       " ['https://huggingface.co/docs/hub/repositories-getting-started',\n",
       "  'Getting Started with Repositories',\n",
       "  'This beginner-friendly guide will help you get the basic skills you need to create and manage your repository on the Hub. Each section builds on the previous one, so feel free to choose where to start!'],\n",
       " ['https://huggingface.co/docs/hub/repositories-settings',\n",
       "  'Repository Settings',\n",
       "  'You can choose a repositorys visibility when you create it, and any repository that you own can have its visibility toggled between public and private in the Settings tab. Unless your repository is owned by an organization, you are the only user that can make changes to your repo or upload any code. Setting your visibility to private will:'],\n",
       " ['https://huggingface.co/docs/hub/repositories-pull-requests-discussions',\n",
       "  'Pull Requests & Discussions',\n",
       "  'Hub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types.'],\n",
       " ['https://huggingface.co/docs/hub/notifications',\n",
       "  'Notifications',\n",
       "  'Notifications allow you to know when new activities (Pull Requests or discussions) happen on models, datasets, and Spaces belonging to users or organizations you are watching.'],\n",
       " ['https://huggingface.co/docs/hub/collections',\n",
       "  'Collections',\n",
       "  'Use Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers) on a dedicated page.'],\n",
       " ['https://huggingface.co/docs/hub/webhooks',\n",
       "  'Webhooks',\n",
       "  'Webhooks are now publicly available!'],\n",
       " ['https://huggingface.co/docs/hub/repositories-recommendations',\n",
       "  'Repository size recommendations',\n",
       "  'There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data, getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying.'],\n",
       " ['https://huggingface.co/docs/hub/repositories-next-steps',\n",
       "  'Next Steps',\n",
       "  'These next sections highlight features and additional information that you may find useful to make the most out of the Git repositories on the Hugging Face Hub.'],\n",
       " ['https://huggingface.co/docs/hub/repositories-licenses',\n",
       "  'Licenses',\n",
       "  'You are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repositorys README.md file, known as a card on the Hub, in the cards metadata section. Remember to seek out and respect a projects license if youre considering using their code or data.'],\n",
       " ['https://huggingface.co/docs/hub/models',\n",
       "  'Models',\n",
       "  'The Hugging Face Hub hosts many models for a variety of machine learning tasks. Models are stored in repositories, so they benefit from all the features possessed by every repo on the Hugging Face Hub. Additionally, model repos have attributes that make exploring and using models as easy as possible. These docs will take you through everything youll need to know to find models on the Hub, upload your models, and make the most of everything the Model Hub offers!'],\n",
       " ['https://huggingface.co/docs/hub/models-the-hub',\n",
       "  'The Model Hub',\n",
       "  'The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the huggingface_hub client library, with  Transformers for fine-tuning and other usages or with any of the over 15 integrated libraries. You can even leverage the Inference API to use models in production settings.'],\n",
       " ['https://huggingface.co/docs/hub/model-cards',\n",
       "  'Model Cards',\n",
       "  'New! Try our experimental Model Card Creator App'],\n",
       " ['https://huggingface.co/docs/hub/models-gated',\n",
       "  'Gated Models',\n",
       "  'To give more control over how models are used, the Hub allows model authors to enable access requests for their models. Users must agree to share their contact information (username and email address) with the model authors to access the model files when enabled. Model authors can configure this request with additional fields. A model with access requests enabled is called a gated model. Access requests are always granted to individual users rather than to entire organizations. A common use case of gated models is to provide access to early research models before the wider release.'],\n",
       " ['https://huggingface.co/docs/hub/models-uploading',\n",
       "  'Uploading Models',\n",
       "  'To upload models to the Hub, youll need to create an account at Hugging Face. Models on the Hub are Git-based repositories, which give you versioning, branches, discoverability and sharing features, integration with dozens of libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files.'],\n",
       " ['https://huggingface.co/docs/hub/models-downloading',\n",
       "  'Downloading Models',\n",
       "  'If a model on the Hub is tied to a supported library, loading the model can be done in just a few lines. For information on accessing the model, you can click on the Use in Library button on the model page to see how to do so. For example, distilbert/distilgpt2 shows how to do so with  Transformers below.'],\n",
       " ['https://huggingface.co/docs/hub/models-libraries',\n",
       "  'Integrated Libraries',\n",
       "  'The Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the huggingface_hub Python library, its easy to enable sharing your models on the Hub. The Hub supports many libraries, and were working on expanding this support. Were happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.'],\n",
       " ['https://huggingface.co/docs/hub/models-widgets',\n",
       "  'Model Widgets',\n",
       "  'Many model repos have a widget that allows anyone to run inferences directly in the browser!'],\n",
       " ['https://huggingface.co/docs/hub/models-inference',\n",
       "  'Inference API docs',\n",
       "  'Please refer to Inference API Documentation for detailed information.'],\n",
       " ['https://huggingface.co/docs/hub/models-download-stats',\n",
       "  'Models Download Stats',\n",
       "  'Counting the number of downloads for models is not a trivial task, as a single model repository might contain multiple files, including multiple model weight files (e.g., with sharded models) and different formats depending on the library (GGUF, PyTorch, TensorFlow, etc.). To avoid double counting downloads (e.g., counting a single download of a model as multiple downloads), the Hub uses a set of query files that are employed for download counting. No information is sent from the user, and no additional calls are made for this. The count is done server-side as the Hub serves files for downloads.'],\n",
       " ['https://huggingface.co/docs/hub/models-faq',\n",
       "  'Frequently Asked Questions',\n",
       "  'Its up to the person who uploaded the model to include the training information! A user can specify the dataset used for training a model. If the datasets used for the model are on the Hub, the uploader may have included them in the model cards metadata. In that case, the datasets would be linked with a handy card on the right side of the model page:'],\n",
       " ['https://huggingface.co/docs/hub/models-advanced', 'Advanced Topics', ''],\n",
       " ['https://huggingface.co/docs/hub/datasets',\n",
       "  'Datasets',\n",
       "  'The Hugging Face Hub is home to a growing collection of datasets that span a variety of domains and tasks. These docs will guide you through interacting with the datasets on the Hub, uploading new datasets, exploring the datasets contents, and using datasets in your projects.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-overview',\n",
       "  'Datasets Overview',\n",
       "  'The Hugging Face Hub hosts a large number of community-curated datasets for a diverse range of tasks such as translation, automatic speech recognition, and image classification. Alongside the information contained in the dataset card, many datasets, such as GLUE, include a Dataset Viewer to showcase the data.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-cards',\n",
       "  'Dataset Cards',\n",
       "  'Each dataset may be documented by the README.md file in the repository. This file is called a dataset card, and the Hugging Face Hub will render its contents on the datasets main page. To inform users about how to responsibly use the data, its a good idea to include information about any potential biases within the dataset. Generally, dataset cards help users understand the contents of the dataset and give context for how the dataset should be used.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-gated',\n",
       "  'Gated Datasets',\n",
       "  'To give more control over how datasets are used, the Hub allows datasets authors to enable access requests for their datasets. Users must agree to share their contact information (username and email address) with the datasets authors to access the datasets files when enabled. Datasets authors can configure this request with additional fields. A dataset with access requests enabled is called a gated dataset. Access requests are always granted to individual users rather than to entire organizations. A common use case of gated datasets is to provide access to early research datasets before the wider release.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-adding',\n",
       "  'Uploading Datasets',\n",
       "  'The Hub is home to an extensive collection of community-curated and research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!'],\n",
       " ['https://huggingface.co/docs/hub/datasets-downloading',\n",
       "  'Downloading Datasets',\n",
       "  'If a dataset on the Hub is tied to a supported library, loading the dataset can be done in just a few lines. For information on accessing the dataset, you can click on the Use in dataset library button on the dataset page to see how to do so. For example, samsum shows how to do so with  Datasets below.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-libraries',\n",
       "  'Integrated Libraries',\n",
       "  'The Datasets Hub has support for several libraries in the Open Source ecosystem. Thanks to the huggingface_hub Python library, its easy to enable sharing your datasets on the Hub. Were happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-viewer',\n",
       "  'Dataset Viewer',\n",
       "  'The dataset page includes a table with the contents of the dataset, arranged by pages of 100 rows. You can navigate between pages using the buttons at the bottom of the table.'],\n",
       " ['https://huggingface.co/docs/hub/datasets-download-stats',\n",
       "  'Datasets Download Stats',\n",
       "  'The Hub provides download stats for all datasets loadable via the datasets library. To determine the number of downloads, the Hub counts every time load_dataset is called in Python, excluding Hugging Faces CI tooling on GitHub. No information is sent from the user, and no additional calls are made for this. The count is done server-side as we serve files for downloads. This means that:'],\n",
       " ['https://huggingface.co/docs/hub/datasets-data-files-configuration',\n",
       "  'Data files Configuration',\n",
       "  'There are no constraints on how to structure dataset repositories.'],\n",
       " ['https://huggingface.co/docs/hub/spaces',\n",
       "  'Spaces',\n",
       "  'Hugging Face Spaces offer a simple way to host ML demo apps directly on your profile or your organizations profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-overview',\n",
       "  'Spaces Overview',\n",
       "  'Hugging Face Spaces make it easy for you to create and deploy ML-powered demos in minutes. Watch the following video for a quick introduction to Spaces:'],\n",
       " ['https://huggingface.co/docs/hub/spaces-gpus',\n",
       "  'Spaces GPU Upgrades',\n",
       "  'You can upgrade your Space to use a GPU accelerator using the Settings button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!'],\n",
       " ['https://huggingface.co/docs/hub/spaces-storage',\n",
       "  'Spaces Persistent Storage',\n",
       "  'Every Space comes with a small amount of disk storage. This disk space is ephemeral, meaning its content will be lost if your Space restarts or is stopped. If you need to persist data with a longer lifetime than the Space itself, you can:'],\n",
       " ['https://huggingface.co/docs/hub/spaces-sdks-gradio',\n",
       "  'Gradio Spaces',\n",
       "  'Gradio provides an easy and intuitive interface for running a model from a list of inputs and displaying the outputs in formats such as images, audio, 3D objects, and more. Gradio now even has a Plot output component for creating data visualizations with Matplotlib, Bokeh, and Plotly! For more details, take a look at the Getting started guide from the Gradio team.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-sdks-streamlit',\n",
       "  'Streamlit Spaces',\n",
       "  'Streamlit gives users freedom to build a full-featured web app with Python in a reactive way. Your code is rerun each time the state of the app changes. Streamlit is also great for data visualization and supports several charting libraries such as Bokeh, Plotly, and Altair. Read this blog post about building and hosting Streamlit apps in Spaces.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-sdks-static',\n",
       "  'Static HTML Spaces',\n",
       "  'Spaces also accommodate custom HTML for your app instead of using Streamlit or Gradio. Set sdk: static inside the YAML block at the top of your Spaces README.md file. Then you can place your HTML code within an index.html file.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-sdks-docker',\n",
       "  'Docker Spaces',\n",
       "  'Spaces accommodate custom Docker containers for apps outside the scope of Streamlit and Gradio. Docker Spaces allow users to go beyond the limits of what was previously possible with the standard SDKs. From FastAPI and Go endpoints to Phoenix apps and ML Ops tools, Docker Spaces can help in many different setups.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-embed',\n",
       "  'Embed your Space',\n",
       "  'Once your Space is up and running you might wish to embed it in a website or in your blog. Embedding or sharing your Space is a great way to allow your audience to interact with your work and demonstrations without requiring any setup on their side. To embed a Space its visibility needs to be public.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-run-with-docker',\n",
       "  'Run Spaces with Docker',\n",
       "  'You can use Docker to run most Spaces locally. To view instructions to download and run Spaces Docker images, click on the Run with Docker button on the top-right corner of your Space page:'],\n",
       " ['https://huggingface.co/docs/hub/spaces-config-reference',\n",
       "  'Spaces Configuration Reference',\n",
       "  'Spaces are configured through the YAML block at the top of the README.md file at the root of the repository. All the accepted parameters are listed below.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-oauth',\n",
       "  'Sign-In with HF button',\n",
       "  'You can enable a built-in sign-in flow in your Space by seamlessly creating and associating an OAuth/OpenID connect app so users can log in with their HF account.'],\n",
       " ['https://huggingface.co/docs/hub/spaces-changelog', 'Spaces Changelog', ''],\n",
       " ['https://huggingface.co/docs/hub/spaces-advanced', 'Advanced Topics', ''],\n",
       " ['https://huggingface.co/docs/hub/other', 'Other', ''],\n",
       " ['https://huggingface.co/docs/hub/organizations',\n",
       "  'Organizations',\n",
       "  'The Hugging Face Hub offers Organizations, which can be used to group accounts and manage datasets, models, and Spaces. The Hub also allows admins to set user roles to control access to repositories and manage their organizations payment method and billing info.'],\n",
       " ['https://huggingface.co/docs/hub/billing',\n",
       "  'Billing',\n",
       "  'At Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), and we monetize by providing simple access to compute for AI, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub, and billed by Hugging Face to the credit card on file.'],\n",
       " ['https://huggingface.co/docs/hub/security',\n",
       "  'Security',\n",
       "  'The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering private repositories for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.'],\n",
       " ['https://huggingface.co/docs/hub/moderation',\n",
       "  'Moderation',\n",
       "  'Check out the Code of Conduct and the Content Guidelines.'],\n",
       " ['https://huggingface.co/docs/hub/paper-pages',\n",
       "  'Paper Pages',\n",
       "  'Paper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper.'],\n",
       " ['https://huggingface.co/docs/hub/search',\n",
       "  'Search',\n",
       "  'You can now easily search anything on the Hub with Full-text search. We index model cards, dataset cards, and Spaces app.py files.'],\n",
       " ['https://huggingface.co/docs/hub/doi',\n",
       "  'Digital Object Identifier (DOI)',\n",
       "  'The Hugging Face Hub offers the possibility to generate DOI for your models or datasets. DOIs (Digital Object Identifiers) are strings uniquely identifying a digital object, anything from articles to figures, including datasets and models. DOIs are tied to object metadata, including the objects URL, version, creation date, description, etc. They are a commonly accepted reference to digital resources across research and academic communities; they are analogous to a books ISBN.'],\n",
       " ['https://huggingface.co/docs/hub/api',\n",
       "  'Hub API Endpoints',\n",
       "  'We have open endpoints that you can use to retrieve information from the Hub as well as perform certain actions such as creating model, dataset or Space repos. We offer a wrapper Python library, huggingface_hub, that allows easy access to these endpoints. We also provide webhooks to receive real-time incremental info about repos. Enjoy!'],\n",
       " ['https://huggingface.co/docs/hub/oauth',\n",
       "  'Sign-In with HF',\n",
       "  'You can use the HF OAuth / OpenID connect flow to create a Sign in with HF flow in any website or App.'],\n",
       " ['https://huggingface.co/docs/transformers/index',\n",
       "  ' Transformers',\n",
       "  'State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX.'],\n",
       " ['https://huggingface.co/docs/transformers/quicktour',\n",
       "  'Quick tour',\n",
       "  'Get up and running with  Transformers! Whether youre a developer or an everyday user, this quick tour will help you get started and show you how to use the pipeline() for inference, load a pretrained model and preprocessor with an AutoClass, and quickly train a model with PyTorch or TensorFlow. If youre a beginner, we recommend checking out our tutorials or course next for more in-depth explanations of the concepts introduced here.'],\n",
       " ['https://huggingface.co/docs/transformers/installation',\n",
       "  'Installation',\n",
       "  'Install  Transformers for whichever deep learning library youre working with, setup your cache, and optionally configure  Transformers to run offline.'],\n",
       " ['https://huggingface.co/docs/transformers/pipeline_tutorial',\n",
       "  'Run inference with pipelines',\n",
       "  'The pipeline() makes it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks. Even if you dont have experience with a specific modality or arent familiar with the underlying code behind the models, you can still use them for inference with the pipeline()! This tutorial will teach you to:'],\n",
       " ['https://huggingface.co/docs/transformers/autoclass_tutorial',\n",
       "  'Write portable code with AutoClass',\n",
       "  'With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of  Transformers core philosophy to make the library easy, simple and flexible to use, an AutoClass automatically infers and loads the correct architecture from a given checkpoint. The from_pretrained() method lets you quickly load a pretrained model for any architecture so you dont have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.'],\n",
       " ['https://huggingface.co/docs/transformers/preprocessing',\n",
       "  'Preprocess data',\n",
       "  'Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors.  Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, youll learn that for:'],\n",
       " ['https://huggingface.co/docs/transformers/training',\n",
       "  'Fine-tune a pretrained model',\n",
       "  'There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch.  Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:'],\n",
       " ['https://huggingface.co/docs/transformers/run_scripts',\n",
       "  'Train with a script',\n",
       "  'Along with the  Transformers notebooks, there are also example scripts demonstrating how to train a model for a task with PyTorch, TensorFlow, or JAX/Flax.'],\n",
       " ['https://huggingface.co/docs/transformers/accelerate',\n",
       "  'Set up distributed training with  Accelerate',\n",
       "  'As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the  Accelerate library to help users easily train a  Transformers model on any type of distributed setup, whether it is multiple GPUs on one machine or multiple GPUs across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.'],\n",
       " ['https://huggingface.co/docs/transformers/peft',\n",
       "  'Load and train adapters with  PEFT',\n",
       "  'Parameter-Efficient Fine Tuning (PEFT) methods freeze the pretrained model parameters during fine-tuning and add a small number of trainable parameters (the adapters) on top of it. The adapters are trained to learn task-specific information. This approach has been shown to be very memory-efficient with lower compute usage while producing results comparable to a fully fine-tuned model.'],\n",
       " ['https://huggingface.co/docs/transformers/model_sharing',\n",
       "  'Share your model',\n",
       "  'The last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and  Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources.'],\n",
       " ['https://huggingface.co/docs/transformers/transformers_agents',\n",
       "  'Agents',\n",
       "  'Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change.'],\n",
       " ['https://huggingface.co/docs/transformers/llm_tutorial',\n",
       "  'Generation with LLMs',\n",
       "  'LLMs, or Large Language Models, are the key component behind text generation. In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model  you need to do autoregressive generation.'],\n",
       " ['https://huggingface.co/docs/transformers/fast_tokenizers',\n",
       "  'Use fast tokenizers from  Tokenizers',\n",
       "  'The PreTrainedTokenizerFast depends on the  Tokenizers library. The tokenizers obtained from the  Tokenizers library can be loaded very simply into  Transformers.'],\n",
       " ['https://huggingface.co/docs/transformers/multilingual',\n",
       "  'Run inference with multilingual models',\n",
       "  'There are several multilingual models in  Transformers, and their inference usage differs from monolingual models. Not all multilingual model usage is different though. Some models, like google-bert/bert-base-multilingual-uncased, can be used just like a monolingual model. This guide will show you how to use multilingual models whose usage differs for inference.'],\n",
       " ['https://huggingface.co/docs/transformers/create_a_model',\n",
       "  'Use model-specific APIs',\n",
       "  'An AutoClass automatically infers the model architecture and downloads pretrained configuration and weights. Generally, we recommend using an AutoClass to produce checkpoint-agnostic code. But users who want more control over specific model parameters can create a custom  Transformers model from just a few base classes. This could be particularly useful for anyone who is interested in studying, training or experimenting with a  Transformers model. In this guide, dive deeper into creating a custom model without an AutoClass. Learn how to:'],\n",
       " ['https://huggingface.co/docs/transformers/custom_models',\n",
       "  'Share a custom model',\n",
       "  'The  Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder of the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs.'],\n",
       " ['https://huggingface.co/docs/transformers/chat_templating',\n",
       "  'Templates for chat models',\n",
       "  'An increasingly common use case for LLMs is chat. In a chat context, rather than continuing a single string of text (as is the case with a standard language model), the model instead continues a conversation that consists of one or more messages, each of which includes a role, like user or assistant, as well as message text.'],\n",
       " ['https://huggingface.co/docs/transformers/trainer',\n",
       "  'Trainer',\n",
       "  'The Trainer is a complete training and evaluation loop for PyTorch models implemented in the Transformers library. You only need to pass it the necessary pieces for training (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.), and the Trainer class takes care of the rest. This makes it easier to start training faster without manually writing your own training loop. But at the same time, Trainer is very customizable and offers a ton of training options so you can tailor it to your exact training needs.'],\n",
       " ['https://huggingface.co/docs/transformers/sagemaker',\n",
       "  'Run training on Amazon SageMaker',\n",
       "  'The documentation has been moved to hf.co/docs/sagemaker. This page will be removed in transformers 5.0.'],\n",
       " ['https://huggingface.co/docs/transformers/serialization',\n",
       "  'Export to ONNX',\n",
       "  'Deploying  Transformers models in production environments often requires, or can benefit from exporting the models into a serialized format that can be loaded and executed on specialized runtimes and hardware.'],\n",
       " ['https://huggingface.co/docs/transformers/tflite',\n",
       "  'Export to TFLite',\n",
       "  'TensorFlow Lite is a lightweight framework for deploying machine learning models on resource-constrained devices, such as mobile phones, embedded systems, and Internet of Things (IoT) devices. TFLite is designed to optimize and run models efficiently on these devices with limited computational power, memory, and power consumption. A TensorFlow Lite model is represented in a special efficient portable format identified by the .tflite file extension.'],\n",
       " ['https://huggingface.co/docs/transformers/torchscript',\n",
       "  'Export to TorchScript',\n",
       "  'This is the very beginning of our experiments with TorchScript and we are still exploring its capabilities with variable-input-size models. It is a focus of interest to us and we will deepen our analysis in upcoming releases, with more code examples, a more flexible implementation, and benchmarks comparing Python-based codes with compiled TorchScript.'],\n",
       " ['https://huggingface.co/docs/transformers/benchmarks',\n",
       "  'Benchmarks',\n",
       "  'Hugging Faces Benchmarking tools are deprecated and it is advised to use external Benchmarking libraries to measure the speed and memory complexity of Transformer models.'],\n",
       " ['https://huggingface.co/docs/transformers/notebooks',\n",
       "  'Notebooks with examples',\n",
       "  'You can find here a list of the official notebooks provided by Hugging Face.'],\n",
       " ['https://huggingface.co/docs/transformers/community',\n",
       "  'Community resources',\n",
       "  'This page regroups resources around  Transformers developed by the community.'],\n",
       " ['https://huggingface.co/docs/transformers/custom_tools',\n",
       "  'Custom Tools and Prompts',\n",
       "  'If you are not aware of what tools and agents are in the context of transformers, we recommend you read the Transformers Agents page first.'],\n",
       " ['https://huggingface.co/docs/transformers/troubleshooting',\n",
       "  'Troubleshoot',\n",
       "  'Sometimes errors occur, but we are here to help! This guide covers some of the most common issues weve seen and how you can resolve them. However, this guide isnt meant to be a comprehensive collection of every  Transformers issue. For more help with troubleshooting your issue, try:'],\n",
       " ['https://huggingface.co/docs/transformers/performance',\n",
       "  'Overview',\n",
       "  'Training large transformer models and deploying them to production present various challenges.\\nDuring training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment phase, the model can struggle to handle the required throughput in a production environment.'],\n",
       " ['https://huggingface.co/docs/transformers/quantization',\n",
       "  'Quantization',\n",
       "  'Quantization techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits. For example, if your model weights are stored as 32-bit floating points and theyre quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory-usage. Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_gpu_one',\n",
       "  'Methods and tools for efficient training on a single GPU',\n",
       "  'This guide demonstrates practical techniques that you can use to increase the efficiency of your models training by optimizing memory utilization, speeding up the training, or both. If youd like to understand how GPU is utilized during training, please refer to the Model training anatomy conceptual guide first. This guide focuses on practical techniques.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_gpu_many',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/fsdp',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_cpu',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_cpu_many',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_tpu_tf',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_train_special',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_hardware',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/hpo_train',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_infer_cpu',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_infer_gpu_one',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/big_models',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/debugging',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/tf_xla',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/perf_torch_compile',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/contributing',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/add_new_model',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/add_tensorflow_model',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/add_new_pipeline',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/testing',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/pr_checks',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/philosophy',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/glossary',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/task_summary',\n",
       "  'Multiple GPUs and parallelism',\n",
       "  'If training a model on a single GPU is too slow or if the models weights do not fit in a single GPUs memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs.'],\n",
       " ['https://huggingface.co/docs/transformers/tasks_explained',\n",
       "  'How  Transformers solve tasks',\n",
       "  'In What  Transformers can do, you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain whats happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, decoder, or encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. Well also explain how a modern CNN works.'],\n",
       " ['https://huggingface.co/docs/transformers/model_summary',\n",
       "  'The Transformer model family',\n",
       "  'Since its introduction in 2017, the original Transformer model has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for predicting the folded structure of proteins, training a cheetah to run, and time series forecasting. With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is theyre based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and itll help you understand Transformers you havent encountered before.'],\n",
       " ['https://huggingface.co/docs/transformers/tokenizer_summary',\n",
       "  'Summary of the tokenizers',\n",
       "  'On this page, we will have a closer look at tokenization.'],\n",
       " ['https://huggingface.co/docs/transformers/attention',\n",
       "  'Attention mechanisms',\n",
       "  'Most transformer models use full attention in the sense that the attention matrix is square. It can be a big computational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and use a sparse version of the attention matrix to speed up training.'],\n",
       " ['https://huggingface.co/docs/transformers/pad_truncation',\n",
       "  'Padding and truncation',\n",
       "  'Batched inputs are often different lengths, so they cant be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special padding token to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences.'],\n",
       " ['https://huggingface.co/docs/transformers/bertology',\n",
       "  'BERTology',\n",
       "  'There is a growing field of study concerned with investigating the inner working of large-scale transformers like BERT (that some call BERTology). Some good examples of this field are:'],\n",
       " ['https://huggingface.co/docs/transformers/perplexity',\n",
       "  'Perplexity of fixed-length models',\n",
       "  'Perplexity (PPL) is one of the most common metrics for evaluating language models. Before diving in, we should note that the metric applies specifically to classical language models (sometimes called autoregressive or causal language models) and is not well defined for masked language models like BERT (see summary of the models).'],\n",
       " ['https://huggingface.co/docs/transformers/pipeline_webserver',\n",
       "  'Pipelines for webserver inference',\n",
       "  'The key thing to understand is that we can use an iterator, just like you would on a dataset, since a webserver is basically a system that waits for requests and treats them as they come in.'],\n",
       " ['https://huggingface.co/docs/transformers/model_memory_anatomy',\n",
       "  'Model training anatomy',\n",
       "  'To understand performance optimization techniques that one can apply to improve efficiency of model training speed and memory utilization, its helpful to get familiar with how GPU is utilized during training, and how compute intensity varies depending on an operation performed.'],\n",
       " ['https://huggingface.co/docs/transformers/llm_tutorial_optimization',\n",
       "  'Getting the most out of LLMs',\n",
       "  'Large Language Models (LLMs) such as GPT3/4, Falcon, and Llama are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries. Deploying these models in real-world tasks remains challenging, however:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/agent',\n",
       "  'Agents and Tools',\n",
       "  'Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change.'],\n",
       " ['https://huggingface.co/docs/transformers/model_doc/auto',\n",
       "  'Auto Classes',\n",
       "  'In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/backbones',\n",
       "  'Backbones',\n",
       "  'A backbone is a model used for feature extraction for higher level computer vision tasks such as object detection and image classification. Transformers provides an AutoBackbone class for initializing a Transformers backbone from pretrained model weights, and two utility classes:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/callback',\n",
       "  'Callbacks',\n",
       "  'Callbacks are objects that can customize the behavior of the training loop in the PyTorch Trainer (this feature is not yet implemented in TensorFlow) that can inspect the training loop state (for progress reporting, logging on TensorBoard or other ML platforms) and take decisions (like early stopping).'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/configuration',\n",
       "  'Configuration',\n",
       "  'The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFaces AWS S3 repository).'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/data_collator',\n",
       "  'Data Collator',\n",
       "  'Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/keras_callbacks',\n",
       "  'Keras callbacks',\n",
       "  'When training a Transformers model with Keras, there are some library-specific callbacks available to automate common tasks:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/logging',\n",
       "  'Logging',\n",
       "  ' Transformers has a centralized logging system, so that you can setup the verbosity of the library easily.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/model',\n",
       "  'Models',\n",
       "  'The base classes PreTrainedModel, TFPreTrainedModel, and FlaxPreTrainedModel implement the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFaces AWS S3 repository).'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/text_generation',\n",
       "  'Text Generation',\n",
       "  'Each framework has a generate method for text generation implemented in their respective GenerationMixin class:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/onnx',\n",
       "  'ONNX',\n",
       "  ' Transformers provides a transformers.onnx package that enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/optimizer_schedules',\n",
       "  'Optimization',\n",
       "  'The .optimization module provides:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/output',\n",
       "  'Model outputs',\n",
       "  'All models have outputs that are instances of subclasses of ModelOutput. Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/pipelines',\n",
       "  'Pipelines',\n",
       "  'The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/processors',\n",
       "  'Processors',\n",
       "  'Processors can mean two different things in the Transformers library:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/quantization',\n",
       "  'Quantization',\n",
       "  'Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldnt be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/tokenizer',\n",
       "  'Tokenizer',\n",
       "  'A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a Fast implementation based on the Rust library  Tokenizers. The Fast implementations allows:'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/trainer',\n",
       "  'Trainer',\n",
       "  'The Trainer class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for NVIDIA GPUs, AMD GPUs, and torch.amp for PyTorch. Trainer goes hand-in-hand with the TrainingArguments class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/deepspeed',\n",
       "  'DeepSpeed',\n",
       "  'DeepSpeed, powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the Trainer class and most of the setup is automatically taken care of for you.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/feature_extractor',\n",
       "  'Feature Extractor',\n",
       "  'A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors.'],\n",
       " ['https://huggingface.co/docs/transformers/main_classes/image_processor',\n",
       "  'Image Processor',\n",
       "  'An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/modeling_utils',\n",
       "  'Custom Layers and Utilities',\n",
       "  'This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/pipelines_utils',\n",
       "  'Utilities for pipelines',\n",
       "  'This page lists all the utility functions the library provides for pipelines.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/tokenization_utils',\n",
       "  'Utilities for Tokenizers',\n",
       "  'This page lists all the utility functions used by the tokenizers, mainly the class PreTrainedTokenizerBase that implements the common methods between PreTrainedTokenizer and PreTrainedTokenizerFast and the mixin SpecialTokensMixin.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/trainer_utils',\n",
       "  'Utilities for Trainer',\n",
       "  'This page lists all the utility functions used by Trainer.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/generation_utils',\n",
       "  'Utilities for Generation',\n",
       "  'This page lists all the utility functions used by generate().'],\n",
       " ['https://huggingface.co/docs/transformers/internal/image_processing_utils',\n",
       "  'Utilities for Image Processors',\n",
       "  'This page lists all the utility functions used by the image processors, mainly the functional transformations used to process the images.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/audio_utils',\n",
       "  'Utilities for Audio processing',\n",
       "  'This page lists all the utility functions that can be used by the audio FeatureExtractor in order to compute special features from a raw audio using common algorithms such as Short Time Fourier Transform or log mel spectrogram.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/file_utils',\n",
       "  'General Utilities',\n",
       "  'This page lists all of Transformers general utility functions that are found in the file utils.py.'],\n",
       " ['https://huggingface.co/docs/transformers/internal/time_series_utils',\n",
       "  'Utilities for Time Series',\n",
       "  'This page lists all the utility functions and classes that can be used for Time Series based models.'],\n",
       " ['https://huggingface.co/docs/diffusers/index', ' Diffusers', ''],\n",
       " ['https://huggingface.co/docs/diffusers/quicktour',\n",
       "  'Quicktour',\n",
       "  'Diffusion models are trained to denoise random Gaussian noise step-by-step to generate a sample of interest, such as an image or audio. This has sparked a tremendous amount of interest in generative AI, and you have probably seen examples of diffusion generated images on the internet.  Diffusers is a library aimed at making diffusion models widely accessible to everyone.'],\n",
       " ['https://huggingface.co/docs/diffusers/stable_diffusion',\n",
       "  'Effective and efficient diffusion',\n",
       "  'Getting the DiffusionPipeline to generate images in a certain style or include what you want can be tricky. Often times, you have to run the DiffusionPipeline several times before you end up with an image youre happy with. But generating something out of nothing is a computationally intensive process, especially if youre running inference over and over again.'],\n",
       " ['https://huggingface.co/docs/diffusers/installation',\n",
       "  'Installation',\n",
       "  ' Diffusers is tested on Python 3.8+, PyTorch 1.7.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:'],\n",
       " ['https://huggingface.co/docs/diffusers/tutorials/tutorial_overview',\n",
       "  'Overview',\n",
       "  'Welcome to  Diffusers! If youre new to diffusion models and generative AI, and want to learn more, then youve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how  Diffusers is meant to be used.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline',\n",
       "  'Understanding pipelines, models and schedulers',\n",
       "  ' Diffusers is designed to be a user-friendly and flexible toolbox for building diffusion systems tailored to your use-case. At the core of the toolbox are models and schedulers. While the DiffusionPipeline bundles these components together for convenience, you can also unbundle the pipeline and use the models and schedulers separately to create new diffusion systems.'],\n",
       " ['https://huggingface.co/docs/diffusers/tutorials/autopipeline',\n",
       "  'AutoPipeline',\n",
       "  ' Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If youre new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if youre using the runwayml/stable-diffusion-v1-5 checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the StableDiffusionImg2ImgPipeline and StableDiffusionInpaintPipeline classes respectively.'],\n",
       " ['https://huggingface.co/docs/diffusers/tutorials/basic_training',\n",
       "  'Train a diffusion model',\n",
       "  'Unconditional image generation is a popular application of diffusion models that generates images that look like those in the dataset used for training. Typically, the best results are obtained from finetuning a pretrained model on a specific dataset. You can find many of these checkpoints on the Hub, but if you cant find one you like, you can always train your own!'],\n",
       " ['https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference',\n",
       "  'Load LoRAs for inference',\n",
       "  'There are many adapter types (with LoRAs being the most popular) trained in different styles to achieve different effects. You can even combine multiple adapters to create new and unique images.'],\n",
       " ['https://huggingface.co/docs/diffusers/tutorials/fast_diffusion',\n",
       "  'Accelerate inference of text-to-image diffusion models',\n",
       "  'Diffusion models are slower than their GAN counterparts because of the iterative and sequential reverse diffusion process. There are several techniques that can address this limitation such as progressive timestep distillation (LCM LoRA), model compression (SSD-1B), and reusing adjacent features of the denoiser (DeepCache).'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/loading_overview',\n",
       "  'Overview',\n",
       "  ' Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - from_pretrained() - that loads any of these components from either the Hugging Face Hub or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/loading',\n",
       "  'Load pipelines, models, and schedulers',\n",
       "  'Having an easy way to use a diffusion system for inference is essential to  Diffusers. Diffusion systems often consist of multiple components like parameterized models, tokenizers, and schedulers that interact in complex ways. That is why we designed the DiffusionPipeline to wrap the complexity of the entire diffusion system into an easy-to-use API, while remaining flexible enough to be adapted for other use cases, such as loading each component individually as building blocks to assemble your own diffusion system.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/schedulers',\n",
       "  'Load and compare different schedulers',\n",
       "  'Diffusion pipelines are inherently a collection of diffusion models and schedulers that are partly independent from each other. This means that one is able to switch out parts of the pipeline to better customize a pipeline to ones use case. The best example of this is the Schedulers.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview',\n",
       "  'Load community pipelines and components',\n",
       "  'Community pipelines are any DiffusionPipeline class that are different from the original implementation as specified in their paper (for example, the StableDiffusionControlNetPipeline corresponds to the Text-to-Image Generation with ControlNet Conditioning paper). They provide additional functionality or extend the original implementation of a pipeline.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors',\n",
       "  'Load safetensors',\n",
       "  'safetensors is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or pickled into a .bin file with Pythons pickle utility. However, pickle is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to pickle, making it ideal for sharing model weights.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/other-formats',\n",
       "  'Load different Stable Diffusion formats',\n",
       "  'Stable Diffusion models are available in different formats depending on the framework theyre trained and saved with, and where you download them from. Converting these formats for use in  Diffusers allows you to use all the features supported by the library, such as using different schedulers for inference, building your custom pipeline, and a variety of techniques and methods for optimizing inference speed.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters',\n",
       "  'Load adapters',\n",
       "  'There are several training techniques for personalizing diffusion models to generate images of a specific subject or images in certain styles. Each of these training methods produces a different type of adapter. Some of the adapters generate an entirely new model, while other adapters only modify a smaller set of embeddings or weights. This means the loading process for each adapter is also different.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/push_to_hub',\n",
       "  'Push files to the Hub',\n",
       "  ' Diffusers provides a PushToHubMixin for uploading your model, scheduler, or pipeline to the Hub. It is an easy way to store your files on the Hub, and also allows you to share your work with others. Under the hood, the PushToHubMixin:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview',\n",
       "  'Overview',\n",
       "  'A pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like StableDiffusionXLPipeline or StableDiffusionControlNetPipeline, with specific capabilities. All pipeline types inherit from the base DiffusionPipeline class; pass it any checkpoint, and itll automatically detect the pipeline type and load the necessary components.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/unconditional_image_generation',\n",
       "  'Unconditional image generation',\n",
       "  'Unconditional image generation generates images that look like a random sample from the training data the model was trained on because the denoising process is not guided by any additional context like text or image.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation',\n",
       "  'Text-to-image',\n",
       "  'When you think of diffusion models, text-to-image is usually one of the first things that come to mind. Text-to-image generates an image from a text description (for example, Astronaut in a jungle, cold color palette, muted colors, detailed, 8k) which is also known as a prompt.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/img2img',\n",
       "  'Image-to-image',\n",
       "  'Image-to-image is similar to text-to-image, but in addition to a prompt, you can also pass an initial image as a starting point for the diffusion process. The initial image is encoded to latent space and noise is added to it. Then the latent diffusion model takes a prompt and the noisy latent image, predicts the added noise, and removes the predicted noise from the initial latent image to get the new latent image. Lastly, a decoder decodes the new latent image back into an image.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/inpaint',\n",
       "  'Inpainting',\n",
       "  'Inpainting replaces or edits specific areas of an image. This makes it a useful tool for image restoration like removing defects and artifacts, or even replacing an image area with something entirely new. Inpainting relies on a mask to determine which regions of an image to fill in; the area to inpaint is represented by white pixels and the area to keep is represented by black pixels. The white pixels are filled in by the prompt.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/depth2img',\n",
       "  'Depth-to-image',\n",
       "  'The StableDiffusionDepth2ImgPipeline lets you pass a text prompt and an initial image to condition the generation of new images. In addition, you can also pass a depth_map to preserve the image structure. If no depth_map is provided, the pipeline automatically predicts the depth via an integrated depth-estimation model.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/textual_inversion_inference',\n",
       "  'Textual inversion',\n",
       "  'The StableDiffusionPipeline supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the Stable Diffusion Conceptualizer.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/distributed_inference',\n",
       "  'Distributed inference with multiple GPUs',\n",
       "  'On distributed setups, you can run inference across multiple GPUs with  Accelerate or PyTorch Distributed, which is useful for generating with multiple prompts in parallel.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/reusing_seeds',\n",
       "  'Improve image quality with deterministic generation',\n",
       "  'A common way to improve the quality of generated images is with deterministic batch generation, generate a batch of images and select one image to improve with a more detailed prompt in a second round of inference. The key is to pass a list of torch.Generators to the pipeline for batched image generation, and tie each Generator to a seed so you can reuse it for an image.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/control_brightness',\n",
       "  'Control image brightness',\n",
       "  'The Stable Diffusion pipeline is mediocre at generating images that are either very bright or dark as explained in the Common Diffusion Noise Schedules and Sample Steps are Flawed paper. The solutions proposed in the paper are currently implemented in the DDIMScheduler which you can use to improve the lighting in your images.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/weighted_prompts',\n",
       "  'Prompt weighting',\n",
       "  'Prompt weighting provides a way to emphasize or de-emphasize certain parts of a prompt, allowing for more control over the generated image. A prompt can include several concepts, which gets turned into contextualized text embeddings. The embeddings are used by the model to condition its cross-attention layers to generate an image (read the Stable Diffusion blog post to learn more about how it works).'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/freeu',\n",
       "  'Improve generation quality with FreeU',\n",
       "  'The UNet is responsible for denoising during the reverse diffusion process, and there are two distinct features in its architecture:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview',\n",
       "  'Overview',\n",
       "  'A pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like StableDiffusionXLPipeline or StableDiffusionControlNetPipeline, with specific capabilities. All pipeline types inherit from the base DiffusionPipeline class; pass it any checkpoint, and itll automatically detect the pipeline type and load the necessary components.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/sdxl',\n",
       "  'Stable Diffusion XL',\n",
       "  'Stable Diffusion XL (SDXL) is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo',\n",
       "  'SDXL Turbo',\n",
       "  'SDXL Turbo is an adversarial time-distilled Stable Diffusion XL (SDXL) model capable of running inference in as little as 1 step.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/kandinsky',\n",
       "  'Kandinsky',\n",
       "  'The Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/controlnet',\n",
       "  'ControlNet',\n",
       "  'ControlNet is a type of model for controlling image diffusion models by conditioning the model with an additional input image. There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model. This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/shap-e',\n",
       "  'Shap-E',\n",
       "  'Shap-E is a conditional model for generating 3D assets which could be used for video game development, interior design, and architecture. It is trained on a large dataset of 3D assets, and post-processed to render more views of each object and produce 16K instead of 4K point clouds. The Shap-E model is trained in two steps:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/diffedit',\n",
       "  'DiffEdit',\n",
       "  'Image editing typically requires providing a mask of the area to be edited. DiffEdit automatically generates the mask for you based on a text query, making it easier overall to create a mask without image editing software. The DiffEdit algorithm works in three steps:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/distilled_sd',\n",
       "  'Distilled Stable Diffusion inference',\n",
       "  'Stable Diffusion inference can be a computationally intensive process because it must iteratively denoise the latents to generate an image. To reduce the computational burden, you can use a distilled version of the Stable Diffusion model from Nota AI. The distilled version of their Stable Diffusion model eliminates some of the residual and attention blocks from the UNet, reducing the model size by 51% and improving latency on CPU/GPU by 43%.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/callback',\n",
       "  'Pipeline callbacks',\n",
       "  'The denoising loop of a pipeline can be modified with custom defined functions using the callback_on_step_end parameter. The callback function is executed at the end of each step, and modifies the pipeline attributes and variables for the next step. This is really useful for dynamically adjusting certain pipeline attributes or modifying tensor variables. This versatility allows for interesting use-cases such as changing the prompt embeddings at each timestep, assigning different weights to the prompt embeddings, and editing the guidance scale. With callbacks, you can implement new features without modifying the underlying code!'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/reproducibility',\n",
       "  'Create reproducible pipelines',\n",
       "  'Reproducibility is important for testing, replicating results, and can even be used to improve image quality. However, the randomness in diffusion models is a desired property because it allows the pipeline to generate different images every time it is run. While you cant expect to get the exact same results across platforms, you can expect results to be reproducible across releases and platforms within a certain tolerance range. Even then, tolerance varies depending on the diffusion pipeline and checkpoint.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_examples',\n",
       "  'Community pipelines',\n",
       "  'For more context about the design choices behind community pipelines, please have a look at this issue.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/contribute_pipeline',\n",
       "  'Contribute a community pipeline',\n",
       "  ' Take a look at GitHub Issue #841 for more context about why were adding community pipelines to help everyone easily share their work without being slowed down.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora',\n",
       "  'Latent Consistency Model-LoRA',\n",
       "  'Latent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm',\n",
       "  'Latent Consistency Model',\n",
       "  'Latent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/svd',\n",
       "  'Stable Video Diffusion',\n",
       "  'Stable Video Diffusion (SVD) is a powerful image-to-video generation model that can generate 2-4 second high resolution (576x1024) videos conditioned on an input image.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/overview',\n",
       "  'Overview',\n",
       "  ' Diffusers provides a collection of training scripts for you to train your own diffusion models. You can find all of our training scripts in diffusers/examples.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/create_dataset',\n",
       "  'Create a dataset for training',\n",
       "  'There are many datasets on the Hub to train a model on, but if you cant find one youre interested in or want to use your own, you can create a dataset with the  Datasets library. The dataset structure depends on the task you want to train your model on. The most basic dataset structure is a directory of images for tasks like unconditional image generation. Another dataset structure may be a directory of images and a text file containing their corresponding text captions for tasks like text-to-image generation.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/adapt_a_model',\n",
       "  'Adapt a model to a new task',\n",
       "  'Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/unconditional_training',\n",
       "  'Unconditional image generation',\n",
       "  'Unconditional image generation models are not conditioned on text or images during training. It only generates images that resemble its training data distribution.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/text2image',\n",
       "  'Text-to-image',\n",
       "  'The text-to-image script is experimental, and its easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/sdxl',\n",
       "  'Stable Diffusion XL',\n",
       "  'This script is experimental, and its easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/kandinsky',\n",
       "  'Kandinsky 2.2',\n",
       "  'This script is experimental, and its easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/wuerstchen',\n",
       "  'Wuerstchen',\n",
       "  'The Wuerstchen model drastically reduces computational costs by compressing the latent space by 42x, without compromising image quality and accelerating inference. During training, Wuerstchen uses two models (VQGAN + autoencoder) to compress the latents, and then a third model (text-conditioned latent diffusion model) is conditioned on this highly compressed space to generate an image.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/controlnet',\n",
       "  'ControlNet',\n",
       "  'ControlNet models are adapters trained on top of another pretrained model. It allows for a greater degree of control over image generation by conditioning the model with an additional input image. The input image can be a canny edge, depth map, human pose, and many more.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/t2i_adapters',\n",
       "  'T2I-Adapters',\n",
       "  'T2I-Adapter is a lightweight adapter model that provides an additional conditioning input image (line art, canny, sketch, depth, pose) to better control image generation. It is similar to a ControlNet, but it is a lot smaller (~77M parameters and ~300MB file size) because its only inserts weights into the UNet instead of copying and training it.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/instructpix2pix',\n",
       "  'InstructPix2Pix',\n",
       "  'InstructPix2Pix is a Stable Diffusion model trained to edit images from human-provided instructions. For example, your prompt can be turn the clouds rainy and the model will edit the input image accordingly. This model is conditioned on the text prompt (or editing instruction) and the input image.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/text_inversion',\n",
       "  'Textual Inversion',\n",
       "  'Textual Inversion is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/dreambooth',\n",
       "  'DreamBooth',\n",
       "  'DreamBooth is a training technique that updates the entire diffusion model by training on just a few images of a subject or style. It works by associating a special word in the prompt with the example images.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/lora',\n",
       "  'LoRA',\n",
       "  'This is experimental and the API may change in the future.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/custom_diffusion',\n",
       "  'Custom Diffusion',\n",
       "  'Custom Diffusion is a training technique for personalizing image generation models. Like Textual Inversion, DreamBooth, and LoRA, Custom Diffusion only requires a few (~4-5) example images. This technique works by only training weights in the cross-attention layers, and it uses a special word to represent the newly learned concept. Custom Diffusion is unique because it can also learn multiple concepts at the same time.'],\n",
       " ['https://huggingface.co/docs/diffusers/training/lcm_distill',\n",
       "  'Latent Consistency Distillation',\n",
       "  'Latent Consistency Models (LCMs) are able to generate high-quality images in just a few steps, representing a big leap forward because many pipelines require at least 25+ steps. LCMs are produced by applying the latent consistency distillation method to any Stable Diffusion model. This method works by applying one-stage guided distillation to the latent space, and incorporating a skipping-step method to consistently skip timesteps to accelerate the distillation process (refer to section 4.1, 4.2, and 4.3 of the paper for more details).'],\n",
       " ['https://huggingface.co/docs/diffusers/training/ddpo',\n",
       "  'Reinforcement learning training with DDPO',\n",
       "  'You can fine-tune Stable Diffusion on a reward function via reinforcement learning with the  TRL library and  Diffusers. This is done with the Denoising Diffusion Policy Optimization (DDPO) algorithm introduced by Black et al. in Training Diffusion Models with Reinforcement Learning, which is implemented in  TRL with the DDPOTrainer.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/other-modalities',\n",
       "  'Other Modalities',\n",
       "  'Diffusers is in the process of expanding to modalities other than images.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/opt_overview',\n",
       "  'Overview',\n",
       "  'Generating high-quality outputs is computationally intensive, especially during each iterative step where you go from a noisy output to a less noisy output. One of  Diffusers goals is to make this technology widely accessible to everyone, which includes enabling fast inference on consumer and specialized hardware.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/fp16',\n",
       "  'Speed up inference',\n",
       "  'There are several ways to optimize  Diffusers for inference speed. As a general rule of thumb, we recommend using either xFormers or torch.nn.functional.scaled_dot_product_attention in PyTorch 2.0 for their memory-efficient attention.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/memory',\n",
       "  'Reduce memory usage',\n",
       "  'A barrier to using diffusion models is the large amount of memory required. To overcome this challenge, there are several memory-reducing techniques you can use to run even some of the largest models on free-tier or consumer GPUs. Some of these techniques can even be combined to further reduce memory usage.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/torch2.0',\n",
       "  'PyTorch 2.0',\n",
       "  ' Diffusers supports the latest optimizations from PyTorch 2.0 which include:'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/xformers',\n",
       "  'xFormers',\n",
       "  'We recommend xFormers for both inference and training. In our tests, the optimizations performed in the attention blocks allow for both faster speed and reduced memory consumption.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/tome',\n",
       "  'Token merging',\n",
       "  'Token merging (ToMe) merges redundant tokens/patches progressively in the forward pass of a Transformer-based network which can speed-up the inference latency of StableDiffusionPipeline.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/deepcache',\n",
       "  'DeepCache',\n",
       "  'DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture.'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to',\n",
       "  'DeepCache',\n",
       "  'DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/onnx',\n",
       "  'DeepCache',\n",
       "  'DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/open_vino',\n",
       "  'DeepCache',\n",
       "  'DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/coreml',\n",
       "  'Core ML',\n",
       "  'Core ML is the model format and machine learning library supported by Apple frameworks. If you are interested in running Stable Diffusion models inside your macOS or iOS/iPadOS apps, this guide will show you how to convert existing PyTorch checkpoints into the Core ML format and use them for inference with Python or Swift.'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/mps',\n",
       "  'Metal Performance Shaders (MPS)',\n",
       "  ' Diffusers is compatible with Apple silicon (M1/M2 chips) using the PyTorch mps device, which uses the Metal framework to leverage the GPU on MacOS devices. Youll need to have:'],\n",
       " ['https://huggingface.co/docs/diffusers/optimization/habana',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/conceptual/philosophy',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/conceptual/contribution',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/conceptual/ethical_guidelines',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/conceptual/evaluation',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/configuration',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/logging',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/outputs',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/ip_adapter',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/lora',\n",
       "  'Habana Gaudi',\n",
       "  ' Diffusers is compatible with Habana Gaudi through  Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/single_file',\n",
       "  'Single files',\n",
       "  'Diffusers supports loading pretrained pipeline (or model) weights stored in a single file, such as a ckpt or safetensors file. These single file types are typically produced from community trained models. There are three classes for loading single file weights:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/textual_inversion',\n",
       "  'Textual Inversion',\n",
       "  'Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/unet',\n",
       "  'UNet',\n",
       "  'Some training methods - like LoRA and Custom Diffusion - typically target the UNets attention layers, but these training methods can also target other non-attention layers. Instead of training all of a models parameters, only a subset of the parameters are trained, which is faster and more efficient. This class is useful if youre only loading weights into a UNet. If you need to load weights into the text encoder or a text encoder and UNet, try using the load_lora_weights() function instead.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/loaders/peft',\n",
       "  'PEFT',\n",
       "  'Diffusers supports loading adapters such as LoRA with the PEFT library with the PeftAdapterMixin class. This allows modeling classes in Diffusers like UNet2DConditionModel to load an adapter.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/overview',\n",
       "  'Overview',\n",
       "  ' Diffusers provides pretrained models for popular algorithms and modules to create custom diffusion systems. The primary function of models is to denoise an input sample as modeled by the distribution\\np\\n\\n(x\\nt1\\nx\\nt\\n).'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/unet',\n",
       "  'UNet1DModel',\n",
       "  'The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in  Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in  Diffusers, depending on its number of dimensions and whether it is a conditional model or not. This is a 1D UNet model.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/unet2d',\n",
       "  'UNet2DModel',\n",
       "  'The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in  Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in  Diffusers, depending on its number of dimensions and whether it is a conditional model or not. This is a 2D UNet model.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/unet2d-cond',\n",
       "  'UNet2DConditionModel',\n",
       "  'The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in  Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in  Diffusers, depending on its number of dimensions and whether it is a conditional model or not. This is a 2D UNet conditional model.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/unet3d-cond',\n",
       "  'UNet3DConditionModel',\n",
       "  'The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in  Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in  Diffusers, depending on its number of dimensions and whether it is a conditional model or not. This is a 3D UNet conditional model.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/unet-motion',\n",
       "  'UNetMotionModel',\n",
       "  'The UNet model was originally introduced by Ronneberger et al for biomedical image segmentation, but it is also commonly used in  Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in  Diffusers, depending on its number of dimensions and whether it is a conditional model or not. This is a 2D UNet model.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/uvit2d',\n",
       "  'UViT2DModel',\n",
       "  'The U-ViT model is a vision transformer (ViT) based UNet. This model incorporates elements from ViT (considers all inputs such as time, conditions and noisy image patches as tokens) and a UNet (long skip connections between the shallow and deep layers). The skip connection is important for predicting pixel-level features. An additional 3x3 convolutional block is applied prior to the final output to improve image quality.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/vq',\n",
       "  'VQModel',\n",
       "  'The VQ-VAE model was introduced in Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray Kavukcuoglu. The model is used in  Diffusers to decode latent representations into images. Unlike AutoencoderKL, the VQModel works in a quantized latent space.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/autoencoderkl',\n",
       "  'AutoencoderKL',\n",
       "  'The variational autoencoder (VAE) model with KL loss was introduced in Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling. The model is used in  Diffusers to encode images into latents and to decode latent representations into images.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl',\n",
       "  'AsymmetricAutoencoderKL',\n",
       "  'Improved larger variational autoencoder (VAE) model with KL loss for inpainting task: Designing a Better Asymmetric VQGAN for StableDiffusion by Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/autoencoder_tiny',\n",
       "  'Tiny AutoEncoder',\n",
       "  'Tiny AutoEncoder for Stable Diffusion (TAESD) was introduced in madebyollin/taesd by Ollin Boer Bohan. It is a tiny distilled version of Stable Diffusions VAE that can quickly decode the latents in a StableDiffusionPipeline or StableDiffusionXLPipeline almost instantly.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/consistency_decoder_vae',\n",
       "  'ConsistencyDecoderVAE',\n",
       "  'Consistency decoder can be used to decode the latents from the denoising UNet in the StableDiffusionPipeline. This decoder was introduced in the DALL-E 3 technical report.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/transformer2d',\n",
       "  'Transformer2D',\n",
       "  'A Transformer model for image-like data from CompVis that is based on the Vision Transformer introduced by Dosovitskiy et al. The Transformer2DModel accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/transformer_temporal',\n",
       "  'Transformer Temporal',\n",
       "  'A Transformer model for video-like data.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/prior_transformer',\n",
       "  'Prior Transformer',\n",
       "  'The Prior Transformer was originally introduced in Hierarchical Text-Conditional Image Generation with CLIP Latents by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/models/controlnet',\n",
       "  'ControlNet',\n",
       "  'The ControlNet model was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, Maneesh Agrawala. It provides a greater degree of control over text-to-image generation by conditioning the model on additional inputs such as edge maps, depth maps, segmentation maps, and keypoints for pose detection.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/overview',\n",
       "  'Overview',\n",
       "  'Pipelines provide a simple way to run state-of-the-art diffusion models in inference by bundling all of the necessary components (multiple independently-trained models, schedulers, and processors) into a single end-to-end class. Pipelines are flexible and they can be adapted to use different schedulers or even model components.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/amused',\n",
       "  'aMUSEd',\n",
       "  'aMUSEd was introduced in aMUSEd: An Open MUSE Reproduction by Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/animatediff',\n",
       "  'AnimateDiff',\n",
       "  'AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning by Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/attend_and_excite',\n",
       "  'Attend-and-Excite',\n",
       "  'Attend-and-Excite for Stable Diffusion was proposed in Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models and provides textual attention control over image generation.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/audioldm',\n",
       "  'AudioLDM',\n",
       "  'AudioLDM was proposed in AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al. Inspired by Stable Diffusion, AudioLDM is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional sound effects, human speech and music.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/audioldm2',\n",
       "  'AudioLDM 2',\n",
       "  'AudioLDM 2 was proposed in AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining by Haohe Liu et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional sound effects, human speech and music.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/auto_pipeline',\n",
       "  'AutoPipeline',\n",
       "  'AutoPipeline is designed to:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion',\n",
       "  'BLIP-Diffusion',\n",
       "  'BLIP-Diffusion was proposed in BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. It enables zero-shot subject-driven generation and control-guided zero-shot generation.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/consistency_models',\n",
       "  'Consistency Models',\n",
       "  'Consistency Models were proposed in Consistency Models by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/controlnet',\n",
       "  'ControlNet',\n",
       "  'ControlNet was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl',\n",
       "  'ControlNet with Stable Diffusion XL',\n",
       "  'ControlNet was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/dance_diffusion',\n",
       "  'Dance Diffusion',\n",
       "  'Dance Diffusion is by Zach Evans.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/ddim',\n",
       "  'DDIM',\n",
       "  'Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/ddpm',\n",
       "  'DDPM',\n",
       "  'Denoising Diffusion Probabilistic Models (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the  Diffusers library, DDPM refers to the discrete denoising scheduler from the paper as well as the pipeline.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if',\n",
       "  'DeepFloyd IF',\n",
       "  'DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/diffedit',\n",
       "  'DiffEdit',\n",
       "  'DiffEdit: Diffusion-based semantic image editing with mask guidance is by Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/dit',\n",
       "  'DiT',\n",
       "  'Scalable Diffusion Models with Transformers (DiT) is by William Peebles and Saining Xie.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl',\n",
       "  'I2VGen-XL',\n",
       "  'I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models by Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/pix2pix',\n",
       "  'InstructPix2Pix',\n",
       "  'InstructPix2Pix: Learning to Follow Image Editing Instructions is by Tim Brooks, Aleksander Holynski and Alexei A. Efros.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/kandinsky',\n",
       "  'Kandinsky 2.1',\n",
       "  'Kandinsky 2.1 is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22',\n",
       "  'Kandinsky 2.2',\n",
       "  'Kandinsky 2.2 is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3',\n",
       "  'Kandinsky 3',\n",
       "  'Kandinsky 3 is created by Vladimir Arkhipkin,Anastasia Maltseva,Igor Pavlov,Andrei Filatov,Arseniy Shakhmatov,Andrey Kuznetsov,Denis Dimitrov, Zein Shaheen'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models',\n",
       "  'Latent Consistency Models',\n",
       "  'Latent Consistency Models (LCMs) were proposed in Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion',\n",
       "  'Latent Diffusion',\n",
       "  'Latent Diffusion was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjrn Ommer.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/panorama',\n",
       "  'MultiDiffusion',\n",
       "  'MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/musicldm',\n",
       "  'MusicLDM',\n",
       "  'MusicLDM was proposed in MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov. MusicLDM takes a text prompt as input and predicts the corresponding music sample.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/paint_by_example',\n",
       "  'Paint by Example',\n",
       "  'Paint by Example: Exemplar-based Image Editing with Diffusion Models is by Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/pia',\n",
       "  'Personalized Image Animator (PIA)',\n",
       "  'PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models by Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/pixart',\n",
       "  'PixArt-',\n",
       "  ''],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/self_attention_guidance',\n",
       "  'Self-Attention Guidance',\n",
       "  'Improving Sample Quality of Diffusion Models Using Self-Attention Guidance is by Susung Hong et al.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion',\n",
       "  'Semantic Guidance',\n",
       "  'Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Text-to-Image Models using Semantic Guidance and provides strong semantic control over image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, while staying true to the original image composition.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/shap_e',\n",
       "  'Shap-E',\n",
       "  'The Shap-E model was proposed in Shap-E: Generating Conditional 3D Implicit Functions by Alex Nichol and Heewoo Jun from OpenAI.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview',\n",
       "  'Overview',\n",
       "  'Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjrn Ommer.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img',\n",
       "  'Text-to-image',\n",
       "  'The Stable Diffusion model was created by researchers and engineers from CompVis, Stability AI, Runway, and LAION. The StableDiffusionPipeline is capable of generating photorealistic images given any text input. Its trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bjrn Ommer.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img',\n",
       "  'Image-to-image',\n",
       "  'The Stable Diffusion model can also be applied to image-to-image generation by passing a text prompt and an initial image to condition the generation of new images.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint',\n",
       "  'Inpainting',\n",
       "  'The Stable Diffusion model can also be applied to inpainting which lets you edit specific parts of an image by providing a mask and a text prompt using Stable Diffusion.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img',\n",
       "  'Depth-to-image',\n",
       "  'The Stable Diffusion model can also infer depth based on an image using MiDaS. This allows you to pass a text prompt and an initial image to condition the generation of new images as well as a depth_map to preserve the image structure.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation',\n",
       "  'Image variation',\n",
       "  'The Stable Diffusion model can also generate variations from an input image. It uses a fine-tuned version of a Stable Diffusion model by Justin Pinkney from Lambda.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_safe',\n",
       "  'Safe Stable Diffusion',\n",
       "  'Safe Stable Diffusion was proposed in Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models and mitigates inappropriate degeneration from Stable Diffusion models because theyre trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2',\n",
       "  'Stable Diffusion 2',\n",
       "  'Stable Diffusion 2 is a text-to-image latent diffusion model built upon the work of the original Stable Diffusion, and it was led by Robin Rombach and Katherine Crowson from Stability AI and LAION.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl',\n",
       "  'Stable Diffusion XL',\n",
       "  'Stable Diffusion XL (SDXL) was proposed in SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis by Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/sdxl_turbo',\n",
       "  'SDXL Turbo',\n",
       "  'Stable Diffusion XL (SDXL) Turbo was proposed in Adversarial Diffusion Distillation by Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale',\n",
       "  'Latent upscaler',\n",
       "  'The Stable Diffusion latent upscaler model was created by Katherine Crowson in collaboration with Stability AI. It is used to enhance the output image resolution by a factor of 2 (see this demo notebook for a demonstration of the original implementation).'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale',\n",
       "  'Super-resolution',\n",
       "  'The Stable Diffusion upscaler diffusion model was created by the researchers and engineers from CompVis, Stability AI, and LAION. It is used to enhance the resolution of input images by a factor of 4.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/k_diffusion',\n",
       "  'K-Diffusion',\n",
       "  'k-diffusion is a popular library created by Katherine Crowson. We provide StableDiffusionKDiffusionPipeline and StableDiffusionXLKDiffusionPipeline that allow you to run Stable DIffusion with samplers from k-diffusion.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/ldm3d_diffusion',\n",
       "  'LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler',\n",
       "  'LDM3D was proposed in LDM3D: Latent Diffusion Model for 3D by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal. LDM3D generates an image and a depth map from a given text prompt unlike the existing text-to-image diffusion models such as Stable Diffusion which only generates an image. With almost the same number of parameters, LDM3D achieves to create a latent space that can compress both the RGB images and the depth maps.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter',\n",
       "  'Stable Diffusion T2I-Adapter',\n",
       "  'T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models by Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/gligen',\n",
       "  'GLIGEN (Grounded Language-to-Image Generation)',\n",
       "  'The GLIGEN model was created by researchers and engineers from University of Wisconsin-Madison, Columbia University, and Microsoft. The StableDiffusionGLIGENPipeline and StableDiffusionGLIGENTextImagePipeline can generate photorealistic images conditioned on grounding inputs. Along with text and bounding boxes with StableDiffusionGLIGENPipeline, if input images are given, StableDiffusionGLIGENTextImagePipeline can insert objects described by text at the region defined by bounding boxes. Otherwise, itll generate an image described by the caption/prompt and insert objects described by text at the region defined by bounding boxes. Its trained on COCO2014D and COCO2014CD datasets, and the model uses a frozen CLIP ViT-L/14 text encoder to condition itself on grounding inputs.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip',\n",
       "  'Stable unCLIP',\n",
       "  'Stable unCLIP checkpoints are finetuned from Stable Diffusion 2.1 checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used for text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/text_to_video',\n",
       "  'Text-to-video',\n",
       "  ' This pipeline is for research purposes only.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero',\n",
       "  'Text2Video-Zero',\n",
       "  'Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators is by Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/unclip',\n",
       "  'unCLIP',\n",
       "  'Hierarchical Text-Conditional Image Generation with CLIP Latents is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in  Diffusers comes from kakaobrains karlo.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser',\n",
       "  'UniDiffuser',\n",
       "  'The UniDiffuser model was proposed in One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale by Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling',\n",
       "  'Value-guided sampling',\n",
       "  ' This is an experimental pipeline for reinforcement learning!'],\n",
       " ['https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen',\n",
       "  'Wuerstchen',\n",
       "  'Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models is by Pablo Pernias, Dominic Rampas, Mats L. Richter and Christopher Pal and Marc Aubreville.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/overview',\n",
       "  'Overview',\n",
       "  ' Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a models output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward n timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be discrete in which case the timestep is an int or continuous in which case the timestep is a float.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/cm_stochastic_iterative',\n",
       "  'CMStochasticIterativeScheduler',\n",
       "  'Consistency Models by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever introduced a multistep and onestep scheduler (Algorithm 1) that is capable of generating good samples in one or a small number of steps.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/consistency_decoder',\n",
       "  'ConsistencyDecoderScheduler',\n",
       "  'This scheduler is a part of the ConsistencyDecoderPipeline and was introduced in DALL-E 3.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/ddim_inverse',\n",
       "  'DDIMInverseScheduler',\n",
       "  'DDIMInverseScheduler is the inverted scheduler from Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon. The implementation is mostly based on the DDIM inversion definition from Null-text Inversion for Editing Real Images using Guided Diffusion Models.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/ddim',\n",
       "  'DDIMScheduler',\n",
       "  'Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/ddpm',\n",
       "  'DDPMScheduler',\n",
       "  'Denoising Diffusion Probabilistic Models (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the context of the  Diffusers library, DDPM refers to the discrete denoising scheduler from the paper as well as the pipeline.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/deis',\n",
       "  'DEISMultistepScheduler',\n",
       "  'Diffusion Exponential Integrator Sampler (DEIS) is proposed in Fast Sampling of Diffusion Models with Exponential Integrator by Qinsheng Zhang and Yongxin Chen. DEISMultistepScheduler is a fast high order solver for diffusion ordinary differential equations (ODEs).'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver_inverse',\n",
       "  'DPMSolverMultistepInverse',\n",
       "  'DPMSolverMultistepInverse is the inverted scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver',\n",
       "  'DPMSolverMultistepScheduler',\n",
       "  'DPMSolverMultistep is a multistep scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/dpm_sde',\n",
       "  'DPMSolverSDEScheduler',\n",
       "  'The DPMSolverSDEScheduler is inspired by the stochastic sampler from the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/singlestep_dpm_solver',\n",
       "  'DPMSolverSinglestepScheduler',\n",
       "  'DPMSolverSinglestepScheduler is a single step scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/euler_ancestral',\n",
       "  'EulerAncestralDiscreteScheduler',\n",
       "  'A scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original k-diffusion implementation by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/euler',\n",
       "  'EulerDiscreteScheduler',\n",
       "  'The Euler scheduler (Algorithm 2) is from the Elucidating the Design Space of Diffusion-Based Generative Models paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original k-diffusion implementation by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/heun',\n",
       "  'HeunDiscreteScheduler',\n",
       "  'The Heun scheduler (Algorithm 1) is from the Elucidating the Design Space of Diffusion-Based Generative Models paper by Karras et al. The scheduler is ported from the k-diffusion library and created by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/ipndm',\n",
       "  'IPNDMScheduler',\n",
       "  'IPNDMScheduler is a fourth-order Improved Pseudo Linear Multistep scheduler. The original implementation can be found at crowsonkb/v-diffusion-pytorch.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/stochastic_karras_ve',\n",
       "  'KarrasVeScheduler',\n",
       "  'KarrasVeScheduler is a stochastic sampler tailored to variance-expanding (VE) models. It is based on the Elucidating the Design Space of Diffusion-Based Generative Models and Score-based generative modeling through stochastic differential equations papers.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete_ancestral',\n",
       "  'KDPM2AncestralDiscreteScheduler',\n",
       "  'The KDPM2DiscreteScheduler with ancestral sampling is inspired by the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete',\n",
       "  'KDPM2DiscreteScheduler',\n",
       "  'The KDPM2DiscreteScheduler is inspired by the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/lcm',\n",
       "  'LCMScheduler',\n",
       "  'Multistep and onestep scheduler (Algorithm 3) introduced alongside latent consistency models in the paper Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. This scheduler should be able to generate good samples from LatentConsistencyModelPipeline in 1-8 steps.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/lms_discrete',\n",
       "  'LMSDiscreteScheduler',\n",
       "  'LMSDiscreteScheduler is a linear multistep scheduler for discrete beta schedules. The scheduler is ported from and created by Katherine Crowson, and the original implementation can be found at crowsonkb/k-diffusion.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/pndm',\n",
       "  'PNDMScheduler',\n",
       "  'PNDMScheduler, or pseudo numerical methods for diffusion models, uses more advanced ODE integration techniques like the Runge-Kutta and linear multi-step method. The original implementation can be found at crowsonkb/k-diffusion.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/repaint',\n",
       "  'RePaintScheduler',\n",
       "  'RePaintScheduler is a DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks. It is designed to be used with the RePaintPipeline, and it is based on the paper RePaint: Inpainting using Denoising Diffusion Probabilistic Models by Andreas Lugmayr et al.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/score_sde_ve',\n",
       "  'ScoreSdeVeScheduler',\n",
       "  'ScoreSdeVeScheduler is a variance exploding stochastic differential equation (SDE) scheduler. It was introduced in the Score-Based Generative Modeling through Stochastic Differential Equations paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/score_sde_vp',\n",
       "  'ScoreSdeVpScheduler',\n",
       "  'ScoreSdeVpScheduler is a variance preserving stochastic differential equation (SDE) scheduler. It was introduced in the Score-Based Generative Modeling through Stochastic Differential Equations paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/unipc',\n",
       "  'UniPCMultistepScheduler',\n",
       "  'UniPCMultistepScheduler is a training-free framework designed for fast sampling of diffusion models. It was introduced in UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models by Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion',\n",
       "  'VQDiffusionScheduler',\n",
       "  'VQDiffusionScheduler converts the transformer models output into a sample for the unnoised image at the previous diffusion timestep. It was introduced in Vector Quantized Diffusion Model for Text-to-Image Synthesis by Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/internal_classes_overview',\n",
       "  'Overview',\n",
       "  'The APIs in this section are more experimental and prone to breaking changes. Most of them are used internally for development, but they may also be useful to you if youre interested in building a diffusion model with some custom parts or if youre interested in some of our helper utilities for working with  Diffusers.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/attnprocessor',\n",
       "  'Attention Processor',\n",
       "  'An attention processor is a class for applying different types of attention mechanisms.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/activations',\n",
       "  'Custom activation functions',\n",
       "  'Customized activation functions for supporting various models in  Diffusers.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/normalization',\n",
       "  'Custom normalization layers',\n",
       "  'Customized normalization layers for supporting various models in  Diffusers.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/utilities',\n",
       "  'Utilities',\n",
       "  'Utility and helper functions for working with  Diffusers.'],\n",
       " ['https://huggingface.co/docs/diffusers/api/image_processor',\n",
       "  'VAE Image Processor',\n",
       "  'The VaeImageProcessor provides a unified API for StableDiffusionPipelines to prepare image inputs for VAE encoding and post-processing outputs once theyre decoded. This includes transformations such as resizing, normalization, and conversion between PIL Image, PyTorch, and NumPy arrays.'],\n",
       " ['https://huggingface.co/docs/datasets/index',\n",
       "  ' Datasets',\n",
       "  ' Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.'],\n",
       " ['https://huggingface.co/docs/datasets/quickstart',\n",
       "  'Quickstart',\n",
       "  'This quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate  Datasets into their model training workflow. If youre a beginner, we recommend starting with our tutorials, where youll get a more thorough introduction.'],\n",
       " ['https://huggingface.co/docs/datasets/installation',\n",
       "  'Installation',\n",
       "  'Before you start, youll need to setup your environment and install the appropriate packages.  Datasets is tested on Python 3.7+.'],\n",
       " ['https://huggingface.co/docs/datasets/tutorial',\n",
       "  'Overview',\n",
       "  'Welcome to the  Datasets tutorials! These beginner-friendly tutorials will guide you through the fundamentals of working with  Datasets. Youll load and prepare a dataset for training with your machine learning framework of choice. Along the way, youll learn how to load different dataset configurations and splits, interact with and see whats inside your dataset, preprocess, and share a dataset to the Hub.'],\n",
       " ['https://huggingface.co/docs/datasets/load_hub',\n",
       "  'Load a dataset from the Hub',\n",
       "  'Finding high-quality datasets that are reproducible and accessible can be difficult. One of  Datasets main goals is to provide a simple way to load a dataset of any format or type. The easiest way to get started is to discover an existing dataset on the Hugging Face Hub - a community-driven collection of datasets for tasks in NLP, computer vision, and audio - and use  Datasets to download and generate the dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/access',\n",
       "  'Know your dataset',\n",
       "  'There are two types of dataset objects, a regular Dataset and then an  IterableDataset . A Dataset provides fast random access to the rows, and memory-mapping so that loading even large datasets only uses a relatively small amount of device memory. But for really, really big datasets that wont even fit on disk or in memory, an IterableDataset allows you to access and use the dataset without waiting for it to download completely!'],\n",
       " ['https://huggingface.co/docs/datasets/use_dataset',\n",
       "  'Preprocess',\n",
       "  'In addition to loading datasets,  Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework.'],\n",
       " ['https://huggingface.co/docs/datasets/metrics',\n",
       "  'Evaluate predictions',\n",
       "  'Metrics is deprecated in  Datasets. To learn more about how to use metrics, take a look at the library  Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets.'],\n",
       " ['https://huggingface.co/docs/datasets/create_dataset',\n",
       "  'Create a dataset',\n",
       "  'Sometimes, you may need to create a dataset if youre working with your own data. Creating a dataset with  Datasets confers all the advantages of the library to your dataset: fast loading and processing, stream enormous datasets, memory-mapping, and more. You can easily and rapidly create a dataset with  Datasets low-code approaches, reducing the time it takes to start training a model. In many cases, it is as easy as dragging and dropping your data files into a dataset repository on the Hub.'],\n",
       " ['https://huggingface.co/docs/datasets/upload_dataset',\n",
       "  'Share a dataset to the Hub',\n",
       "  'The Hub is home to an extensive collection of community-curated and popular research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!'],\n",
       " ['https://huggingface.co/docs/datasets/how_to',\n",
       "  'Overview',\n",
       "  'The how-to guides offer a more comprehensive overview of all the tools  Datasets offers and how to use them. This will help you tackle messier real-world datasets where you may need to manipulate the dataset structure or content to get it ready for training.'],\n",
       " ['https://huggingface.co/docs/datasets/loading',\n",
       "  'Load',\n",
       "  'Your data can be stored in various places; they can be on your local machines disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored,  Datasets can help you load it.'],\n",
       " ['https://huggingface.co/docs/datasets/process',\n",
       "  'Process',\n",
       "  ' Datasets provides many tools for modifying the structure and content of a dataset. These tools are important for tidying up a dataset, creating additional columns, converting between features and formats, and much more.'],\n",
       " ['https://huggingface.co/docs/datasets/stream',\n",
       "  'Stream',\n",
       "  'Dataset streaming lets you work with a dataset without downloading it. The data is streamed as you iterate over the dataset. This is especially helpful when:'],\n",
       " ['https://huggingface.co/docs/datasets/use_with_tensorflow',\n",
       "  'Use with TensorFlow',\n",
       "  'This document is a quick introduction to using datasets with TensorFlow, with a particular focus on how to get tf.Tensor objects out of our datasets, and how to stream data from Hugging Face Dataset objects to Keras methods like model.fit().'],\n",
       " ['https://huggingface.co/docs/datasets/use_with_pytorch',\n",
       "  'Use with PyTorch',\n",
       "  'This document is a quick introduction to using datasets with PyTorch, with a particular focus on how to get torch.Tensor objects out of our datasets, and how to use a PyTorch DataLoader and a Hugging Face Dataset with the best performance.'],\n",
       " ['https://huggingface.co/docs/datasets/use_with_jax',\n",
       "  'Use with JAX',\n",
       "  'This document is a quick introduction to using datasets with JAX, with a particular focus on how to get jax.Array objects out of our datasets, and how to use them to train JAX models.'],\n",
       " ['https://huggingface.co/docs/datasets/use_with_spark',\n",
       "  'Use with Spark',\n",
       "  'This document is a quick introduction to using  Datasets with Spark, with a particular focus on how to load a Spark DataFrame into a Dataset object.'],\n",
       " ['https://huggingface.co/docs/datasets/cache',\n",
       "  'Cache management',\n",
       "  'When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows  Datasets to avoid re-downloading or processing the entire dataset every time you use it.'],\n",
       " ['https://huggingface.co/docs/datasets/filesystems',\n",
       "  'Cloud storage',\n",
       "  ' Datasets supports access to cloud storage providers through a fsspec FileSystem implementations. You can save and load datasets from any cloud storage in a Pythonic way. Take a look at the following table for some example of supported cloud storage providers:'],\n",
       " ['https://huggingface.co/docs/datasets/faiss_es',\n",
       "  'Search index',\n",
       "  'FAISS and Elasticsearch enables searching for examples in a dataset. This can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task. For example, if you are working on a Open Domain Question Answering task, you may want to only return examples that are relevant to answering your question.'],\n",
       " ['https://huggingface.co/docs/datasets/how_to_metrics',\n",
       "  'Metrics',\n",
       "  'Metrics is deprecated in  Datasets. To learn more about how to use metrics, take a look at the library  Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets.'],\n",
       " ['https://huggingface.co/docs/datasets/beam',\n",
       "  'Beam Datasets',\n",
       "  'Some datasets are too large to be processed on a single machine. Instead, you can process them with Apache Beam, a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as Apache Flink, Apache Spark, or Google Cloud Dataflow.'],\n",
       " ['https://huggingface.co/docs/datasets/troubleshoot',\n",
       "  'Troubleshooting',\n",
       "  'This guide aims to provide you the tools and knowledge required to navigate some common issues. If the suggestions listed in this guide do not cover your such situation, please refer to the Asking for Help section to learn where to find help with your specific issue.'],\n",
       " ['https://huggingface.co/docs/datasets/audio_load',\n",
       "  'Load audio data',\n",
       "  'You can load an audio dataset using the Audio feature that automatically decodes and resamples the audio files when you access the examples. Audio decoding is based on the soundfile python package, which uses the libsndfile C library under the hood.'],\n",
       " ['https://huggingface.co/docs/datasets/audio_process',\n",
       "  'Process audio data',\n",
       "  'This guide shows specific methods for processing audio datasets. Learn how to:'],\n",
       " ['https://huggingface.co/docs/datasets/audio_dataset',\n",
       "  'Create an audio dataset',\n",
       "  'You can share a dataset with your team or with anyone in the community by creating a dataset repository on the Hugging Face Hub:'],\n",
       " ['https://huggingface.co/docs/datasets/image_load',\n",
       "  'Load image data',\n",
       "  'Image datasets have Image type columns, which contain PIL objects.'],\n",
       " ['https://huggingface.co/docs/datasets/image_process',\n",
       "  'Process image data',\n",
       "  'This guide shows specific methods for processing image datasets. Learn how to:'],\n",
       " ['https://huggingface.co/docs/datasets/image_dataset',\n",
       "  'Create an image dataset',\n",
       "  'There are two methods for creating and sharing an image dataset. This guide will show you how to:'],\n",
       " ['https://huggingface.co/docs/datasets/depth_estimation',\n",
       "  'Depth estimation',\n",
       "  'Depth estimation datasets are used to train a model to approximate the relative distance of every pixel in an image from the camera, also known as depth. The applications enabled by these datasets primarily lie in areas like visual machine perception and perception in robotics. Example applications include mapping streets for self-driving cars. This guide will show you how to apply transformations to a depth estimation dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/image_classification',\n",
       "  'Image classification',\n",
       "  'Image classification datasets are used to train a model to classify an entire image. There are a wide variety of applications enabled by these datasets such as identifying endangered wildlife species or screening for disease in medical images. This guide will show you how to apply transformations to an image classification dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/semantic_segmentation',\n",
       "  'Semantic segmentation',\n",
       "  'Semantic segmentation datasets are used to train a model to classify every pixel in an image. There are a wide variety of applications enabled by these datasets such as background removal from images, stylizing images, or scene understanding for autonomous driving. This guide will show you how to apply transformations to an image segmentation dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/object_detection',\n",
       "  'Object detection',\n",
       "  'Object detection models identify something in an image, and object detection datasets are used for applications such as autonomous driving and detecting natural hazards like wildfire. This guide will show you how to apply transformations to an object detection dataset following the tutorial from Albumentations.'],\n",
       " ['https://huggingface.co/docs/datasets/nlp_load',\n",
       "  'Load text data',\n",
       "  'This guide shows you how to load text datasets. To learn how to load any type of dataset, take a look at the general loading guide.'],\n",
       " ['https://huggingface.co/docs/datasets/nlp_process',\n",
       "  'Process text data',\n",
       "  'This guide shows specific methods for processing text datasets. Learn how to:'],\n",
       " ['https://huggingface.co/docs/datasets/tabular_load',\n",
       "  'Load tabular data',\n",
       "  'A tabular dataset is a generic dataset used to describe any data stored in rows and columns, where the rows represent an example and the columns represent a feature (can be continuous or categorical). These datasets are commonly stored in CSV files, Pandas DataFrames, and in database tables. This guide will show you how to load and create a tabular dataset from:'],\n",
       " ['https://huggingface.co/docs/datasets/share',\n",
       "  'Share',\n",
       "  'At Hugging Face, we are on a mission to democratize good Machine Learning and we believe in the value of open source. Thats why we designed  Datasets so that anyone can share a dataset with the greater ML community. There are currently thousands of datasets in over 100 languages in the Hugging Face Hub, and the Hugging Face team always welcomes new contributions!'],\n",
       " ['https://huggingface.co/docs/datasets/dataset_card',\n",
       "  'Create a dataset card',\n",
       "  'Each dataset should have a dataset card to promote responsible usage and inform users of any potential biases within the dataset. This idea was inspired by the Model Cards proposed by Mitchell, 2018. Dataset cards help users understand a datasets contents, the context for using the dataset, how it was created, and any other considerations a user should be aware of.'],\n",
       " ['https://huggingface.co/docs/datasets/repository_structure',\n",
       "  'Structure your repository',\n",
       "  'To host and share your dataset, create a dataset repository on the Hugging Face Hub and upload your data files.'],\n",
       " ['https://huggingface.co/docs/datasets/dataset_script',\n",
       "  'Create a dataset loading script',\n",
       "  'The dataset loading script is likely not needed if your dataset is in one of the following formats: CSV, JSON, JSON lines, text, images, audio or Parquet. With those formats, you should be able to load your dataset automatically with load_dataset(), as long as your dataset repository has a required structure.'],\n",
       " ['https://huggingface.co/docs/datasets/about_arrow',\n",
       "  'Datasets  Arrow',\n",
       "  'Arrow enables large amounts of data to be processed and moved quickly. It is a specific data format that stores data in a columnar memory layout. This provides several significant advantages:'],\n",
       " ['https://huggingface.co/docs/datasets/about_cache',\n",
       "  'The cache',\n",
       "  'The cache is one of the reasons why  Datasets is so efficient. It stores previously downloaded and processed datasets so when you need to use them again, they are reloaded directly from the cache. This avoids having to download a dataset all over again, or reapplying processing functions. Even after you close and start another Python session,  Datasets will reload your dataset directly from the cache!'],\n",
       " ['https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable',\n",
       "  'Dataset or IterableDataset',\n",
       "  'There are two types of dataset objects, a Dataset and an IterableDataset. Whichever type of dataset you choose to use or create depends on the size of the dataset. In general, an IterableDataset is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a Dataset is great for everything else. This page will compare the differences between a Dataset and an IterableDataset to help you pick the right dataset object for you.'],\n",
       " ['https://huggingface.co/docs/datasets/about_dataset_features',\n",
       "  'Dataset features',\n",
       "  'Features defines the internal structure of a dataset. It is used to specify the underlying serialization format. Whats more interesting to you though is that Features contains high-level information about everything from the column names and types, to the ClassLabel. You can think of Features as the backbone of a dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/about_dataset_load',\n",
       "  'Build and load',\n",
       "  'Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With  Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: load_dataset() to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use.'],\n",
       " ['https://huggingface.co/docs/datasets/about_map_batch',\n",
       "  'Batch mapping',\n",
       "  'Combining the utility of Dataset.map() with batch mode is very powerful. It allows you to speed up processing, and freely control the size of the generated dataset.'],\n",
       " ['https://huggingface.co/docs/datasets/about_metrics',\n",
       "  'All about metrics',\n",
       "  'Metrics is deprecated in  Datasets. To learn more about how to use metrics, take a look at the library  Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets.'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/main_classes',\n",
       "  'Main classes',\n",
       "  '( description: str = <factory>citation: str = <factory>homepage: str = <factory>license: str = <factory>features: Optional = Nonepost_processed: Optional = Nonesupervised_keys: Optional = Nonetask_templates: Optional = Nonebuilder_name: Optional = Nonedataset_name: Optional = Noneconfig_name: Optional = Noneversion: Union = Nonesplits: Optional = Nonedownload_checksums: Optional = Nonedownload_size: Optional = Nonepost_processing_size: Optional = Nonedataset_size: Optional = Nonesize_in_bytes: Optional = None )'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/builder_classes',\n",
       "  'Builder classes',\n",
       "  ' Datasets relies on two main classes during the dataset building process: DatasetBuilder and BuilderConfig.'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/loading_methods',\n",
       "  'Loading methods',\n",
       "  'Methods for listing and loading datasets and metrics:'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/table_classes',\n",
       "  'Table Classes',\n",
       "  'Each Dataset object is backed by a PyArrow Table. A Table can be loaded from either the disk (memory mapped) or in memory. Several Table types are available, and they all inherit from table.Table.'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/utilities',\n",
       "  'Utilities',\n",
       "  ' Datasets strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to WARNING.'],\n",
       " ['https://huggingface.co/docs/datasets/package_reference/task_templates',\n",
       "  'Task templates',\n",
       "  'The Task API is deprecated in favor of train-eval-index and will be removed in the next major release.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/index',\n",
       "  'Home',\n",
       "  'The huggingface_hub library allows you to interact with the Hugging Face Hub, a machine learning platform for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the hundreds of machine learning apps hosted on the Hub. You can also create and share your own models and datasets with the community. The huggingface_hub library provides a simple way to do all these things with Python.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/quick-start',\n",
       "  'Quickstart',\n",
       "  'The Hugging Face Hub is the go-to place for sharing machine learning models, demos, datasets, and metrics. huggingface_hub library helps you interact with the Hub without leaving your development environment. You can create and manage repositories easily, download and upload files, and get useful model and dataset metadata from the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/installation',\n",
       "  'Installation',\n",
       "  'Before you start, you will need to setup your environment by installing the appropriate packages.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/overview',\n",
       "  'Overview',\n",
       "  'In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/download',\n",
       "  'Download files',\n",
       "  'The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/upload',\n",
       "  'Upload files',\n",
       "  'Sharing your files and work is an important aspect of the Hub. The huggingface_hub offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/cli',\n",
       "  'Use the CLI',\n",
       "  'The huggingface_hub Python package comes with a built-in CLI called huggingface-cli. This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can login to your account, create a repository, upload and download files, etc. It also comes with handy features to configure your machine or manage your cache. In this guide, we will have a look at the main features of the CLI and how to use them.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/hf_file_system',\n",
       "  'HfFileSystem',\n",
       "  'In addition to the HfApi, the huggingface_hub library provides HfFileSystem, a pythonic fsspec-compatible file interface to the Hugging Face Hub. The HfFileSystem builds of top of the HfApi and offers typical filesystem style operations like cp, mv, ls, du, glob, get_file, and put_file.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/repository',\n",
       "  'Repository',\n",
       "  'The Hugging Face Hub is a collection of git repositories. Git is a widely used tool in software development to easily version projects when working collaboratively. This guide will show you how to interact with the repositories on the Hub, especially:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/search',\n",
       "  'Search',\n",
       "  'In this tutorial, you will learn how to search models, datasets and spaces on the Hub using huggingface_hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/inference',\n",
       "  'Inference',\n",
       "  'Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints',\n",
       "  'Inference Endpoints',\n",
       "  'Inference Endpoints provides a secure production solution to easily deploy any transformers, sentence-transformers, and diffusers models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the Hub. In this guide, we will learn how to programmatically manage Inference Endpoints with huggingface_hub. For more information about the Inference Endpoints product itself, check out its official documentation.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/community',\n",
       "  'Community Tab',\n",
       "  'The huggingface_hub library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit the dedicated documentation page for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/collections',\n",
       "  'Collections',\n",
       "  'A collection is a group of related items on the Hub (models, datasets, Spaces, papers) that are organized together on the same page. Collections are useful for creating your own portfolio, bookmarking content in categories, or presenting a curated list of items you want to share. Check out this guide to understand in more detail what collections are and how they look on the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/manage-cache',\n",
       "  'Cache',\n",
       "  'The Hugging Face Hub cache-system is designed to be the central cache shared across libraries that depend on the Hub. It has been updated in v0.8.0 to prevent re-downloading same files between revisions.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/model-cards',\n",
       "  'Model Cards',\n",
       "  'The huggingface_hub library provides a Python interface to create, share, and update Model Cards. Visit the dedicated documentation page for a deeper view of what Model Cards on the Hub are, and how they work under the hood.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/manage-spaces',\n",
       "  'Manage your Space',\n",
       "  'In this guide, we will see how to manage your Space runtime (secrets, hardware, and storage) using huggingface_hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/integrations',\n",
       "  'Integrate a library',\n",
       "  'The Hugging Face Hub makes hosting and sharing models with the community easy. It supports dozens of libraries in the Open Source ecosystem. We are always working on expanding this support to push collaborative Machine Learning forward. The huggingface_hub library plays a key role in this process, allowing any Python script to easily push and load files.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/guides/webhooks_server',\n",
       "  'Webhooks server',\n",
       "  'Webhooks are a foundation for MLOps-related features. They allow you to listen for new changes on specific repos or to all repos belonging to particular users/organizations youre interested in following. This guide will explain how to leverage huggingface_hub to create a server listening to webhooks and deploy it to a Space. It assumes you are familiar with the concept of webhooks on the Huggingface Hub. To learn more about webhooks themselves, you can read this guide first.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http',\n",
       "  'Git vs HTTP paradigm',\n",
       "  'The huggingface_hub library is a library for interacting with the Hugging Face Hub, which is a collections of git-based repositories (models, datasets or Spaces). There are two main ways to access the Hub using huggingface_hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/overview',\n",
       "  'Overview',\n",
       "  'This section contains an exhaustive and technical description of huggingface_hub classes and methods.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/login',\n",
       "  'Login and logout',\n",
       "  'The huggingface_hub library allows users to programmatically login and logout the machine to the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables',\n",
       "  'Environment variables',\n",
       "  'huggingface_hub can be configured using environment variables.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/repository',\n",
       "  'Managing local and online repositories',\n",
       "  'The Repository class is a helper class that wraps git and git-lfs commands. It provides tooling adapted for managing repositories which can be very large.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/hf_api',\n",
       "  'Hugging Face Hub API',\n",
       "  'Below is the documentation for the HfApi class, which serves as a Python wrapper for the Hugging Face Hubs API.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/file_download',\n",
       "  'Downloading files',\n",
       "  \"( repo_id: strfilename: strsubfolder: Optional = Nonerepo_type: Optional = Nonerevision: Optional = Nonelibrary_name: Optional = Nonelibrary_version: Optional = Nonecache_dir: Union = Nonelocal_dir: Union = Nonelocal_dir_use_symlinks: Union = 'auto'user_agent: Union = Noneforce_download: bool = Falseforce_filename: Optional = Noneproxies: Optional = Noneetag_timeout: float = 10resume_download: bool = Falsetoken: Union = Nonelocal_files_only: bool = Falseheaders: Optional = Nonelegacy_cache_layout: bool = Falseendpoint: Optional = None )\"],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/mixins',\n",
       "  'Mixins & serialization methods',\n",
       "  'The huggingface_hub library offers a range of mixins that can be used as a parent class for your objects, in order to provide simple uploading and downloading functions. Check out our integration guide to learn how to integrate any ML framework with the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/inference_client',\n",
       "  'Inference Client',\n",
       "  'Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints',\n",
       "  'Inference Endpoints',\n",
       "  'Inference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the Hub. This page is a reference for huggingface_hubs integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its official documentation.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/hf_file_system',\n",
       "  'HfFileSystem',\n",
       "  'The HfFileSystem class provides a pythonic file interface to the Hugging Face Hub based on fsspec.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/utilities',\n",
       "  'Utilities',\n",
       "  'The huggingface_hub package exposes a logging utility to control the logging level of the package itself. You can import it as such:'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/community',\n",
       "  'Discussions and Pull Requests',\n",
       "  'Check the HfApi documentation page for the reference of methods enabling interaction with Pull Requests and Discussions on the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/cache',\n",
       "  'Cache-system reference',\n",
       "  'The caching system was updated in v0.8.0 to become the central cache-system shared across libraries that depend on the Hub. Read the cache-system guide for a detailed presentation of caching at HF.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/cards',\n",
       "  'Repo Cards and Repo Card Data',\n",
       "  'The huggingface_hub library provides a Python interface to create, share, and update Model/Dataset Cards. Visit the dedicated documentation page for a deeper view of what Model Cards on the Hub are, and how they work under the hood. You can also check out our Model Cards guide to get a feel for how you would use these utilities in your own projects.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/space_runtime',\n",
       "  'Space runtime',\n",
       "  'Check the HfApi documentation page for the reference of methods to manage your Space on the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/collections',\n",
       "  'Collections',\n",
       "  'Check out the HfApi documentation page for the reference of methods to manage your Space on the Hub.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/tensorboard',\n",
       "  'TensorBoard logger',\n",
       "  'TensorBoard is a visualization toolkit for machine learning experimentation. TensorBoard allows tracking and visualizing metrics such as loss and accuracy, visualizing the model graph, viewing histograms, displaying images and much more. TensorBoard is well integrated with the Hugging Face Hub. The Hub automatically detects TensorBoard traces (such as tfevents) when pushed to the Hub which starts an instance to visualize them. To get more information about TensorBoard integration on the Hub, check out this guide.'],\n",
       " ['https://huggingface.co/docs/huggingface_hub/package_reference/webhooks_server',\n",
       "  'Webhooks server',\n",
       "  'Webhooks are a foundation for MLOps-related features. They allow you to listen for new changes on specific repos or to all repos belonging to particular users/organizations youre interested in following. To learn more about webhooks on the Huggingface Hub, you can read the Webhooks guide.'],\n",
       " ['https://huggingface.co/docs/huggingface.js/index',\n",
       "  ' Hugging Face JS Libraries',\n",
       "  ''],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/README',\n",
       "  'Use Inference Endpoints',\n",
       "  'A Typescript powered wrapper for the Hugging Face Inference Endpoints API. Learn more about Inference Endpoints at Hugging Face. It works with both Inference API (serverless) and Inference Endpoints (dedicated).'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/modules',\n",
       "  'API reference',\n",
       "  ' AudioClassificationArgs: BaseArgs & { data: Blob | ArrayBuffer }'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/classes/HfInference',\n",
       "  'HfInference',\n",
       "  'TaskWithNoAccessToken'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/classes/HfInferenceEndpoint',\n",
       "  'HfInferenceEndpoint',\n",
       "  'TaskWithNoAccessTokenNoModel'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/classes/InferenceOutputError',\n",
       "  'InferenceOutputError',\n",
       "  'TypeError'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioClassificationOutputValue',\n",
       "  'AudioClassificationOutputValue',\n",
       "  ' label: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioToAudioOutputValue',\n",
       "  'AudioToAudioOutputValue',\n",
       "  ' blob: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/AutomaticSpeechRecognitionOutput',\n",
       "  'AutomaticSpeechRecognitionOutput',\n",
       "  ' text: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/BaseArgs',\n",
       "  'BaseArgs',\n",
       "  ' Optional accessToken: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ConversationalOutput',\n",
       "  'BaseArgs',\n",
       "  ' Optional accessToken: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/DocumentQuestionAnsweringOutput',\n",
       "  'DocumentQuestionAnsweringOutput',\n",
       "  ' answer: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageClassificationOutputValue',\n",
       "  'ImageClassificationOutputValue',\n",
       "  ' label: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageSegmentationOutputValue',\n",
       "  'ImageSegmentationOutputValue',\n",
       "  ' label: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageToTextOutput',\n",
       "  'ImageToTextOutput',\n",
       "  ' generated_text: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ObjectDetectionOutputValue',\n",
       "  'ObjectDetectionOutputValue',\n",
       "  ' box: Object'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/Options',\n",
       "  'Options',\n",
       "  ' Optional dont_load_model: boolean'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/QuestionAnsweringOutput',\n",
       "  'QuestionAnsweringOutput',\n",
       "  ' answer: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/SummarizationOutput',\n",
       "  'SummarizationOutput',\n",
       "  ' summary_text: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TableQuestionAnsweringOutput',\n",
       "  'TableQuestionAnsweringOutput',\n",
       "  ' aggregator: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationOutput',\n",
       "  'TextGenerationOutput',\n",
       "  'Outputs for Text Generation inference'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamBestOfSequence',\n",
       "  'TextGenerationStreamBestOfSequence',\n",
       "  ' finish_reason: TextGenerationStreamFinishReason'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamDetails',\n",
       "  'TextGenerationStreamDetails',\n",
       "  ' Optional best_of_sequences: TextGenerationStreamBestOfSequence[]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamOutput',\n",
       "  'TextGenerationStreamOutput',\n",
       "  ' details: null | TextGenerationStreamDetails'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamPrefillToken',\n",
       "  'TextGenerationStreamPrefillToken',\n",
       "  ' id: number'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamToken',\n",
       "  'TextGenerationStreamToken',\n",
       "  ' id: number'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TokenClassificationOutputValue',\n",
       "  'TokenClassificationOutputValue',\n",
       "  ' end: number'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/TranslationOutputValue',\n",
       "  'TranslationOutputValue',\n",
       "  ' translation_text: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/VisualQuestionAnsweringOutput',\n",
       "  'VisualQuestionAnsweringOutput',\n",
       "  ' answer: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotClassificationOutputValue',\n",
       "  'ZeroShotClassificationOutputValue',\n",
       "  ' labels: string[]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotImageClassificationOutputValue',\n",
       "  'ZeroShotImageClassificationOutputValue',\n",
       "  ' label: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/README',\n",
       "  'Interact with the Hub',\n",
       "  'Official utilities to use the Hugging Face Hub API.'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/modules',\n",
       "  'API Reference',\n",
       "  ' AccessToken: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/classes/HubApiError',\n",
       "  'HubApiError',\n",
       "  'Error thrown when an API call to the Hugging Face Hub fails.'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/classes/InvalidApiResponseFormatError',\n",
       "  'InvalidApiResponseFormatError',\n",
       "  'Error'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/AuthInfo',\n",
       "  'AuthInfo',\n",
       "  ' Optional accessToken: Object'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitDeletedEntry',\n",
       "  'CommitDeletedEntry',\n",
       "  ' operation: \"delete\"'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitFile',\n",
       "  'CommitFile',\n",
       "  ' content: ContentSource'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitOutput',\n",
       "  'CommitOutput',\n",
       "  ' commit: Object'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitParams',\n",
       "  'CommitParams',\n",
       "  ' Optional abortSignal: AbortSignal'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/Credentials',\n",
       "  'Credentials',\n",
       "  ' accessToken: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/DatasetEntry',\n",
       "  'DatasetEntry',\n",
       "  ' downloads: number'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/FileDownloadInfoOutput',\n",
       "  'FileDownloadInfoOutput',\n",
       "  ' downloadLink: null | string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/ListFileEntry',\n",
       "  'ListFileEntry',\n",
       "  ' Optional lastCommit: Object'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/ModelEntry',\n",
       "  'ModelEntry',\n",
       "  ' downloads: number'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/OAuthResult',\n",
       "  'OAuthResult',\n",
       "  ' accessToken: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/RepoId',\n",
       "  'RepoId',\n",
       "  ' name: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/SafetensorsIndexJson',\n",
       "  'SafetensorsIndexJson',\n",
       "  ' Optional dtype: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceConfig',\n",
       "  'SpaceResourceConfig',\n",
       "  ' Optional is_custom: boolean'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceRequirement',\n",
       "  'SpaceResourceRequirement',\n",
       "  ' Optional cpu: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceRuntime',\n",
       "  'SpaceRuntime',\n",
       "  ' Optional errorMessage: string'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/TensorInfo',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIApp',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIOrg',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIUser',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/agents/README',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/agents/modules',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/huggingface.js/agents/classes/HfAgent',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/index',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/installation',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/pipelines',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/custom_usage',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/vanilla-js',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/react',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/next',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/browser-extension',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/electron',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/tutorials/node',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/guides/private',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/guides/node-audio-processing',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/transformers',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/pipelines',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/models',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/tokenizers',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/processors',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/configs',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/transformers.js/api/env',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/index',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/quicktour',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/detailed_parameters',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/parallelism',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/usage',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/api-inference/faq',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/index',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/security',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/supported_tasks',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/api_reference',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/autoscaling',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/pricing',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/support',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/faq',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/access',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/create_endpoint',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/test_endpoint',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/update_endpoint',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/advanced',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/private_link',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/custom_handler',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/custom_container',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/logs',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/metrics',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/change_organization',\n",
       "  'TensorInfo',\n",
       "  ' data_offsets: [number, number]'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/guides/pause_endpoint',\n",
       "  'Pause and Resume your Endpoint',\n",
       "  'You can pause & resume endpoints to save cost and configurations. Please note that if your endpoint is in a failed state, you will need to create a new endpoint. To pause/resume your endpoint, navigate to the overview tab and click the button at top right corner, which will show Pause endpoint to pause, or Resume endpoint to reactivate the paused endpoint.'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/others/runtime',\n",
       "  'Inference Endpoints Version',\n",
       "  'Hugging Face Inference Endpoints comes with a default serving container which is used for all supported Transformers and Sentence-Transformers tasks and for custom inference handler and implement batching. Below you will find information about the installed packages and versions used.'],\n",
       " ['https://huggingface.co/docs/inference-endpoints/others/serialization',\n",
       "  'Serialization & Deserialization for Requests',\n",
       "  'Hugging Face Inference Endpount comes with a default serving container which is used for all supported Transformers and Sentence-Transformers tasks and for custom inference handler. The serving container takes care of serialization and deserialization of the request and response payloads based on the content-type and accept headers of the request. That means that when you send a request with a JSON body and a content-type: application/json header, the serving container will deserialize the JSON payload into a Python dictionary and pass it to the inference handler and if you send a request with a accept: image/png header, the serving container will seralize the response from the task/custom handler into a image.'],\n",
       " ['https://huggingface.co/docs/peft/index',\n",
       "  ' PEFT',\n",
       "  ' PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a models parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.'],\n",
       " ['https://huggingface.co/docs/peft/quicktour',\n",
       "  'Quicktour',\n",
       "  'PEFT offers parameter-efficient methods for finetuning large pretrained models. The traditional paradigm is to finetune all of a models parameters for each downstream task, but this is becoming exceedingly costly and impractical because of the enormous number of parameters in models today. Instead, it is more efficient to train a smaller number of prompt parameters or use a reparametrization method like low-rank adaptation (LoRA) to reduce the number of trainable parameters.'],\n",
       " ['https://huggingface.co/docs/peft/install',\n",
       "  'Installation',\n",
       "  'Before you start, you will need to setup your environment, install the appropriate packages, and configure  PEFT.  PEFT is tested on Python 3.8+.'],\n",
       " ['https://huggingface.co/docs/peft/tutorial/peft_model_config',\n",
       "  'Configurations and models',\n",
       "  'The sheer size of todays large pretrained models - which commonly have billions of parameters - present a significant training challenge because they require more storage space and more computational power to crunch all those calculations. Youll need access to powerful GPUs or TPUs to train these large pretrained models which is expensive, not widely accessible to everyone, not environmentally friendly, and not very practical. PEFT methods address many of these challenges. There are several types of PEFT methods (soft prompting, matrix decomposition, adapters), but they all focus on the same thing, reduce the number of trainable parameters. This makes it more accessible to train and store large models on consumer hardware.'],\n",
       " ['https://huggingface.co/docs/peft/tutorial/peft_integrations',\n",
       "  'Integrations',\n",
       "  'PEFTs practical benefits extends to other Hugging Face libraries like Diffusers and Transformers. One of the main benefits of PEFT is that an adapter file generated by a PEFT method is a lot smaller than the original model, which makes it super easy to manage and use multiple adapters. You can use one pretrained base model for multiple tasks by simply loading a new adapter finetuned for the task youre solving. Or you can combine multiple adapters with a text-to-image diffusion model to create new effects.'],\n",
       " ['https://huggingface.co/docs/peft/task_guides/prompt_based_methods',\n",
       "  'Prompt-based methods',\n",
       "  'A prompt can describe a task or provide an example of a task you want the model to learn. Instead of manually creating these prompts, soft prompting methods add learnable parameters to the input embeddings that can be optimized for a specific task while keeping the pretrained models parameters frozen. This makes it both faster and easier to finetune large language models (LLMs) for new downstream tasks.'],\n",
       " ['https://huggingface.co/docs/peft/task_guides/lora_based_methods',\n",
       "  'LoRA methods',\n",
       "  'A popular way to efficiently train large models is to insert (typically in the attention blocks) smaller trainable matrices that are a low-rank decomposition of the delta weight matrix to be learnt during finetuning. The pretrained models original weight matrix is frozen and only the smaller matrices are updated during training. This reduces the number of trainable parameters, reducing memory usage and training time which can be very expensive for large models.'],\n",
       " ['https://huggingface.co/docs/peft/task_guides/ia3',\n",
       "  'IA3',\n",
       "  'IA3 multiplies the models activations (the keys and values in the self-attention and encoder-decoder attention blocks, and the intermediate activation of the position-wise feedforward network) by three learned vectors. This PEFT method introduces an even smaller number of trainable parameters than LoRA which introduces weight matrices instead of vectors. The original models parameters are kept frozen and only these vectors are updated. As a result, it is faster, cheaper and more efficient to finetune for a new downstream task.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/quantization',\n",
       "  'Quantization',\n",
       "  'Quantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs). There are several ways to quantize a model including:'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/lora',\n",
       "  'LoRA',\n",
       "  'LoRA is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory. In PEFT, using LoRA is as easy as setting up a LoraConfig and wrapping it with get_peft_model() to create a trainable PeftModel.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/custom_models',\n",
       "  'Custom models',\n",
       "  'Some fine-tuning techniques, such as prompt tuning, are specific to language models. That means in  PEFT, it is assumed a  Transformers model is being used. However, other fine-tuning techniques - like LoRA - are not restricted to specific model types.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/low_level_api',\n",
       "  'Adapter injection',\n",
       "  'With PEFT, you can inject trainable adapters into any torch module which allows you to use adapter methods without relying on the modeling classes in PEFT. Currently, PEFT supports injecting LoRA, AdaLoRA, and IA3 into models because for these adapters, inplace modification of the model is sufficient for finetuning it.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/mixed_models',\n",
       "  'Mixed adapter types',\n",
       "  'Normally, it isnt possible to mix different adapter types in  PEFT. You can create a PEFT model with two different LoRA adapters (which can have different config options), but it is not possible to combine a LoRA and LoHa adapter. With PeftMixedModel however, this works as long as the adapter types are compatible. The main purpose of allowing mixed adapter types is to combine trained adapters for inference. While it is possible to train a mixed adapter model, this has not been tested and is not recommended.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/contributing',\n",
       "  'Contribute to PEFT',\n",
       "  'We are happy to accept contributions to PEFT. If you plan to contribute, please read this to make the process as smooth as possible.'],\n",
       " ['https://huggingface.co/docs/peft/developer_guides/troubleshooting',\n",
       "  'Troubleshooting',\n",
       "  'If you encounter any issue when using PEFT, please check the following list of common issues and their solutions.'],\n",
       " ['https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload',\n",
       "  'Troubleshooting',\n",
       "  'If you encounter any issue when using PEFT, please check the following list of common issues and their solutions.'],\n",
       " ['https://huggingface.co/docs/peft/accelerate/fsdp',\n",
       "  'Fully Sharded Data Parallel',\n",
       "  'Fully sharded data parallel (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.'],\n",
       " ['https://huggingface.co/docs/peft/conceptual_guides/adapter',\n",
       "  'Adapters',\n",
       "  'Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources.'],\n",
       " ['https://huggingface.co/docs/peft/conceptual_guides/prompting',\n",
       "  'Soft prompts',\n",
       "  'Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as prompting. Prompting primes a frozen pretrained model for a specific downstream task by including a text prompt that describes the task or even demonstrates an example of the task. With prompting, you can avoid fully training a separate model for each downstream task, and use the same frozen pretrained model instead. This is a lot easier because you can use the same model for several different tasks, and it is significantly more efficient to train and store a smaller set of prompt parameters than to train all the models parameters.'],\n",
       " ['https://huggingface.co/docs/peft/conceptual_guides/ia3',\n",
       "  'IA3',\n",
       "  'This conceptual guide gives a brief overview of IA3, a parameter-efficient fine tuning technique that is intended to improve over LoRA.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/auto_class',\n",
       "  'AutoPeftModel',\n",
       "  'The AutoPeftModel classes loads the appropriate PEFT model for the task type by automatically inferring it from the configuration file. They are designed to quickly and easily load a PEFT model in a single line of code without having to worry about which exact model class you need or manually loading a PeftConfig.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/peft_model',\n",
       "  'PEFT model',\n",
       "  'PeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base PeftModel contains methods for loading and saving models from the Hub.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/peft_types',\n",
       "  'PEFT types',\n",
       "  'PeftType includes the supported adapters in PEFT, and TaskType includes PEFT-supported tasks.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/config',\n",
       "  'Configuration',\n",
       "  'PeftConfigMixin is the base configuration class for storing the adapter configuration of a PeftModel, and PromptLearningConfig is the base configuration class for soft prompt methods (p-tuning, prefix tuning, and prompt tuning). These base classes contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/tuners',\n",
       "  'Tuner',\n",
       "  'A tuner (or adapter) is a module that can be plugged into a torch.nn.Module. BaseTuner base class for other tuners and provides shared methods and attributes for preparing an adapter configuration and replacing a target module with the adapter module. BaseTunerLayer is a base class for adapter layers. It offers methods and attributes for managing adapters such as activating and disabling adapters.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/adalora',\n",
       "  'AdaLoRA',\n",
       "  'AdaLoRA is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/ia3',\n",
       "  'IA3',\n",
       "  'Infused Adapter by Inhibiting and Amplifying Inner Activations, or IA3, is a method that adds three learned vectors to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/llama_adapter',\n",
       "  'Llama-Adapter',\n",
       "  'Llama-Adapter is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/loha',\n",
       "  'LoHa',\n",
       "  'Low-Rank Hadamard Product (LoHa), is similar to LoRA except it approximates the large weight matrix with more low-rank matrices and combines them with the Hadamard product. This method is even more parameter-efficient than LoRA and achieves comparable performance.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/lokr',\n",
       "  'LoKr',\n",
       "  'Low-Rank Kronecker Product (LoKr), is a LoRA-variant method that approximates the large weight matrix with two low-rank matrices and combines them with the Kronecker product. LoKr also provides an optional third low-rank matrix to provide better control during fine-tuning.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/lora',\n",
       "  'LoRA',\n",
       "  'Low-Rank Adaptation (LoRA) is a PEFT method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This drastically reduces the number of parameters that need to be fine-tuned.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/adapter_utils',\n",
       "  'LyCORIS',\n",
       "  'LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) are LoRA-like matrix decomposition adapters that modify the cross-attention layer of the UNet. The LoHa and LoKr methods inherit from the Lycoris classes here.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning',\n",
       "  'Multitask Prompt Tuning',\n",
       "  'Multitask prompt tuning decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/oft',\n",
       "  'OFT',\n",
       "  'Orthogonal Finetuning (OFT) is a method developed for adapting text-to-image diffusion models. It works by reparameterizing the pretrained weight matrices with its orthogonal matrix to preserve information in the pretrained model. To reduce the number of parameters, OFT introduces a block-diagonal structure in the orthogonal matrix.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/poly',\n",
       "  'Polytropon',\n",
       "  'Polytropon is a multitask model with a number of different LoRA adapters in its inventory. The model learns the correct combination of adapters from the inventory with a routing function to choose the best subset of modules for a specific task. PEFT also supports Multi-Head Adapter Routing (MHR) for Polytropon which builds on and improves the routing function by combining the adapter heads more granularly. The adapter heads are separated into disjoint blocks and a different routing function is learned for each one, allowing for more expressivity.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/p_tuning',\n",
       "  'P-tuning',\n",
       "  'P-tuning adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/prefix_tuning',\n",
       "  'Prefix tuning',\n",
       "  'Prefix tuning prefixes a series of task-specific vectors to the input sequence that can be learned while keeping the pretrained model frozen. The prefix parameters are inserted in all of the model layers.'],\n",
       " ['https://huggingface.co/docs/peft/package_reference/prompt_tuning',\n",
       "  'Prompt tuning',\n",
       "  'Prompt tuning adds task-specific prompts to the input, and these prompt parameters are updated independently of the pretrained model parameters which are frozen.'],\n",
       " ['https://huggingface.co/docs/accelerate/index',\n",
       "  ' Accelerate',\n",
       "  ' Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/install',\n",
       "  'Installation',\n",
       "  'Before you start, you will need to setup your environment, install the appropriate packages, and configure  Accelerate.  Accelerate is tested on Python 3.8+.'],\n",
       " ['https://huggingface.co/docs/accelerate/quicktour',\n",
       "  'Quicktour',\n",
       "  'There are many ways to launch and run your code depending on your training environment (torchrun, DeepSpeed, etc.) and available hardware. Accelerate offers a unified interface for launching and training on different distributed setups, allowing you to focus on your PyTorch training code instead of the intricacies of adapting your code to these different setups. This allows you to easily scale your PyTorch code for training and inference on distributed setups with hardware like GPUs and TPUs. Accelerate also provides Big Model Inference to make loading and running inference with really large models that usually dont fit in memory more accessible.'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/overview',\n",
       "  'Overview',\n",
       "  'Welcome to the  Accelerate tutorials! These introductory guides will help catch you up to speed on working with  Accelerate. Youll learn how to modify your code to have it work with the API seamlessly, how to launch your script properly, and more!'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/migration',\n",
       "  'Add Accelerate to your code',\n",
       "  'Each distributed training framework has their own way of doing things which can require writing a lot of custom code to adapt it to your PyTorch training code and training environment. Accelerate offers a friendly way to interface with these distributed training frameworks without having to learn the specific details of each one. Accelerate takes care of those details for you, so you can focus on the training code and scale it to any distributed training environment.'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/launch',\n",
       "  'Launching distributed code',\n",
       "  'In the previous tutorial, you were introduced to how to modify your current training script to use  Accelerate. The final version of that code is shown below:'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/notebook',\n",
       "  'Launching distributed training from Jupyter Notebooks',\n",
       "  'This tutorial teaches you how to fine tune a computer vision model with  Accelerate from a Jupyter Notebook on a distributed system. You will also learn how to setup a few requirements needed for ensuring your environment is configured properly, your data has been prepared properly, and finally how to launch training.'],\n",
       " ['https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting',\n",
       "  'Troubleshoot',\n",
       "  'This guide provides solutions to some issues you might encounter when using Accelerate. Not all errors are covered because Accelerate is an active library that is continuously evolving and there are many different use cases and distributed training setups. If the solutions described here dont help with your specific error, please take a look at the Ask for help section to learn where and how to get help.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/explore',\n",
       "  'Start Here!',\n",
       "  'Please use the interactive tool below to help you get started with learning about a particular feature of  Accelerate and how to utilize it! It will provide you with a code diff, an explanation towards what is going on, as well as provide you with some useful links to explore more within the documentation!'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/training_zoo',\n",
       "  'Example Zoo',\n",
       "  'Below contains a non-exhaustive list of tutorials and scripts showcasing  Accelerate'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/big_modeling',\n",
       "  'Big Model Inference',\n",
       "  'One of the biggest advancements  Accelerate provides is the concept of large model inference wherein you can perform inference on models that cannot fully fit on your graphics card.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator',\n",
       "  'Model memory estimator',\n",
       "  'One very difficult aspect when exploring potential models to use on your machine is knowing just how big of a model will fit into memory with your current graphics card (such as loading the model onto CUDA).'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/quantization',\n",
       "  'Model quantization',\n",
       "  ' Accelerate brings bitsandbytes quantization to your model. You can now load any pytorch model in 8-bit or 4-bit with a few lines of code.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/distributed_inference',\n",
       "  'Distributed inference',\n",
       "  'Distributed inference can fall into three brackets:'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/local_sgd',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/checkpoint',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/tracking',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/mps',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/low_precision_training',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/deepspeed',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/fsdp',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/megatron_lm',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/sagemaker',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/usage_guides/ipex',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/internal_mechanism',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/big_model_inference',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/performance',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/deferring_execution',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/low_precision_training',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/concept_guides/training_tpu',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/accelerator',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/state',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/cli',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/torch_wrappers',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/tracking',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/launchers',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/deepspeed',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/logging',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/big_modeling',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/kwargs',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/utilities',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/megatron_lm',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/accelerate/package_reference/fsdp',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum/index',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum/installation',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum/quicktour',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum/notebooks',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum/concept_guides/quantization',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/index',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/installation',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/quickstart',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/overview',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/notebooks',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/stable_diffusion',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_llama_7b',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/tutorials/sentence_transformers',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/overview',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/setup_aws_instance',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/sagemaker',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/cache_system',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/fine_tune',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/distributed_training',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/export_model',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/models',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/guides/pipelines',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/community/contributing',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/package_reference/trainer',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/package_reference/distributed',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/package_reference/export',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/package_reference/modeling',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/optimum-neuron/benchmarks/inferentia-llama2',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/tokenizers/index',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/tokenizers/quicktour',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/tokenizers/installation',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/tokenizers/pipeline',\n",
       "  'Gradient accumulation',\n",
       "  'Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed.'],\n",
       " ['https://huggingface.co/docs/tokenizers/components',\n",
       "  'Components',\n",
       "  'A Normalizer is in charge of pre-processing the input string in order to normalize it as relevant for a given use case. Some common examples of normalization are the Unicode normalization algorithms (NFD, NFKD, NFC & NFKC), lowercasing etc The specificity of tokenizers is that we keep track of the alignment while normalizing. This is essential to allow mapping from the generated tokens back to the input text.'],\n",
       " ['https://huggingface.co/docs/tokenizers/training_from_memory',\n",
       "  'Training from memory',\n",
       "  'For all the examples listed below, well use the same Tokenizer and Trainer, built as following:'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/input-sequences',\n",
       "  'Input Sequences',\n",
       "  'A str that represents an input sequence'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/encode-inputs',\n",
       "  'Encode Inputs',\n",
       "  'Represents a textual input for encoding. Can be either:'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/tokenizer',\n",
       "  'Tokenizer',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/encoding',\n",
       "  'Encoding',\n",
       "  'The Encoding represents the output of a Tokenizer.'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/added-tokens',\n",
       "  'Added Tokens',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/models', 'Models', 'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/normalizers',\n",
       "  'Normalizers',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/pre-tokenizers',\n",
       "  'Pre-tokenizers',\n",
       "  'BertPreTokenizer'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/post-processors',\n",
       "  'Post-processors',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/trainers',\n",
       "  'Trainers',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/decoders',\n",
       "  'Decoders',\n",
       "  'Parameters'],\n",
       " ['https://huggingface.co/docs/tokenizers/api/visualizer',\n",
       "  'Visualizer',\n",
       "  '( tokenizer: Tokenizerdefault_to_notebook: bool = Trueannotation_converter: typing.Union[typing.Callable[[typing.Any], tokenizers.tools.visualizer.Annotation], NoneType] = None )'],\n",
       " ['https://huggingface.co/docs/evaluate/index',\n",
       "  ' Evaluate',\n",
       "  'A library for easily evaluating machine learning models and datasets.'],\n",
       " ['https://huggingface.co/docs/evaluate/installation',\n",
       "  'Installation',\n",
       "  'You should install  Evaluate in a virtual environment to keep everything neat and tidy.'],\n",
       " ['https://huggingface.co/docs/evaluate/a_quick_tour',\n",
       "  'A quick tour',\n",
       "  'There are different aspects of a typical machine learning pipeline that can be evaluated and for each aspect  Evaluate provides a tool:'],\n",
       " ['https://huggingface.co/docs/evaluate/choosing_a_metric',\n",
       "  'Choosing the right metric',\n",
       "  'There is no one size fits all approach to choosing an evaluation metric, but some good guidelines to keep in mind are:'],\n",
       " ['https://huggingface.co/docs/evaluate/creating_and_sharing',\n",
       "  'Adding new evaluations',\n",
       "  'Also make sure your Hugging Face token is registered so you can connect to the Hugging Face Hub:'],\n",
       " ['https://huggingface.co/docs/evaluate/base_evaluator',\n",
       "  'Using the evaluator',\n",
       "  'Currently supported tasks are:'],\n",
       " ['https://huggingface.co/docs/evaluate/custom_evaluator',\n",
       "  'Using the evaluator with custom pipelines',\n",
       "  'First we need to train a model. Well train a simple text classifier on the IMDb dataset, so lets start by downloading the dataset:'],\n",
       " ['https://huggingface.co/docs/evaluate/evaluation_suite',\n",
       "  'Creating an EvaluationSuite',\n",
       "  'The EvaluationSuite provides a way to compose any number of (evaluator, dataset, metric) tuples as a SubTask to evaluate a model on a collection of several evaluation tasks. See the evaluator documentation for a list of currently supported tasks.'],\n",
       " ['https://huggingface.co/docs/evaluate/transformers_integrations',\n",
       "  'Transformers',\n",
       "  'The metrics in evaluate can be easily integrated with the Trainer. The Trainer accepts a compute_metrics keyword argument that passes a function to compute metrics. One can specify the evaluation interval with evaluation_strategy in the TrainerArguments, and based on that, the model is evaluated accordingly, and the predictions and labels passed to compute_metrics.'],\n",
       " ['https://huggingface.co/docs/evaluate/keras_integrations',\n",
       "  'Keras and Tensorflow',\n",
       "  'Suppose we want to keep track of model metrics while a model is training. We can use a Callback in order to calculate this metric during training, after an epoch ends.'],\n",
       " ['https://huggingface.co/docs/evaluate/sklearn_integrations',\n",
       "  'scikit-learn',\n",
       "  'The metrics in evaluate can be easily integrated with an Scikit-Learn estimator or pipeline.'],\n",
       " ['https://huggingface.co/docs/evaluate/types_of_evaluations',\n",
       "  'Types of evaluations',\n",
       "  'Here are the types of evaluations that are currently supported with a few examples for each:'],\n",
       " ['https://huggingface.co/docs/evaluate/considerations',\n",
       "  'Considerations for model evaluation',\n",
       "  'Here are some things to keep in mind when evaluating your model using the  Evaluate library:'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/main_classes',\n",
       "  'Main classes',\n",
       "  \"( description: strcitation: strfeatures: typing.Union[datasets.features.features.Features, typing.List[datasets.features.features.Features]]inputs_description: str = <factory>homepage: str = <factory>license: str = <factory>codebase_urls: typing.List[str] = <factory>reference_urls: typing.List[str] = <factory>streamable: bool = Falseformat: typing.Optional[str] = Nonemodule_type: str = 'metric'module_name: typing.Optional[str] = Noneconfig_name: typing.Optional[str] = Noneexperiment_id: typing.Optional[str] = None )\"],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/loading_methods',\n",
       "  'Loading methods',\n",
       "  '( module_type = Noneinclude_community = Truewith_details = False )'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/saving_methods',\n",
       "  'Saving methods',\n",
       "  '( path_or_file**data )'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/hub_methods',\n",
       "  'Hub methods',\n",
       "  '( model_id: strtask_type: strdataset_type: strdataset_name: strmetric_type: strmetric_name: strmetric_value: floattask_name: str = Nonedataset_config: str = Nonedataset_split: str = Nonedataset_revision: str = Nonedataset_args: typing.Dict[str, int] = Nonemetric_config: str = Nonemetric_args: typing.Dict[str, int] = Noneoverwrite: bool = False )'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/evaluator_classes',\n",
       "  'Evaluator classes',\n",
       "  'The main entry point for using the evaluator:'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/visualization_methods',\n",
       "  'Visualization methods',\n",
       "  '( datamodel_namesinvert_range = []config = Nonefig = None )'],\n",
       " ['https://huggingface.co/docs/evaluate/package_reference/logging_methods',\n",
       "  'Logging methods',\n",
       "  'To change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the INFO level:'],\n",
       " ['https://huggingface.co/docs/datasets-server/index',\n",
       "  ' Dataset viewer',\n",
       "  'The dataset viewer is a lightweight web API for visualizing and exploring all types of datasets - computer vision, speech, text, and tabular - stored on the Hugging Face Hub.'],\n",
       " ['https://huggingface.co/docs/datasets-server/quick_start',\n",
       "  'Quickstart',\n",
       "  'In this quickstart, youll learn how to use the dataset viewers REST API to:'],\n",
       " ['https://huggingface.co/docs/datasets-server/analyze_data',\n",
       "  'Analyze a dataset on the Hub',\n",
       "  'In the Quickstart, you were introduced to various endpoints for interacting with datasets on the Hub. One of the most useful ones is the /parquet endpoint, which allows you to get a dataset stored on the Hub and analyze it. This is a great way to explore the dataset, and get a better understanding of its contents.'],\n",
       " ['https://huggingface.co/docs/datasets-server/valid',\n",
       "  'Check dataset validity',\n",
       "  'Before you download a dataset from the Hub, it is helpful to know if a specific dataset youre interested in is available. The dataset viewer provides the /is-valid endpoint to check if a specific dataset works without any errors.'],\n",
       " ['https://huggingface.co/docs/datasets-server/splits',\n",
       "  'List splits and configurations',\n",
       "  'Datasets typically have splits and may also have configurations. A split is a subset of the dataset, like train and test, that are used during different stages of training and evaluating a model. A configuration is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If youre interested in learning more about splits and configurations, check out the conceptual guide on Splits and configurations!'],\n",
       " ['https://huggingface.co/docs/datasets-server/info',\n",
       "  'Get dataset information',\n",
       "  'The dataset viewer provides an /info endpoint for exploring the general information about dataset, including such fields as description, citation, homepage, license and features.'],\n",
       " ['https://huggingface.co/docs/datasets-server/first_rows',\n",
       "  'Preview a dataset',\n",
       "  'The dataset viewer provides a /first-rows endpoint for visualizing the first 100 rows of a dataset. Thisll give you a good idea of the data types and example data contained in a dataset.'],\n",
       " ['https://huggingface.co/docs/datasets-server/rows',\n",
       "  'Download slices of rows',\n",
       "  'The dataset viewer provides a /rows endpoint for visualizing any slice of rows of a dataset. This will let you walk-through and inspect the data contained in a dataset.'],\n",
       " ['https://huggingface.co/docs/datasets-server/search',\n",
       "  'Search text in a dataset',\n",
       "  'The dataset viewer provides a /search endpoint for searching words in a dataset.'],\n",
       " ['https://huggingface.co/docs/datasets-server/filter',\n",
       "  'Filter rows in a dataset',\n",
       "  'The dataset viewer provides a /filter endpoint for filtering rows in a dataset.'],\n",
       " ['https://huggingface.co/docs/datasets-server/parquet',\n",
       "  'List Parquet files',\n",
       "  'Datasets can be published in any format (CSV, JSONL, directories of images, etc.) to the Hub, and they are easily accessed with the  Datasets library. For a more performant experience (especially when it comes to large datasets), the dataset viewer automatically converts every dataset to the Parquet format.'],\n",
       " ['https://huggingface.co/docs/datasets-server/size',\n",
       "  'Get the number of rows and the bytes size',\n",
       "  'This guide shows you how to use the dataset viewers /size endpoint to retrieve a datasets size programmatically. Feel free to also try it out with ReDoc.'],\n",
       " ['https://huggingface.co/docs/datasets-server/statistics',\n",
       "  'Explore dataset statistics',\n",
       "  'The dataset viewer provides a /statistics endpoint for fetching some basic statistics precomputed for a requested dataset. This will get you a quick insight on how the data is distributed.'],\n",
       " ['https://huggingface.co/docs/datasets-server/parquet_process',\n",
       "  'Overview',\n",
       "  'The dataset viewer automatically converts and publishes public datasets less than 5GB on the Hub as Parquet files. If the dataset is already in Parquet format, it will be published as is. Parquet files are column-based and they shine when youre working with big data.'],\n",
       " ['https://huggingface.co/docs/datasets-server/clickhouse',\n",
       "  'ClickHouse',\n",
       "  'ClickHouse is a fast and efficient column-oriented database for analytical workloads, making it easy to analyze Hub-hosted datasets with SQL. To get started quickly, use clickhouse-local to run SQL queries from the command line and avoid the need to fully install ClickHouse.'],\n",
       " ['https://huggingface.co/docs/datasets-server/duckdb',\n",
       "  'DuckDB',\n",
       "  'DuckDB is a database that supports reading and querying Parquet files really fast. Begin by creating a connection to DuckDB, and then install and load the httpfs extension to read and write remote files:'],\n",
       " ['https://huggingface.co/docs/datasets-server/pandas',\n",
       "  'Pandas',\n",
       "  'Pandas is a popular DataFrame library for data analysis.'],\n",
       " ['https://huggingface.co/docs/datasets-server/polars',\n",
       "  'Polars',\n",
       "  'Polars is a fast DataFrame library written in Rust with Arrow as its foundation.'],\n",
       " ['https://huggingface.co/docs/datasets-server/configs_and_splits',\n",
       "  'Splits and configurations',\n",
       "  'Machine learning datasets are commonly organized in splits and they may also have configurations. These internal structures provide the scaffolding for building out a dataset, and determines how a dataset should be split and organized. Understanding a datasets structure can help you create your own dataset, and know which subset of data you should use when during model training and evaluation.'],\n",
       " ['https://huggingface.co/docs/datasets-server/data_types',\n",
       "  'Data types',\n",
       "  'Datasets supported by the dataset viewer have a tabular format, meaning a data point is represented in a row and its features are contained in columns. Using the /first-rows endpoint allows you to preview the first 100 rows of a dataset and information about each feature. Within the features key, youll notice it returns a _type field. This value describes the data type of the column, and it is also known as a datasets Features.'],\n",
       " ['https://huggingface.co/docs/datasets-server/server',\n",
       "  'Server infrastructure',\n",
       "  'The dataset viewer has two main components that work together to return queries about a dataset instantly:'],\n",
       " ['https://huggingface.co/docs/trl/index',\n",
       "  'TRL',\n",
       "  'TRL is a full stack library where we provide a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. The library is integrated with  transformers.'],\n",
       " ['https://huggingface.co/docs/trl/quickstart',\n",
       "  'Quickstart',\n",
       "  'Fine-tuning a language model via PPO consists of roughly three steps:'],\n",
       " ['https://huggingface.co/docs/trl/installation',\n",
       "  'Installation',\n",
       "  'You can install TRL either from pypi or from source:'],\n",
       " ['https://huggingface.co/docs/trl/how_to_train',\n",
       "  'PPO Training FAQ',\n",
       "  'When performing classical supervised fine-tuning of language models, the loss (especially the validation loss) serves as a good indicator of the training progress. However, in Reinforcement Learning (RL), the loss becomes less informative about the models performance, and its value may fluctuate while the actual performance improves.'],\n",
       " ['https://huggingface.co/docs/trl/use_model',\n",
       "  'Use Trained Models',\n",
       "  'Once you have trained a model using either the SFTTrainer, PPOTrainer, or DPOTrainer, you will have a fine-tuned model that can be used for text generation. In this section, well walk through the process of loading the fine-tuned model and generating text. If you need to run an inference server with the trained model, you can explore libraries such as text-generation-inference.'],\n",
       " ['https://huggingface.co/docs/trl/customization',\n",
       "  'Customize the Training',\n",
       "  'TRL is designed with modularity in mind so that users to be able to efficiently customize the training loop for their needs. Below are some examples on how you can apply and test different techniques.'],\n",
       " ['https://huggingface.co/docs/trl/logging',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/models',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/reward_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/sft_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/ppo_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/best_of_n',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/dpo_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/ddpo_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/iterative_sft_trainer',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/text_environments',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/example_overview',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/sentiment_tuning',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/lora_tuning_peft',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/detoxifying_a_lm',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/using_llama_models',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/learning_tools',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/trl/multi_adapter_rl',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/sagemaker/index',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/sagemaker/getting-started',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/sagemaker/train',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/sagemaker/inference',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/sagemaker/reference',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/index',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/quickstart',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/installation',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/feature_extraction',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/training_script',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/hf_hub',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/reference/models',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/reference/data',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/reference/optimizers',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/timm/reference/schedulers',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/index',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/speed',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/torch_shared_tensors',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/metadata_parsing',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/convert-weights',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/api/torch',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/api/tensorflow',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/api/paddle',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/api/flax',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/safetensors/api/numpy',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/index',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/quicktour',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/installation',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/supported_models',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/messages_api',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/gated_model_access',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/using_cli',\n",
       "  'Understanding Logs',\n",
       "  'As reinforcement learning algorithms are historically challenging to debug, its important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher',\n",
       "  'All TGI CLI options',\n",
       "  ''],\n",
       " ['https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models',\n",
       "  'Non-core Model Serving',\n",
       "  'TGI supports various LLM architectures (see full list here). If you wish to serve a model that is not one of the supported models, TGI will fallback to the transformers implementation of that model. This means you will be unable to use some of the features introduced by TGI, such as tensor-parallel sharding or flash attention. However, you can still get many benefits of TGI, such as continuous batching or streaming outputs.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/streaming',\n",
       "  'Streaming',\n",
       "  'Token streaming is the mode in which the server returns the tokens one by one as the model generates them. This enables showing progressive generations to the user rather than waiting for the whole generation. Streaming is an essential aspect of the end-user experience as it reduces latency, one of the most critical aspects of a smooth experience.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/quantization',\n",
       "  'Quantization',\n",
       "  'TGI offers GPTQ and bits-and-bytes quantization to quantize large language models.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism',\n",
       "  'Tensor Parallelism',\n",
       "  'Tensor parallelism is a technique used to fit a large model in multiple GPUs. For example, when multiplying the input tensors with the first weight tensor, the matrix multiplication is equivalent to splitting the weight tensor column-wise, multiplying each column with the input separately, and then concatenating the separate outputs. These outputs are then transferred from the GPUs and concatenated together to get the final result, like below '],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention',\n",
       "  'PagedAttention',\n",
       "  'LLMs struggle with memory limitations during generation. In the decoding part of generation, all the attention keys and values generated for previous tokens are stored in GPU memory for reuse. This is called KV cache, and it may take up a large amount of memory for large models and long sequences.'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/safetensors',\n",
       "  'Safetensors',\n",
       "  'Safetensors is a model serialization format for deep learning models. It is faster and safer compared to other serialization formats like pickle (which is used under the hood in many deep learning libraries).'],\n",
       " ['https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention',\n",
       "  'Flash Attention',\n",
       "  'Scaling the transformer architecture is heavily bottlenecked by the self-attention mechanism, which has quadratic time and memory complexity. Recent developments in accelerator hardware mainly focus on enhancing compute capacities and not memory and transferring data between hardware. This results in attention operation having a memory bottleneck. Flash Attention is an attention algorithm used to reduce this problem and scale transformer-based models more efficiently, enabling faster training and inference.'],\n",
       " ['https://huggingface.co/docs/autotrain/index',\n",
       "  ' AutoTrain',\n",
       "  ' AutoTrain is a no-code tool for training state-of-the-art models for Natural Language Processing (NLP) tasks, for Computer Vision (CV) tasks, and for Speech tasks and even for Tabular tasks. It is built on top of the awesome tools developed by the Hugging Face team, and it is designed to be easy to use.'],\n",
       " ['https://huggingface.co/docs/autotrain/getting_started',\n",
       "  'Installation',\n",
       "  'There is no installation required! AutoTrain Advanced runs on Hugging Face Spaces. All you need to do is create a new space with the AutoTrain Advanced template: https://huggingface.co/new-space?template=autotrain-projects/autotrain-advanced. Please make sure you keep the space private.'],\n",
       " ['https://huggingface.co/docs/autotrain/cost',\n",
       "  'How much does it cost?',\n",
       "  'AutoTrain provides you with best models which are deployable with just a few clicks. Unlike other services, we dont own your models. Once the training is done, you can download them and use them anywhere you want.'],\n",
       " ['https://huggingface.co/docs/autotrain/support',\n",
       "  'Get help and support',\n",
       "  'To get help and support for autotrain, there are 3 ways:'],\n",
       " ['https://huggingface.co/docs/autotrain/text_classification',\n",
       "  'Text Classification',\n",
       "  'Training a text classification model with AutoTrain is super-easy! Get your data ready in proper format and then with just a few clicks, your state-of-the-art model will be ready to be used in production.'],\n",
       " ['https://huggingface.co/docs/autotrain/llm_finetuning',\n",
       "  'LLM Finetuning',\n",
       "  'With AutoTrain, you can easily finetune large language models (LLMs) on your own data!'],\n",
       " ['https://huggingface.co/docs/autotrain/image_classification',\n",
       "  'Image Classification',\n",
       "  'Image classification is a supervised learning problem: define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos. Using AutoTrain, its super-easy to train a state-of-the-art image classification model. Just upload a set of images, and AutoTrain will automatically train a model to classify them.'],\n",
       " ['https://huggingface.co/docs/autotrain/dreambooth',\n",
       "  'DreamBooth',\n",
       "  'DreamBooth is a method to personalize text-to-image models like Stable Diffusion given just a few (3-5) images of a subject. It allows the model to generate contextualized images of the subject in different scenes, poses, and views.'],\n",
       " ['https://huggingface.co/docs/autotrain/seq2seq',\n",
       "  'Seq2Seq',\n",
       "  'Seq2Seq is a task that involves converting a sequence of words into another sequence of words. It is used in machine translation, text summarization, and question answering.'],\n",
       " ['https://huggingface.co/docs/autotrain/token_classification',\n",
       "  'Token Classification',\n",
       "  'Token classification is the task of classifying each token in a sequence. This can be used for Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and more. Get your data ready in proper format and then with just a few clicks, your state-of-the-art model will be ready to be used in production.'],\n",
       " ['https://huggingface.co/docs/autotrain/tabular',\n",
       "  'Tabular',\n",
       "  'Using AutoTrain, you can train a model to classify or regress tabular data easily. All you need to do is select from a list of models and upload your dataset. Parameter tuning is done automatically.'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/index',\n",
       "  'Text Embeddings Inference',\n",
       "  'Text Embeddings Inference (TEI) is a comprehensive toolkit designed for efficient deployment and serving of open source text embeddings models. It enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE, and E5.'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/quick_tour',\n",
       "  'Quick Tour',\n",
       "  'The easiest way to get started with TEI is to use one of the official Docker containers (see Supported models and hardware to choose the right container).'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/supported_models',\n",
       "  'Supported models and hardware',\n",
       "  'We are continually expanding our support for other model types and plan to include them in future updates.'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/local_cpu',\n",
       "  'Using TEI locally with CPU',\n",
       "  'You can install text-embeddings-inference locally to run it on your own machine. Here are the step-by-step instructions for installation:'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/local_metal',\n",
       "  'Using TEI locally with Metal',\n",
       "  'You can install text-embeddings-inference locally to run it on your own Mac with Metal support. Here are the step-by-step instructions for installation:'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/local_gpu',\n",
       "  'Using TEI locally with GPU',\n",
       "  'You can install text-embeddings-inference locally to run it on your own machine with a GPU. To make sure that your hardware is supported, check out the Supported models and hardware page.'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/private_models',\n",
       "  'Serving private and gated models',\n",
       "  'If the model you wish to serve is behind gated access or resides in a private model repository on Hugging Face Hub, you will need to have access to the model to serve it.'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/custom_container',\n",
       "  'Build custom container for TEI',\n",
       "  'You can build our own CPU or CUDA TEI container using Docker. To build a CPU container, run the following command in the directory containing your custom Dockerfile:'],\n",
       " ['https://huggingface.co/docs/text-embeddings-inference/cli_arguments',\n",
       "  'CLI arguments',\n",
       "  'To see all options to serve your models, run the following:'],\n",
       " ['https://huggingface.co/docs/competitions/index',\n",
       "  ' Competitions',\n",
       "  'Create a machine learning competition for your organization, friends or the world!'],\n",
       " ['https://huggingface.co/docs/competitions/pricing',\n",
       "  'Pricing',\n",
       "  'Creating a competition is free. However, you will need to pay for the compute resources used to run the competition. The cost of the compute resources depends the type of competition you create.'],\n",
       " ['https://huggingface.co/docs/competitions/create_competition',\n",
       "  'Create competition',\n",
       "  'Creating a competition is super easy and you have full control over the data, evaluation metric and the hardware used.'],\n",
       " ['https://huggingface.co/docs/competitions/competition_repo',\n",
       "  'Competition repo',\n",
       "  'NOTE: Competition repo must always be kept private. Do NOT make it public!'],\n",
       " ['https://huggingface.co/docs/competitions/competition_space',\n",
       "  'Competition space',\n",
       "  'A competition space is a Hugging Face Space where the actual competition takes place. It is a space where you can submit your model and get a score. It is also a space where competitors can see the leaderboard, discuss, and make submissions.'],\n",
       " ['https://huggingface.co/docs/competitions/custom_metric',\n",
       "  'Custom metric',\n",
       "  'In case you dont settle for the default scikit-learn metrics, you can define your own metric.'],\n",
       " ['https://huggingface.co/docs/competitions/submit',\n",
       "  'Making a submission',\n",
       "  'The submission format and example submissions are usually provided by the competition organizer. This page describes how to make submissions using the competition UI.'],\n",
       " ['https://huggingface.co/docs/competitions/leaderboard',\n",
       "  'Leaderboard',\n",
       "  'There are two types of leaderboards for all competitions:'],\n",
       " ['https://huggingface.co/docs/competitions/teams', 'Teams', 'Coming soon!']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('hf.json', 'r') as f:\n",
    "    all_sublinks = json.load(f)\n",
    "\n",
    "((all_sublinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chait\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\inference\\_generated\\types\\base.py:139: FutureWarning: Accessing 'TextGenerationOutput' values through dict is deprecated and will be removed from version '0.25'. Use dataclass attributes instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://huggingface.co/docs/datasets/process', 'https://huggingface.co/docs/datasets/quickstart', 'https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map', 'https://huggingface.co/docs/datasets/load_dataset']\n",
      "\n",
      "Process\n",
      " Datasets provides many tools for modifying the structure and content of a dataset. These tools are important for tidying up a dataset, creating additional columns, converting between features and formats, and much more.\n",
      "This guide will show you how to:\n",
      "- Reorder rows and split the dataset.\n",
      "- Rename and remove columns, and other common column operations.\n",
      "- Apply processing functions to each example in a dataset.\n",
      "- Concatenate datasets.\n",
      "- Apply a custom formatting transform.\n",
      "- Save and export processed datasets.\n",
      "For more details specific to processing other dataset modalities, take a look at the process audio dataset guide, the process image dataset guide, or the process text dataset guide.\n",
      "The examples in this guide use the MRPC dataset, but feel free to load any dataset of your choice and follow along!\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
      "```\n",
      "All processing methods in this guide return a new Dataset object. Modification is not done in-place. Be careful about overriding your previous dataset!\n",
      "Sort, shuffle, select, split, and shard\n",
      "There are several functions for rearranging the structure of a dataset. These functions are useful for selecting only the rows you want, creating train and test splits, and sharding very large datasets into smaller chunks.\n",
      "Sort\n",
      "Use sort() to sort column values according to their numerical values. The provided column must be NumPy compatible.\n",
      "```\n",
      ">>> dataset[\"label\"][:10]\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      ">>> sorted_dataset = dataset.sort(\"label\")\n",
      ">>> sorted_dataset[\"label\"][:10]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      ">>> sorted_dataset[\"label\"][-10:]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "```\n",
      "Under the hood, this creates a list of indices that is sorted according to values of the column. This indices mapping is then used to access the right rows in the underlying Arrow table.\n",
      "Shuffle\n",
      "The shuffle() function randomly rearranges the column values. You can specify the generator parameter in this function to use a different numpy.random.Generator if you want more control over the algorithm used to shuffle the dataset.\n",
      "```\n",
      ">>> shuffled_dataset = sorted_dataset.shuffle(seed=42)\n",
      ">>> shuffled_dataset[\"label\"][:10]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
      "```\n",
      "Shuffling takes the list of indices [0:len(my_dataset)] and shuffles it to create an indices mapping. However as soon as your Dataset has an indices mapping, the speed can become 10x slower. This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you arent reading contiguous chunks of data anymore. To restore the speed, youd need to rewrite the entire dataset on your disk again using Dataset.flatten_indices(), which removes the indices mapping. Alternatively, you can switch to an IterableDataset and leverage its fast approximate shuffling IterableDataset.shuffle():\n",
      "```\n",
      ">>> iterable_dataset = dataset.to_iterable_dataset(num_shards=128)\n",
      ">>> shuffled_iterable_dataset = iterable_dataset.shuffle(seed=42, buffer_size=1000)\n",
      "```\n",
      "Select and Filter\n",
      "There are two options for filtering rows in a dataset: select() and filter().\n",
      "- select() returns rows according to a list of indices:\n",
      "```\n",
      ">>> small_dataset = dataset.select([0, 10, 20, 30, 40, 50])\n",
      ">>> len(small_dataset)\n",
      "6\n",
      "```\n",
      "- filter() returns rows that match a specified condition:\n",
      "```\n",
      ">>> start_with_ar = dataset.filter(lambda example: example[\"sentence1\"].startswith(\"Ar\"))\n",
      ">>> len(start_with_ar)\n",
      "6\n",
      ">>> start_with_ar[\"sentence1\"]\n",
      "['Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
      "'Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .',\n",
      "'Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .',\n",
      "'Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .',\n",
      "\"Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo 's ban is not justified on scientific grounds .\",\n",
      "'Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .'\n",
      "]\n",
      "```\n",
      "filter() can also filter by indices if you set with_indices=True:\n",
      "```\n",
      ">>> even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)\n",
      ">>> len(even_dataset)\n",
      "1834\n",
      ">>> len(dataset) / 2\n",
      "1834.0\n",
      "```\n",
      "Unless the list of indices to keep is contiguous, those methods also create an indices mapping under the hood.\n",
      "Split\n",
      "The train_test_split() function creates train and test splits if your dataset doesnt already have them. This allows you to adjust the relative proportions or an absolute number of samples in each split. In the example below, use the test_size parameter to create a test split that is 10% of the original dataset:\n",
      "```\n",
      ">>> dataset.train_test_split(test_size=0.1)\n",
      "{'train': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 3301),\n",
      "'test': Dataset(schema: {'sentence1': 'string', 'sentence2': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 367)}\n",
      ">>> 0.1 * len(dataset)\n",
      "366.8\n",
      "```\n",
      "The splits are shuffled by default, but you can set shuffle=False to prevent shuffling.\n",
      "Shard\n",
      " Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the num_shards parameter in shard() to determine the number of shards to split the dataset into. Youll also need to provide the shard you want to return with the index parameter.\n",
      "For example, the imdb dataset has 25000 examples:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> datasets = load_dataset(\"imdb\", split=\"train\")\n",
      ">>> print(dataset)\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "```\n",
      "After sharding the dataset into four chunks, the first shard will only have 6250 examples:\n",
      "```\n",
      ">>> dataset.shard(num_shards=4, index=0)\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 6250\n",
      "})\n",
      ">>> print(25000/4)\n",
      "6250.0\n",
      "```\n",
      "Rename, remove, cast, and flatten\n",
      "The following functions allow you to modify the columns of a dataset. These functions are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures.\n",
      "Rename\n",
      "Use rename_column() when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place.\n",
      "Provide rename_column() with the name of the original column, and the new column name:\n",
      "```\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      ">>> dataset = dataset.rename_column(\"sentence1\", \"sentenceA\")\n",
      ">>> dataset = dataset.rename_column(\"sentence2\", \"sentenceB\")\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['sentenceA', 'sentenceB', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "```\n",
      "Remove\n",
      "When you need to remove one or more columns, provide the column name to remove to the remove_columns() function. Remove more than one column by providing a list of column names:\n",
      "```\n",
      ">>> dataset = dataset.remove_columns(\"label\")\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      ">>> dataset = dataset.remove_columns([\"sentence1\", \"sentence2\"])\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "```\n",
      "Conversely, select_columns() selects one or more columns to keep and removes the rest. This function takes either one or a list of column names:\n",
      "```\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      ">>> dataset = dataset.select_columns(['sentence1', 'sentence2', 'idx'])\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['sentence1', 'sentence2', 'idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      ">>> dataset = dataset.select_columns('idx')\n",
      ">>> dataset\n",
      "Dataset({\n",
      "    features: ['idx'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "```\n",
      "Cast\n",
      "The cast() function transforms the feature type of one or more columns. This function accepts your new Features as its argument. The example below demonstrates how to change the ClassLabel and Value features:\n",
      "```\n",
      ">>> dataset.features\n",
      "{'sentence1': Value(dtype='string', id=None),\n",
      "'sentence2': Value(dtype='string', id=None),\n",
      "'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
      "'idx': Value(dtype='int32', id=None)}\n",
      "\n",
      ">>> from datasets import ClassLabel, Value\n",
      ">>> new_features = dataset.features.copy()\n",
      ">>> new_features[\"label\"] = ClassLabel(names=[\"negative\", \"positive\"])\n",
      ">>> new_features[\"idx\"] = Value(\"int64\")\n",
      ">>> dataset = dataset.cast(new_features)\n",
      ">>> dataset.features\n",
      "{'sentence1': Value(dtype='string', id=None),\n",
      "'sentence2': Value(dtype='string', id=None),\n",
      "'label': ClassLabel(num_classes=2, names=['negative', 'positive'], names_file=None, id=None),\n",
      "'idx': Value(dtype='int64', id=None)}\n",
      "```\n",
      "Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type Value(\"int32\") to Value(\"bool\") if the original column only contains ones and zeros.\n",
      "Use the cast_column() function to change the feature type of a single column. Pass the column name and its new feature type as arguments:\n",
      "```\n",
      ">>> dataset.features\n",
      "{'audio': Audio(sampling_rate=44100, mono=True, id=None)}\n",
      "\n",
      ">>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
      ">>> dataset.features\n",
      "{'audio': Audio(sampling_rate=16000, mono=True, id=None)}\n",
      "```\n",
      "Flatten\n",
      "Sometimes a column can be a nested structure of several types. Take a look at the nested structure below from the SQuAD dataset:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> dataset = load_dataset(\"squad\", split=\"train\")\n",
      ">>> dataset.features\n",
      "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      "'context': Value(dtype='string', id=None),\n",
      "'id': Value(dtype='string', id=None),\n",
      "'question': Value(dtype='string', id=None),\n",
      "'title': Value(dtype='string', id=None)}\n",
      "```\n",
      "The answers field contains two subfields: text and answer_start. Use the flatten() function to extract the subfields into their own separate columns:\n",
      "```\n",
      ">>> flat_dataset = dataset.flatten()\n",
      ">>> flat_dataset\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " num_rows: 87599\n",
      "})\n",
      "```\n",
      "Notice how the subfields are now their own independent columns: answers.text and answers.answer_start.\n",
      "Map\n",
      "Some of the more powerful applications of  Datasets come from using the map() function. The primary purpose of map() is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns.\n",
      "In the following example, prefix each sentence1 value in the dataset with 'My sentence: '.\n",
      "Start by creating a function that adds 'My sentence: ' to the beginning of each sentence. The function needs to accept and output a dict:\n",
      "```\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"sentence1\"] = 'My sentence: ' + example[\"sentence1\"]\n",
      "...     return example\n",
      "```\n",
      "Now use map() to apply the add_prefix function to the entire dataset:\n",
      "```\n",
      ">>> updated_dataset = small_dataset.map(add_prefix)\n",
      ">>> updated_dataset[\"sentence1\"][:5]\n",
      "['My sentence: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
      "\"My sentence: Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
      "'My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .',\n",
      "'My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .',\n",
      "]\n",
      "```\n",
      "Lets take a look at another example, except this time, youll remove a column with map(). When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed.\n",
      "Specify the column to remove with the remove_columns parameter in map():\n",
      "```\n",
      ">>> updated_dataset = dataset.map(lambda example: {\"new_sentence\": example[\"sentence1\"]}, remove_columns=[\"sentence1\"])\n",
      ">>> updated_dataset.column_names\n",
      "['sentence2', 'label', 'idx', 'new_sentence']\n",
      "```\n",
      " Datasets also has a remove_columns() function which is faster because it doesnt copy the data of the remaining columns.\n",
      "You can also use map() with indices if you set with_indices=True. The example below adds the index to the beginning of each sentence:\n",
      "```\n",
      ">>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, with_indices=True)\n",
      ">>> updated_dataset[\"sentence2\"][:5]\n",
      "['0: Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
      " \"1: Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\",\n",
      " \"2: On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\",\n",
      " '3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .',\n",
      " '4: PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .'\n",
      "]\n",
      "```\n",
      "Multiprocessing\n",
      "Multiprocessing significantly speeds up processing by parallelizing processes on the CPU. Set the num_proc parameter in map() to set the number of processes to use:\n",
      "```\n",
      ">>> updated_dataset = dataset.map(lambda example, idx: {\"sentence2\": f\"{idx}: \" + example[\"sentence2\"]}, num_proc=4)\n",
      "```\n",
      "The map() also works with the rank of the process if you set with_rank=True. This is analogous to the with_indices parameter. The with_rank parameter in the mapped function goes after the index one if it is already present.\n",
      "```\n",
      ">>> import torch\n",
      ">>> from multiprocess import set_start_method\n",
      ">>> from transformers import AutoTokenizer, AutoModelForCausalLM \n",
      ">>> from datasets import load_dataset\n",
      ">>> \n",
      ">>> # Get an example dataset\n",
      ">>> dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\n",
      ">>> \n",
      ">>> # Get an example model and its tokenizer \n",
      ">>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\").eval()\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\")\n",
      ">>>\n",
      ">>> def gpu_computation(batch, rank):\n",
      "...     # Move the model on the right GPU if it's not there already\n",
      "...     device = f\"cuda:{(rank or 0) % torch.cuda.device_count()}\"\n",
      "...     model.to(device)\n",
      "...     \n",
      "...     # Your big GPU call goes here, for example:\n",
      "...     chats = [[\n",
      "...         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "...         {\"role\": \"user\", \"content\": prompt}\n",
      "...     ] for prompt in batch[\"prompt\"]]\n",
      "...     texts = [tokenizer.apply_chat_template(\n",
      "...         chat,\n",
      "...         tokenize=False,\n",
      "...         add_generation_prompt=True\n",
      "...     ) for chat in chats]\n",
      "...     model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
      "...     with torch.no_grad():\n",
      "...         outputs = model.generate(**model_inputs, max_new_tokens=512)\n",
      "...     batch[\"output\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
      "...     return batch\n",
      ">>>\n",
      ">>> if __name__ == \"__main__\":\n",
      "...     set_start_method(\"spawn\")\n",
      "...     updated_dataset = dataset.map(\n",
      "...         gpu_computation,\n",
      "...         batched=True,\n",
      "...         batch_size=16,\n",
      "...         with_rank=True,\n",
      "...         num_proc=torch.cuda.device_count(),  # one process per GPU\n",
      "...     )\n",
      "```\n",
      "The main use-case for rank is to parallelize computation across several GPUs. This requires setting multiprocess.set_start_method(\"spawn\"). If you dont youll receive the following CUDA error:\n",
      "```\n",
      "RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.\n",
      "```\n",
      "Batch processing\n",
      "The map() function supports working with batches of examples. Operate on batches by setting batched=True. The default batch size is 1000, but you can adjust it with the batch_size parameter. Batch processing enables interesting applications such as splitting long sentences into shorter chunks and data augmentation.\n",
      "Split long examples\n",
      "When examples are too long, you may want to split them into several smaller chunks. Begin by creating a function that:\n",
      "Splits the sentence1 field into chunks of 50 characters.\n",
      "Stacks all the chunks together to create the new dataset.\n",
      "```\n",
      ">>> def chunk_examples(examples):\n",
      "...     chunks = []\n",
      "...     for sentence in examples[\"sentence1\"]:\n",
      "...         chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n",
      "...     return {\"chunks\": chunks}\n",
      "```\n",
      "Apply the function with map():\n",
      "```\n",
      ">>> chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)\n",
      ">>> chunked_dataset[:10]\n",
      "{'chunks': ['Amrozi accused his brother , whom he called \" the ',\n",
      "            'witness \" , of deliberately distorting his evidenc',\n",
      "            'e .',\n",
      "            \"Yucaipa owned Dominick 's before selling the chain\",\n",
      "            ' to Safeway in 1998 for $ 2.5 billion .',\n",
      "            'They had published an advertisement on the Interne',\n",
      "            't on June 10 , offering the cargo for sale , he ad',\n",
      "            'ded .',\n",
      "            'Around 0335 GMT , Tab shares were up 19 cents , or',\n",
      "            ' 4.4 % , at A $ 4.56 , having earlier set a record']}\n",
      "```\n",
      "Notice how the sentences are split into shorter chunks now, and there are more rows in the dataset.\n",
      "```\n",
      ">>> dataset\n",
      "Dataset({\n",
      " features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      " num_rows: 3668\n",
      "})\n",
      ">>> chunked_dataset\n",
      "Dataset({\n",
      "    features: ['chunks'],\n",
      "    num_rows: 10470\n",
      "})\n",
      "```\n",
      "Data augmentation\n",
      "The map() function could also be used for data augmentation. The following example generates additional words for a masked token in a sentence.\n",
      "Load and use the RoBERTA model in  Transformers FillMaskPipeline:\n",
      "```\n",
      ">>> from random import randint\n",
      ">>> from transformers import pipeline\n",
      "\n",
      ">>> fillmask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
      ">>> mask_token = fillmask.tokenizer.mask_token\n",
      ">>> smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)\n",
      "```\n",
      "Create a function to randomly select a word to mask in the sentence. The function should also return the original sentence and the top two replacements generated by RoBERTA.\n",
      "```\n",
      ">>> def augment_data(examples):\n",
      "...     outputs = []\n",
      "...     for sentence in examples[\"sentence1\"]:\n",
      "...         words = sentence.split(' ')\n",
      "...         K = randint(1, len(words)-1)\n",
      "...         masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n",
      "...         predictions = fillmask(masked_sentence)\n",
      "...         augmented_sequences = [predictions[i][\"sequence\"] for i in range(3)]\n",
      "...         outputs += [sentence] + augmented_sequences\n",
      "...\n",
      "...     return {\"data\": outputs}\n",
      "```\n",
      "Use map() to apply the function over the whole dataset:\n",
      "```\n",
      ">>> augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)\n",
      ">>> augmented_dataset[:9][\"data\"]\n",
      "['Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
      " 'Amrozi accused his brother, whom he called \" the witness \", of deliberately withholding his evidence.',\n",
      " 'Amrozi accused his brother, whom he called \" the witness \", of deliberately suppressing his evidence.',\n",
      " 'Amrozi accused his brother, whom he called \" the witness \", of deliberately destroying his evidence.',\n",
      " \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\",\n",
      " 'Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.',\n",
      " \"Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $ 2.5 billion.\",\n",
      " 'Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.'\n",
      "]\n",
      "```\n",
      "For each original sentence, RoBERTA augmented a random word with three alternatives. The original word distorting is supplemented by withholding, suppressing, and destroying.\n",
      "Process multiple splits\n",
      "Many datasets have splits that can be processed simultaneously with DatasetDict.map(). For example, tokenize the sentence1 field in the train and test split by:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      "\n",
      "# load all the splits\n",
      ">>> dataset = load_dataset('glue', 'mrpc')\n",
      ">>> encoded_dataset = dataset.map(lambda examples: tokenizer(examples[\"sentence1\"]), batched=True)\n",
      ">>> encoded_dataset[\"train\"][0]\n",
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
      "'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
      "'label': 1,\n",
      "'idx': 0,\n",
      "'input_ids': [  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102],\n",
      "'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "}\n",
      "```\n",
      "Distributed usage\n",
      "When you use map() in a distributed setting, you should also use torch.distributed.barrier. This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work.\n",
      "The following example shows how you can use torch.distributed.barrier to synchronize the processes:\n",
      "```\n",
      ">>> from datasets import Dataset\n",
      ">>> import torch.distributed\n",
      "\n",
      ">>> dataset1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
      "\n",
      ">>> if training_args.local_rank > 0:\n",
      "...     print(\"Waiting for main process to perform the mapping\")\n",
      "...     torch.distributed.barrier()\n",
      "\n",
      ">>> dataset2 = dataset1.map(lambda x: {\"a\": x[\"a\"] + 1})\n",
      "\n",
      ">>> if training_args.local_rank == 0:\n",
      "...     print(\"Loading results from main process\")\n",
      "...     torch.distributed.barrier()\n",
      "```\n",
      "Concatenate\n",
      "Separate datasets can be concatenated if they share the same column types. Concatenate datasets with concatenate_datasets():\n",
      "```\n",
      ">>> from datasets import concatenate_datasets, load_dataset\n",
      "\n",
      ">>> bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
      ">>> wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
      ">>> wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\n",
      "\n",
      ">>> assert bookcorpus.features.type == wiki.features.type\n",
      ">>> bert_dataset = concatenate_datasets([bookcorpus, wiki])\n",
      "```\n",
      "You can also concatenate two datasets horizontally by setting axis=1 as long as the datasets have the same number of rows:\n",
      "```\n",
      ">>> from datasets import Dataset\n",
      ">>> bookcorpus_ids = Dataset.from_dict({\"ids\": list(range(len(bookcorpus)))})\n",
      ">>> bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)\n",
      "```\n",
      "Interleave\n",
      "You can also mix several datasets together by taking alternating examples from each one to create a new dataset. This is known as interleaving, which is enabled by the interleave_datasets() function. Both interleave_datasets() and concatenate_datasets() work with regular Dataset and IterableDataset objects. Refer to the Stream guide for an example of how to interleave IterableDataset objects.\n",
      "You can define sampling probabilities for each of the original datasets to specify how to interleave the datasets. In this case, the new dataset is constructed by getting examples one by one from a random dataset until one of the datasets runs out of samples.\n",
      "```\n",
      ">>> seed = 42\n",
      ">>> probabilities = [0.3, 0.5, 0.2]\n",
      ">>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
      ">>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
      ">>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)\n",
      ">>> dataset[\"a\"]\n",
      "[10, 11, 20, 12, 0, 21, 13]\n",
      "```\n",
      "You can also specify the stopping_strategy. The default strategy, first_exhausted, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples. You can specify stopping_strategy=all_exhausted to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached. Note that if no sampling probabilities are specified, the new dataset will have max_length_datasets*nb_dataset samples.\n",
      "```\n",
      ">>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
      ">>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
      ">>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n",
      ">>> dataset[\"a\"]\n",
      "[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 20]\n",
      "```\n",
      "Format\n",
      "The set_format() function changes the format of a column to be compatible with some common data formats. Specify the output youd like in the type parameter and the columns you want to format. Formatting is applied on-the-fly.\n",
      "For example, create PyTorch tensors by setting type=\"torch\":\n",
      "```\n",
      ">>> import torch\n",
      ">>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
      "```\n",
      "The with_format() function also changes the format of a column, except it returns a new Dataset object:\n",
      "```\n",
      ">>> dataset = dataset.with_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"])\n",
      "```\n",
      " Datasets also provides support for other common data formats such as NumPy, Pandas, and JAX. Check out the Using Datasets with TensorFlow guide for more details on how to efficiently create a TensorFlow dataset.\n",
      "If you need to reset the dataset to its original format, use the reset_format() function:\n",
      "```\n",
      ">>> dataset.format\n",
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['label'], 'output_all_columns': False}\n",
      ">>> dataset.reset_format()\n",
      ">>> dataset.format\n",
      "{'type': 'python', 'format_kwargs': {}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\n",
      "```\n",
      "Format transform\n",
      "The set_transform() function applies a custom formatting transform on-the-fly. This function replaces any previously specified format. For example, you can use this function to tokenize and pad tokens on-the-fly. Tokenization is only applied when examples are accessed:\n",
      "```\n",
      ">>> from transformers import AutoTokenizer\n",
      "\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      ">>> def encode(batch):\n",
      "...     return tokenizer(batch[\"sentence1\"], padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
      ">>> dataset.set_transform(encode)\n",
      ">>> dataset.format\n",
      "{'type': 'custom', 'format_kwargs': {'transform': <function __main__.encode(batch)>}, 'columns': ['idx', 'label', 'sentence1', 'sentence2'], 'output_all_columns': False}\n",
      "```\n",
      "You can also use the set_transform() function to decode formats not supported by Features. For example, the Audio feature uses soundfile - a fast and simple library to install - but it does not provide support for less common audio formats. Here is where you can use set_transform() to apply a custom decoding transform on the fly. Youre free to use any library you like to decode the audio files.\n",
      "The example below uses the pydub package to open an audio format not supported by soundfile:\n",
      "```\n",
      ">>> import numpy as np\n",
      ">>> from pydub import AudioSegment\n",
      "\n",
      ">>> audio_dataset_amr = Dataset.from_dict({\"audio\": [\"audio_samples/audio.amr\"]})\n",
      "\n",
      ">>> def decode_audio_with_pydub(batch, sampling_rate=16_000):\n",
      "...     def pydub_decode_file(audio_path):\n",
      "...         sound = AudioSegment.from_file(audio_path)\n",
      "...         if sound.frame_rate != sampling_rate:\n",
      "...             sound = sound.set_frame_rate(sampling_rate)\n",
      "...         channel_sounds = sound.split_to_mono()\n",
      "...         samples = [s.get_array_of_samples() for s in channel_sounds]\n",
      "...         fp_arr = np.array(samples).T.astype(np.float32)\n",
      "...         fp_arr /= np.iinfo(samples[0].typecode).max\n",
      "...         return fp_arr\n",
      "...\n",
      "...     batch[\"audio\"] = [pydub_decode_file(audio_path) for audio_path in batch[\"audio\"]]\n",
      "...     return batch\n",
      "\n",
      ">>> audio_dataset_amr.set_transform(decode_audio_with_pydub)\n",
      "```\n",
      "Save\n",
      "Once you are done processing your dataset, you can save and reuse it later with save_to_disk().\n",
      "Save your dataset by providing the path to the directory you wish to save it to:\n",
      "```\n",
      ">>> encoded_dataset.save_to_disk(\"path/of/my/dataset/directory\")\n",
      "```\n",
      "Use the load_from_disk() function to reload the dataset:\n",
      "```\n",
      ">>> from datasets import load_from_disk\n",
      ">>> reloaded_dataset = load_from_disk(\"path/of/my/dataset/directory\")\n",
      "```\n",
      "Want to save your dataset to a cloud storage provider? Read our Cloud Storage guide to learn how to save your dataset to AWS or Google Cloud Storage.\n",
      "Export\n",
      " Datasets supports exporting as well so you can work with your dataset in other applications. The following table shows currently supported file formats you can export to:\n",
      "For example, export your dataset to a CSV file like this:\n",
      "```\n",
      ">>> encoded_dataset.to_csv(\"path/of/my/dataset.csv\")\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quickstart\n",
      "This quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate  Datasets into their model training workflow. If youre a beginner, we recommend starting with our tutorials, where youll get a more thorough introduction.\n",
      "Each dataset is unique, and depending on the task, some datasets may require additional steps to prepare it for training. But you can always use  Datasets tools to load and process a dataset. The fastest and easiest way to get started is by loading an existing dataset from the Hugging Face Hub. There are thousands of datasets to choose from, spanning many tasks. Choose the type of dataset you want to work with, and lets get started!\n",
      "Resample an audio dataset and get it ready for a model to classify what type of banking issue a speaker is calling about.\n",
      "Apply data augmentation to an image dataset and get it ready for a model to diagnose disease in bean plants.\n",
      "Tokenize a dataset and get it ready for a model to determine whether a pair of sentences have the same meaning.\n",
      "Check out Chapter 5 of the Hugging Face course to learn more about other important topics such as loading remote or local datasets, tools for cleaning up a dataset, and creating your own dataset.\n",
      "Start by installing  Datasets:\n",
      "```\n",
      "pip install datasets\n",
      "```\n",
      " Datasets also support audio and image data formats:\n",
      "- To work with audio datasets, install the Audio feature:\n",
      "pip install datasets[audio]\n",
      "- To work with image datasets, install the Image feature:\n",
      "pip install datasets[vision]\n",
      "To work with audio datasets, install the Audio feature:\n",
      "```\n",
      "pip install datasets[audio]\n",
      "```\n",
      "To work with image datasets, install the Image feature:\n",
      "```\n",
      "pip install datasets[vision]\n",
      "```\n",
      "Besides  Datasets, make sure your preferred machine learning framework is installed:\n",
      "```\n",
      "pip install torch\n",
      "```\n",
      "```\n",
      "pip install tensorflow\n",
      "```\n",
      "Audio\n",
      "Audio datasets are loaded just like text datasets. However, an audio dataset is preprocessed a bit differently. Instead of a tokenizer, youll need a feature extractor. An audio input may also require resampling its sampling rate to match the sampling rate of the pretrained model youre using. In this quickstart, youll prepare the MInDS-14 dataset for a model train on and classify the banking issue a customer is having.\n",
      "1. Load the MInDS-14 dataset by providing the load_dataset() function with the dataset name, dataset configuration (not all datasets will have a configuration), and a dataset split:\n",
      "```\n",
      ">>> from datasets import load_dataset, Audio\n",
      "\n",
      ">>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n",
      "```\n",
      "2. Next, load a pretrained Wav2Vec2 model and its corresponding feature extractor from the  Transformers library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task.\n",
      "```\n",
      ">>> from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
      "\n",
      ">>> model = AutoModelForAudioClassification.from_pretrained(\"facebook/wav2vec2-base\")\n",
      ">>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
      "```\n",
      "3. The MInDS-14 dataset card indicates the sampling rate is 8kHz, but the Wav2Vec2 model was pretrained on a sampling rate of 16kHZ. Youll need to upsample the audio column with the cast_column() function and Audio feature to match the models sampling rate.\n",
      "```\n",
      ">>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
      ">>> dataset[0][\"audio\"]\n",
      "{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n",
      "         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n",
      " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n",
      " 'sampling_rate': 16000}\n",
      "```\n",
      "4. Create a function to preprocess the audio array with the feature extractor, and truncate and pad the sequences into tidy rectangular tensors. The most important thing to remember is to call the audio array in the feature extractor since the array - the actual speech signal - is the model input.\n",
      "Once you have a preprocessing function, use the map() function to speed up processing by applying the function to batches of examples in the dataset.\n",
      "```\n",
      ">>> def preprocess_function(examples):\n",
      "...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
      "...     inputs = feature_extractor(\n",
      "...         audio_arrays,\n",
      "...         sampling_rate=16000,\n",
      "...         padding=True,\n",
      "...         max_length=100000,\n",
      "...         truncation=True,\n",
      "...     )\n",
      "...     return inputs\n",
      "\n",
      ">>> dataset = dataset.map(preprocess_function, batched=True)\n",
      "```\n",
      "5. Use the rename_column() function to rename the intent_class column to labels, which is the expected input name in Wav2Vec2ForSequenceClassification:\n",
      "```\n",
      ">>> dataset = dataset.rename_column(\"intent_class\", \"labels\")\n",
      "```\n",
      "6. Set the dataset format according to the machine learning framework youre using.\n",
      "Use the set_format() function to set the dataset format to torch and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in torch.utils.data.DataLoader:\n",
      "```\n",
      ">>> from torch.utils.data import DataLoader\n",
      "\n",
      ">>> dataset.set_format(type=\"torch\", columns=[\"input_values\", \"labels\"])\n",
      ">>> dataloader = DataLoader(dataset, batch_size=4)\n",
      "```\n",
      "Use the prepare_tf_dataset method from  Transformers to prepare the dataset to be compatible with TensorFlow, and ready to train/fine-tune a model, as it wraps a HuggingFace Dataset as a tf.data.Dataset with collation and batching, so one can pass it directly to Keras methods like fit() without further modification.\n",
      "```\n",
      ">>> import tensorflow as tf\n",
      "\n",
      ">>> tf_dataset = model.prepare_tf_dataset(\n",
      "...     dataset,\n",
      "...     batch_size=4,\n",
      "...     shuffle=True,\n",
      "... )\n",
      "```\n",
      "7. Start training with your machine learning framework! Check out the  Transformers audio classification guide for an end-to-end example of how to train a model on an audio dataset.\n",
      "Vision\n",
      "Image datasets are loaded just like text datasets. However, instead of a tokenizer, youll need a feature extractor to preprocess the dataset. Applying data augmentation to an image is common in computer vision to make the model more robust against overfitting. Youre free to use any data augmentation library you want, and then you can apply the augmentations with  Datasets. In this quickstart, youll load the Beans dataset and get it ready for the model to train on and identify disease from the leaf images.\n",
      "1. Load the Beans dataset by providing the load_dataset() function with the dataset name and a dataset split:\n",
      "```\n",
      ">>> from datasets import load_dataset, Image\n",
      "\n",
      ">>> dataset = load_dataset(\"beans\", split=\"train\")\n",
      "```\n",
      "2. Now you can add some data augmentations with any library (Albumentations, imgaug, Kornia) you like. Here, youll use torchvision to randomly change the color properties of an image:\n",
      "```\n",
      ">>> from torchvision.transforms import Compose, ColorJitter, ToTensor\n",
      "\n",
      ">>> jitter = Compose(\n",
      "...     [ColorJitter(brightness=0.5, hue=0.5), ToTensor()]\n",
      "... )\n",
      "```\n",
      "3. Create a function to apply your transform to the dataset and generate the model input: pixel_values.\n",
      "```\n",
      ">>> def transforms(examples):\n",
      "...     examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
      "...     return examples\n",
      "```\n",
      "4. Use the with_transform() function to apply the data augmentations on-the-fly:\n",
      "```\n",
      ">>> dataset = dataset.with_transform(transforms)\n",
      "```\n",
      "5. Set the dataset format according to the machine learning framework youre using.\n",
      "Wrap the dataset in torch.utils.data.DataLoader. Youll also need to create a collate function to collate the samples into batches:\n",
      "```\n",
      ">>> from torch.utils.data import DataLoader\n",
      "\n",
      ">>> def collate_fn(examples):\n",
      "...     images = []\n",
      "...     labels = []\n",
      "...     for example in examples:\n",
      "...         images.append((example[\"pixel_values\"]))\n",
      "...         labels.append(example[\"labels\"])\n",
      "...         \n",
      "...     pixel_values = torch.stack(images)\n",
      "...     labels = torch.tensor(labels)\n",
      "...     return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
      ">>> dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4)\n",
      "```\n",
      "Use the prepare_tf_dataset method from  Transformers to prepare the dataset to be compatible with TensorFlow, and ready to train/fine-tune a model, as it wraps a HuggingFace Dataset as a tf.data.Dataset with collation and batching, so one can pass it directly to Keras methods like fit() without further modification.\n",
      "Before you start, make sure you have up-to-date versions of albumentations and cv2 installed:\n",
      "```\n",
      "pip install -U albumentations opencv-python\n",
      "```\n",
      "```\n",
      ">>> import albumentations\n",
      ">>> import numpy as np\n",
      "\n",
      ">>> transform = albumentations.Compose([\n",
      "...     albumentations.RandomCrop(width=256, height=256),\n",
      "...     albumentations.HorizontalFlip(p=0.5),\n",
      "...     albumentations.RandomBrightnessContrast(p=0.2),\n",
      "... ])\n",
      "\n",
      ">>> def transforms(examples):\n",
      "...     examples[\"pixel_values\"] = [\n",
      "...         transform(image=np.array(image))[\"image\"] for image in examples[\"image\"]\n",
      "...     ]\n",
      "...     return examples\n",
      "\n",
      ">>> dataset.set_transform(transforms)\n",
      ">>> tf_dataset = model.prepare_tf_dataset(\n",
      "...     dataset,\n",
      "...     batch_size=4,\n",
      "...     shuffle=True,\n",
      "... )\n",
      "```\n",
      "6. Start training with your machine learning framework! Check out the  Transformers image classification guide for an end-to-end example of how to train a model on an image dataset.\n",
      "NLP\n",
      "Text needs to be tokenized into individual tokens by a tokenizer. For the quickstart, youll load the Microsoft Research Paraphrase Corpus (MRPC) training dataset to train a model to determine whether a pair of sentences mean the same thing.\n",
      "1. Load the MRPC dataset by providing the load_dataset() function with the dataset name, dataset configuration (not all datasets will have a configuration), and dataset split:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      "\n",
      ">>> dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
      "```\n",
      "2. Next, load a pretrained BERT model and its corresponding tokenizer from the  Transformers library. It is totally normal to see a warning after you load the model about some weights not being initialized. This is expected because you are loading this model checkpoint for training with another task.\n",
      "```\n",
      " Pytorch\n",
      "  TensorFlow\n",
      " >>> from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      ">>> model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "```\n",
      "Pytorch\n",
      "TensorFlow\n",
      "3. Create a function to tokenize the dataset, and you should also truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: input_ids, token_type_ids, and an attention_mask. These are the model inputs.\n",
      "Use the map() function to speed up processing by applying your tokenization function to batches of examples in the dataset:\n",
      "```\n",
      ">>> def encode(examples):\n",
      "...     return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
      "\n",
      ">>> dataset = dataset.map(encode, batched=True)\n",
      ">>> dataset[0]\n",
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
      "'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
      "'label': 1,\n",
      "'idx': 0,\n",
      "'input_ids': array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102, 11336,  6732, 3384,  1106,  1140,  1112,  1178,   107,  1103,  7737,   107, 117,  7277,  2180,  5303,  4806,  1117,  1711,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102]),\n",
      "'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      "'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "```\n",
      "4. Rename the label column to labels, which is the expected input name in BertForSequenceClassification:\n",
      "```\n",
      ">>> dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
      "```\n",
      "5. Set the dataset format according to the machine learning framework youre using.\n",
      "Use the set_format() function to set the dataset format to torch and specify the columns you want to format. This function applies formatting on-the-fly. After converting to PyTorch tensors, wrap the dataset in torch.utils.data.DataLoader:\n",
      "```\n",
      ">>> import torch\n",
      "\n",
      ">>> dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
      ">>> dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
      "```\n",
      "Use the prepare_tf_dataset method from  Transformers to prepare the dataset to be compatible with TensorFlow, and ready to train/fine-tune a model, as it wraps a HuggingFace Dataset as a tf.data.Dataset with collation and batching, so one can pass it directly to Keras methods like fit() without further modification.\n",
      "```\n",
      ">>> import tensorflow as tf\n",
      "\n",
      ">>> tf_dataset = model.prepare_tf_dataset(\n",
      "...     dataset,\n",
      "...     batch_size=4,\n",
      "...     shuffle=True,\n",
      "... )\n",
      "```\n",
      "6. Start training with your machine learning framework! Check out the  Transformers text classification guide for an end-to-end example of how to train a model on a text dataset.\n",
      "Whats next?\n",
      "This completes the  Datasets quickstart! You can load any text, audio, or image dataset with a single function and get it ready for your model to train on.\n",
      "For your next steps, take a look at our How-to guides and learn how to do more specific things like loading different dataset formats, aligning labels, and streaming large datasets. If youre interested in learning more about  Datasets core concepts, grab a cup of coffee and read our Conceptual Guides!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main classes\n",
      "DatasetInfo\n",
      "class datasets.DatasetInfo\n",
      "( description: str = <factory>citation: str = <factory>homepage: str = <factory>license: str = <factory>features: Optional = Nonepost_processed: Optional = Nonesupervised_keys: Optional = Nonetask_templates: Optional = Nonebuilder_name: Optional = Nonedataset_name: Optional = Noneconfig_name: Optional = Noneversion: Union = Nonesplits: Optional = Nonedownload_checksums: Optional = Nonedownload_size: Optional = Nonepost_processing_size: Optional = Nonedataset_size: Optional = Nonesize_in_bytes: Optional = None )\n",
      "Parameters\n",
      "- description (str)  A description of the dataset.\n",
      "- citation (str)  A BibTeX citation of the dataset.\n",
      "- homepage (str)  A URL to the official homepage for the dataset.\n",
      "- license (str)  The datasets license. It can be the name of the license or a paragraph containing the terms of the license.\n",
      "- features (Features, optional)  The features used to specify the datasets column types.\n",
      "- post_processed (PostProcessedInfo, optional)  Information regarding the resources of a possible post-processing of a dataset. For example, it can contain the information of an index.\n",
      "- supervised_keys (SupervisedKeysData, optional)  Specifies the input feature and the label for supervised learning if applicable for the dataset (legacy from TFDS).\n",
      "- builder_name (str, optional)  The name of the GeneratorBasedBuilder subclass used to create the dataset. Usually matched to the corresponding script name. It is also the snake_case version of the dataset builder class name.\n",
      "- config_name (str, optional)  The name of the configuration derived from BuilderConfig.\n",
      "- version (str or Version, optional)  The version of the dataset.\n",
      "- splits (dict, optional)  The mapping between split name and metadata.\n",
      "- download_checksums (dict, optional)  The mapping between the URL to download the datasets checksums and corresponding metadata.\n",
      "- download_size (int, optional)  The size of the files to download to generate the dataset, in bytes.\n",
      "- post_processing_size (int, optional)  Size of the dataset in bytes after post-processing, if any.\n",
      "- dataset_size (int, optional)  The combined size in bytes of the Arrow tables for all splits.\n",
      "- size_in_bytes (int, optional)  The combined size in bytes of all files associated with the dataset (downloaded files + Arrow files).\n",
      "- task_templates (List[TaskTemplate], optional)  The task templates to prepare the dataset for during training and evaluation. Each template casts the datasets Features to standardized column names and types as detailed in datasets.tasks.\n",
      "- **config_kwargs (additional keyword arguments)  Keyword arguments to be passed to the BuilderConfig and used in the DatasetBuilder.\n",
      "Information about a dataset.\n",
      "DatasetInfo documents datasets, including its name, version, and features. See the constructor arguments and properties for a full list.\n",
      "Not all fields are known on construction and may be updated later.\n",
      "from_directory\n",
      "( dataset_info_dir: strfs = 'deprecated'storage_options: Optional = None )\n",
      "Parameters\n",
      "- dataset_info_dir (str)  The directory containing the metadata file. This should be the root directory of a specific dataset version.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem used to download the files from.\n",
      "Deprecated in 2.9.0\n",
      "fs was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options.\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.9.0\n",
      "\n",
      "Deprecated in 2.9.0\n",
      "fs was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options.\n",
      "\n",
      "Added in 2.9.0\n",
      "Create DatasetInfo from the JSON file in dataset_info_dir.\n",
      "This function updates all the dynamically generated fields (num_examples, hash, time of creation,) of the DatasetInfo.\n",
      "This will overwrite all previous metadata.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import DatasetInfo\n",
      ">>> ds_info = DatasetInfo.from_directory(\"/path/to/directory/\")\n",
      "```\n",
      "write_to_directory\n",
      "( dataset_info_dirpretty_print = Falsefs = 'deprecated'storage_options: Optional = None )\n",
      "Parameters\n",
      "- dataset_info_dir (str)  Destination directory.\n",
      "- pretty_print (bool, defaults to False)  If True, the JSON will be pretty-printed with the indent level of 4.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem used to download the files from.\n",
      "Deprecated in 2.9.0\n",
      "fs was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options.\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.9.0\n",
      "\n",
      "Deprecated in 2.9.0\n",
      "fs was deprecated in version 2.9.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options.\n",
      "\n",
      "Added in 2.9.0\n",
      "Write DatasetInfo and license (if present) as JSON files to dataset_info_dir.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.info.write_to_directory(\"/path/to/directory/\")\n",
      "```\n",
      "Dataset\n",
      "The base class Dataset implements a Dataset backed by an Apache Arrow table.\n",
      "class datasets.Dataset\n",
      "( arrow_table: Tableinfo: Optional = Nonesplit: Optional = Noneindices_table: Optional = Nonefingerprint: Optional = None )\n",
      "A Dataset backed by an Arrow table.\n",
      "add_column\n",
      "( name: strcolumn: Unionnew_fingerprint: str )\n",
      "Parameters\n",
      "- name (str)  Column name.\n",
      "- column (list or np.array)  Column data to be added.\n",
      "Add column to Dataset.\n",
      "Added in 1.7\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> more_text = ds[\"text\"]\n",
      ">>> ds.add_column(name=\"text_2\", column=more_text)\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'text_2'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "```\n",
      "add_item\n",
      "( item: dictnew_fingerprint: str )\n",
      "Parameters\n",
      "- item (dict)  Item data to be added.\n",
      "Add item to Dataset.\n",
      "Added in 1.7\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      ">>> ds = ds.add_item(new_review)\n",
      ">>> ds[-1]\n",
      "{'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      "```\n",
      "from_file\n",
      "( filename: strinfo: Optional = Nonesplit: Optional = Noneindices_filename: Optional = Nonein_memory: bool = False )\n",
      "Parameters\n",
      "- filename (str)  File name of the dataset.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "- indices_filename (str, optional)  File names of the indices.\n",
      "- in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "Instantiate a Dataset backed by an Arrow table at filename.\n",
      "from_buffer\n",
      "( buffer: Bufferinfo: Optional = Nonesplit: Optional = Noneindices_buffer: Optional = None )\n",
      "Parameters\n",
      "- buffer (pyarrow.Buffer)  Arrow buffer.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "- indices_buffer (pyarrow.Buffer, optional)  Indices Arrow buffer.\n",
      "Instantiate a Dataset backed by an Arrow buffer.\n",
      "from_pandas\n",
      "( df: DataFramefeatures: Optional = Noneinfo: Optional = Nonesplit: Optional = Nonepreserve_index: Optional = None )\n",
      "Parameters\n",
      "- df (pandas.DataFrame)  Dataframe that contains the dataset.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "- preserve_index (bool, optional)  Whether to store the index as an additional column in the resulting Dataset. The default of None will store the index as a column, except for RangeIndex which is stored as metadata only. Use preserve_index=True to force it to be stored as a column.\n",
      "Convert pandas.DataFrame to a pyarrow.Table to create a Dataset.\n",
      "The column types in the resulting Arrow Table are inferred from the dtypes of the pandas.Series in the DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the case of object, we need to guess the datatype by looking at the Python objects in this Series.\n",
      "Be aware that Series of the object dtype dont carry enough information to always lead to a meaningful Arrow type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. This behavior can be avoided by constructing explicit features and passing it to this function.\n",
      "Example:\n",
      "```\n",
      ">>> ds = Dataset.from_pandas(df)\n",
      "```\n",
      "from_dict\n",
      "( mapping: dictfeatures: Optional = Noneinfo: Optional = Nonesplit: Optional = None )\n",
      "Parameters\n",
      "- mapping (Mapping)  Mapping of strings to Arrays or Python lists.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "Convert dict to a pyarrow.Table to create a Dataset.\n",
      "from_generator\n",
      "( generator: Callablefeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsegen_kwargs: Optional = Nonenum_proc: Optional = None**kwargs )\n",
      "Parameters\n",
      "- generator ( Callable): A generator function that yields examples.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- gen_kwargs(dict, optional)  Keyword arguments to be passed to the generator callable. You can define a sharded dataset by passing the list of shards in gen_kwargs and setting num_proc greater than 1.\n",
      "- num_proc (int, optional, defaults to None)  Number of processes when downloading and generating the dataset locally. This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default. If num_proc is greater than one, then all list values in gen_kwargs must be the same length. These values will be split between calls to the generator. The number of shards will be the minimum of the shortest list in gen_kwargs and num_proc.\n",
      "Added in 2.7.0\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to :GeneratorConfig.\n",
      "\n",
      "Added in 2.7.0\n",
      "Create a Dataset from a generator.\n",
      "Example:\n",
      "```\n",
      ">>> def gen():\n",
      "...     yield {\"text\": \"Good\", \"label\": 0}\n",
      "...     yield {\"text\": \"Bad\", \"label\": 1}\n",
      "...\n",
      ">>> ds = Dataset.from_generator(gen)\n",
      "```\n",
      "```\n",
      ">>> def gen(shards):\n",
      "...     for shard in shards:\n",
      "...         with open(shard) as f:\n",
      "...             for line in f:\n",
      "...                 yield {\"line\": line}\n",
      "...\n",
      ">>> shards = [f\"data{i}.txt\" for i in range(32)]\n",
      ">>> ds = Dataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n",
      "```\n",
      "data\n",
      "( )\n",
      "The Apache Arrow table backing the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.data\n",
      "MemoryMappedTable\n",
      "text: string\n",
      "label: int64\n",
      "----\n",
      "text: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",\"so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .\",\"a visual spectacle full of stunning images and effects .\",\"a gentle and engrossing character study .\",\"it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .\",\"an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .\",...,\"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .\",\"ah-nuld's action hero days might be over .\",\"it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .\",\"feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .\",\"when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .\",\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\n",
      "label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n",
      "```\n",
      "cache_files\n",
      "( )\n",
      "The cache files containing the Apache Arrow table backing the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.cache_files\n",
      "[{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n",
      "```\n",
      "num_columns\n",
      "( )\n",
      "Number of columns in the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.num_columns\n",
      "2\n",
      "```\n",
      "num_rows\n",
      "( )\n",
      "Number of rows in the dataset (same as Dataset.len()).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.num_rows\n",
      "1066\n",
      "```\n",
      "column_names\n",
      "( )\n",
      "Names of the columns in the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.column_names\n",
      "['text', 'label']\n",
      "```\n",
      "shape\n",
      "( )\n",
      "Shape of the dataset (number of columns, number of rows).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.shape\n",
      "(1066, 2)\n",
      "```\n",
      "unique\n",
      "( column: str )  list\n",
      "Parameters\n",
      "- column (str)  Column name (list all the column names with column_names).\n",
      "Returns\n",
      "list\n",
      "List of unique elements in the given column.\n",
      "List of unique elements in the given column.\n",
      "Return a list of the unique elements in a column.\n",
      "This is implemented in the low-level backend and as such, very fast.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.unique('label')\n",
      "[1, 0]\n",
      "```\n",
      "flatten\n",
      "( new_fingerprint: Optional = Nonemax_depth = 16 )  Dataset\n",
      "Parameters\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset with flattened columns.\n",
      "A copy of the dataset with flattened columns.\n",
      "Flatten the table. Each column with a struct type is flattened into one column per struct field. Other columns are left unchanged.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"squad\", split=\"train\")\n",
      ">>> ds.features\n",
      "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'id': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None)}\n",
      ">>> ds.flatten()\n",
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      "    num_rows: 87599\n",
      "})\n",
      "```\n",
      "cast\n",
      "( features: Featuresbatch_size: Optional = 1000keep_in_memory: bool = Falseload_from_cache_file: Optional = Nonecache_file_name: Optional = Nonewriter_batch_size: Optional = 1000num_proc: Optional = None )  Dataset\n",
      "Parameters\n",
      "- features (Features)  New features to cast the dataset to. The name of the fields in the features must match the current column names. The type of the data must also be convertible from one type to the other. For non-trivial conversion, e.g. str <-> ClassLabel you should use map() to update the Dataset.\n",
      "- batch_size (int, defaults to 1000)  Number of examples per batch provided to cast. If batch_size <= 0 or batch_size == None then provide the full dataset as a single batch to cast.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- load_from_cache_file (bool, defaults to True if caching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- cache_file_name (str, optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map().\n",
      "- num_proc (int, optional, defaults to None)  Number of processes for multiprocessing. By default it doesnt use multiprocessing.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset with casted features.\n",
      "A copy of the dataset with casted features.\n",
      "Cast the dataset to a new set of features.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset, ClassLabel, Value\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> new_features = ds.features.copy()\n",
      ">>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      ">>> new_features['text'] = Value('large_string')\n",
      ">>> ds = ds.cast(new_features)\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='large_string', id=None)}\n",
      "```\n",
      "cast_column\n",
      "( column: strfeature: Unionnew_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- column (str)  Column name.\n",
      "- feature (FeatureType)  Target feature.\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Cast column to feature for decoding.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      "```\n",
      "remove_columns\n",
      "( column_names: Unionnew_fingerprint: Optional = None )  Dataset\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to remove.\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset object without the columns to remove.\n",
      "A copy of the dataset object without the columns to remove.\n",
      "Remove one or several column(s) in the dataset and the features associated to them.\n",
      "You can also remove a column using map() with remove_columns but the present method is in-place (doesnt copy the data to a new dataset) and is thus faster.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.remove_columns('label')\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1066\n",
      "})\n",
      ">>> ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0\n",
      "Dataset({\n",
      "    features: [],\n",
      "    num_rows: 0\n",
      "})\n",
      "```\n",
      "rename_column\n",
      "( original_column_name: strnew_column_name: strnew_fingerprint: Optional = None )  Dataset\n",
      "Parameters\n",
      "- original_column_name (str)  Name of the column to rename.\n",
      "- new_column_name (str)  New name for the column.\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset with a renamed column.\n",
      "A copy of the dataset with a renamed column.\n",
      "Rename a column in the dataset, and move the features associated to the original column under the new column name.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.rename_column('label', 'label_new')\n",
      "Dataset({\n",
      "    features: ['text', 'label_new'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "```\n",
      "rename_columns\n",
      "( column_mapping: Dictnew_fingerprint: Optional = None )  Dataset\n",
      "Parameters\n",
      "- column_mapping (Dict[str, str])  A mapping of columns to rename to their new names\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset with renamed columns\n",
      "A copy of the dataset with renamed columns\n",
      "Rename several columns in the dataset, and move the features associated to the original columns under the new column names.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      "Dataset({\n",
      "    features: ['text_new', 'label_new'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "```\n",
      "select_columns\n",
      "( column_names: Unionnew_fingerprint: Optional = None )  Dataset\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to keep.\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Returns\n",
      "Dataset\n",
      "A copy of the dataset object which only consists of selected columns.\n",
      "A copy of the dataset object which only consists of selected columns.\n",
      "Select one or several column(s) in the dataset and the features associated to them.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.select_columns(['text'])\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "```\n",
      "class_encode_column\n",
      "( column: strinclude_nulls: bool = False )\n",
      "Parameters\n",
      "- column (str)  The name of the column to cast (list all the column names with column_names)\n",
      "- include_nulls (bool, defaults to False)  Whether to include null values in the class labels. If True, the null values will be encoded as the \"None\" class label.\n",
      "Added in 1.14.2\n",
      "\n",
      "Added in 1.14.2\n",
      "Casts the given column as ClassLabel and updates the table.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"boolq\", split=\"validation\")\n",
      ">>> ds.features\n",
      "{'answer': Value(dtype='bool', id=None),\n",
      " 'passage': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.class_encode_column('answer')\n",
      ">>> ds.features\n",
      "{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
      " 'passage': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n",
      "```\n",
      "__len__\n",
      "( )\n",
      "Number of rows in the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.__len__\n",
      "<bound method Dataset.__len__ of Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1066\n",
      "})>\n",
      "```\n",
      "__iter__\n",
      "( )\n",
      "Iterate through the examples.\n",
      "If a formatting is set with Dataset.set_format() rows will be returned with the selected format.\n",
      "iter\n",
      "( batch_size: intdrop_last_batch: bool = False )\n",
      "Parameters\n",
      "- batch_size (int)  size of each batch to yield.\n",
      "- drop_last_batch (bool, default False)  Whether a last batch smaller than the batch_size should be dropped\n",
      "Iterate through the batches of size batch_size.\n",
      "If a formatting is set with [~datasets.Dataset.set_format] rows will be returned with the selected format.\n",
      "formatted_as\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means `getitem returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects).\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "To be used in a with statement. Set __getitem__ return format (type and columns).\n",
      "set_format\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects).\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly. The format type (for example numpy) is used to format batches when using __getitem__. Its also possible to use custom transforms for formatting using set_transform().\n",
      "It is possible to call map() after calling set_format. Since map may add new columns, then the list of formatted columns\n",
      "gets updated. In this case, if you apply map on a dataset to add a new column, then this column will be formatted as:\n",
      "```\n",
      "new formatted columns = (all columns - previously unformatted columns)\n",
      "```\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      ">>> ds.set_format(type='numpy', columns=['text', 'label'])\n",
      ">>> ds.format\n",
      "{'type': 'numpy',\n",
      "'format_kwargs': {},\n",
      "'columns': ['text', 'label'],\n",
      "'output_all_columns': False}\n",
      "```\n",
      "set_transform\n",
      "( transform: Optionalcolumns: Optional = Noneoutput_all_columns: bool = False )\n",
      "Parameters\n",
      "- transform (Callable, optional)  User-defined formatting transform, replaces the format defined by set_format(). A formatting function is a callable that takes a batch (as a dict) as input and returns a batch. This function is applied right before returning the objects in __getitem__.\n",
      "- columns (List[str], optional)  Columns to format in the output. If specified, then the input batch of the transform only contains those columns.\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects). If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      "Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called. As set_format(), this can be reset using reset_format().\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
      ">>> def encode(batch):\n",
      "...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
      ">>> ds.set_transform(encode)\n",
      ">>> ds[0]\n",
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " 1, 1]),\n",
      " 'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n",
      "         20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n",
      "         5637,  1998, 11690,  2336,  1012,   102]),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0])}\n",
      "```\n",
      "reset_format\n",
      "( )\n",
      "Reset __getitem__ return format to python objects and all columns.\n",
      "Same as self.set_format()\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      ">>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      ">>> ds.format\n",
      "{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': 'numpy'}\n",
      ">>> ds.reset_format()\n",
      ">>> ds.format\n",
      "{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': None}\n",
      "```\n",
      "with_format\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects).\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly. The format type (for example numpy) is used to format batches when using __getitem__.\n",
      "Its also possible to use custom transforms for formatting using with_transform().\n",
      "Contrary to set_format(), with_format returns a new Dataset object.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      ">>> ds.format\n",
      "{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': None}\n",
      ">>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      ">>> ds.format\n",
      "{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': 'tensorflow'}\n",
      "```\n",
      "with_transform\n",
      "( transform: Optionalcolumns: Optional = Noneoutput_all_columns: bool = False )\n",
      "Parameters\n",
      "- transform (Callable, optional)  User-defined formatting transform, replaces the format defined by set_format(). A formatting function is a callable that takes a batch (as a dict) as input and returns a batch. This function is applied right before returning the objects in __getitem__.\n",
      "- columns (List[str], optional)  Columns to format in the output. If specified, then the input batch of the transform only contains those columns.\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects). If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      "Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n",
      "As set_format(), this can be reset using reset_format().\n",
      "Contrary to set_transform(), with_transform returns a new Dataset object.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> def encode(example):\n",
      "...     return tokenizer(example[\"text\"], padding=True, truncation=True, return_tensors='pt')\n",
      ">>> ds = ds.with_transform(encode)\n",
      ">>> ds[0]\n",
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " 1, 1, 1, 1, 1]),\n",
      " 'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n",
      "         1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n",
      "         1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0])}\n",
      "```\n",
      "__getitem__\n",
      "( key )\n",
      "Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\n",
      "cleanup_cache_files\n",
      "( )  int\n",
      "Returns\n",
      "int\n",
      "Number of removed files.\n",
      "Number of removed files.\n",
      "Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.\n",
      "Be careful when running this command that no other process is currently using other cache files.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.cleanup_cache_files()\n",
      "10\n",
      "```\n",
      "map\n",
      "( function: Optional = Nonewith_indices: bool = Falsewith_rank: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000drop_last_batch: bool = Falseremove_columns: Union = Nonekeep_in_memory: bool = Falseload_from_cache_file: Optional = Nonecache_file_name: Optional = Nonewriter_batch_size: Optional = 1000features: Optional = Nonedisable_nullable: bool = Falsefn_kwargs: Optional = Nonenum_proc: Optional = Nonesuffix_template: str = '_{rank:05d}_of_{num_proc:05d}'new_fingerprint: Optional = Nonedesc: Optional = None )\n",
      "Parameters\n",
      "- function (Callable)  Function with one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False and with_rank=False\n",
      "function(example: Dict[str, Any], *extra_args) -> Dict[str, Any] if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False and with_rank=False\n",
      "function(batch: Dict[str, List], *extra_args) -> Dict[str, List] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> Dict[str, Any] if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> Dict[str, List] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx[, rank]): ....\n",
      "- with_rank (bool, defaults to False)  Provide process rank to function. Note that in this case the signature of function should be def function(example[, idx], rank): ....\n",
      "- input_columns (Optional[Union[str, List[str]]], defaults to None)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True. If batch_size <= 0 or batch_size == None, provide the full dataset as a single batch to function.\n",
      "- drop_last_batch (bool, defaults to False)  Whether a last batch smaller than the batch_size should be dropped instead of being processed by the function.\n",
      "- remove_columns (Optional[Union[str, List[str]]], defaults to None)  Remove a selection of columns while doing the mapping. Columns will be removed before updating the examples with the output of function, i.e. if function is adding columns with names in remove_columns, these columns will be kept.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- cache_file_name (str, optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- features (Optional[datasets.Features], defaults to None)  Use a specific Features to store the cache file instead of the automatically generated one.\n",
      "- disable_nullable (bool, defaults to False)  Disallow null values in the table.\n",
      "- fn_kwargs (Dict, optional, defaults to None)  Keyword arguments to be passed to function.\n",
      "- num_proc (int, optional, defaults to None)  Max number of processes when generating cache. Already cached shards are loaded sequentially.\n",
      "- suffix_template (str)  If cache_file_name is specified, then this suffix will be added at the end of the base name of each. Defaults to \"_{rank:05d}_of_{num_proc:05d}\". For example, if cache_file_name is processed.arrow, then for rank=1 and num_proc=4, the resulting file would be \"processed_00001_of_00004.arrow\" for the default suffix.\n",
      "- new_fingerprint (str, optional, defaults to None)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "- desc (str, optional, defaults to None)  Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> Dict[str, Any] if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> Dict[str, List] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "Apply a function to all the examples in the table (individually or in batches) and update the table. If your function returns a column that already exists, then it overwrites it.\n",
      "You can specify whether the function should be batched or not with the batched parameter:\n",
      "- If batched is False, then the function takes 1 example in and should return 1 example. An example is a dictionary, e.g. {\"text\": \"Hello there !\"}.\n",
      "- If batched is True and batch_size is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples. A batch is a dictionary, e.g. a batch of 1 example is {\"text\": [\"Hello there !\"]}.\n",
      "- If batched is True and batch_size is n > 1, then the function takes a batch of n examples as input and can return a batch with n examples, or with an arbitrary number of examples. Note that the last batch may have less than n examples. A batch is a dictionary, e.g. a batch of n examples is {\"text\": [\"Hello there !\"] * n}.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "...     return example\n",
      ">>> ds = ds.map(add_prefix)\n",
      ">>> ds[0:3][\"text\"]\n",
      "['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
      " 'Review: the soundtrack alone is worth the price of admission .',\n",
      " 'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n",
      "\n",
      "# process a batch of examples\n",
      ">>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      "# set number of processors\n",
      ">>> ds = ds.map(add_prefix, num_proc=4)\n",
      "```\n",
      "filter\n",
      "( function: Optional = Nonewith_indices: bool = Falsewith_rank: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000keep_in_memory: bool = Falseload_from_cache_file: Optional = Nonecache_file_name: Optional = Nonewriter_batch_size: Optional = 1000fn_kwargs: Optional = Nonenum_proc: Optional = Nonesuffix_template: str = '_{rank:05d}_of_{num_proc:05d}'new_fingerprint: Optional = Nonedesc: Optional = None )\n",
      "Parameters\n",
      "- function (Callable)  Callable with one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "- function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx[, rank]): ....\n",
      "- with_rank (bool, defaults to False)  Provide process rank to function. Note that in this case the signature of function should be def function(example[, idx], rank): ....\n",
      "- input_columns (str or List[str], optional)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched = True. If batched = False, one example per batch is passed to function. If batch_size <= 0 or batch_size == None, provide the full dataset as a single batch to function.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- cache_file_name (str, optional)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- fn_kwargs (dict, optional)  Keyword arguments to be passed to function.\n",
      "- num_proc (int, optional)  Number of processes for multiprocessing. By default it doesnt use multiprocessing.\n",
      "- suffix_template (str)  If cache_file_name is specified, then this suffix will be added at the end of the base name of each. For example, if cache_file_name is \"processed.arrow\", then for rank = 1 and num_proc = 4, the resulting file would be \"processed_00001_of_00004.arrow\" for the default suffix (default _{rank:05d}_of_{num_proc:05d}).\n",
      "- new_fingerprint (str, optional)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "- desc (str, optional, defaults to None)  Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "Apply a filter function to all the elements in the table in batches and update the table so that the dataset only includes examples according to the filter function.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 533\n",
      "})\n",
      "```\n",
      "select\n",
      "( indices: Iterablekeep_in_memory: bool = Falseindices_cache_file_name: Optional = Nonewriter_batch_size: Optional = 1000new_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- indices (range, list, iterable, ndarray or Series)  Range, list or 1D-array of integer indices for indexing. If the indices correspond to a contiguous range, the Arrow table is simply sliced. However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient, but still faster than recreating an Arrow table made of the requested rows.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the indices mapping in memory instead of writing it to a cache file.\n",
      "- indices_cache_file_name (str, optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the indices mapping instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- new_fingerprint (str, optional, defaults to None)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Create a new dataset with rows selected following the list/array of indices.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds.select(range(4))\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4\n",
      "})\n",
      "```\n",
      "sort\n",
      "( column_names: Unionreverse: Union = Falsekind = 'deprecated'null_placement: str = 'at_end'keep_in_memory: bool = Falseload_from_cache_file: Optional = Noneindices_cache_file_name: Optional = Nonewriter_batch_size: Optional = 1000new_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- column_names (Union[str, Sequence[str]])  Column name(s) to sort by.\n",
      "- reverse (Union[bool, Sequence[bool]], defaults to False)  If True, sort by descending order rather than ascending. If a single bool is provided, the value is applied to the sorting of all column names. Otherwise a list of bools with the same length and order as column_names must be provided.\n",
      "- kind (str, optional)  Pandas algorithm for sorting selected in {quicksort, mergesort, heapsort, stable}, The default is quicksort. Note that both stable and mergesort use timsort under the covers and, in general, the actual implementation will vary with data type. The mergesort option is retained for backwards compatibility.\n",
      "Deprecated in 2.8.0\n",
      "kind was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
      "- null_placement (str, defaults to at_end)  Put None values at the beginning if at_start or first or at the end if at_end or last\n",
      "Added in 1.14.2\n",
      "- keep_in_memory (bool, defaults to False)  Keep the sorted indices in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the sorted indices can be identified, use it instead of recomputing.\n",
      "- indices_cache_file_name (str, optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the sorted indices instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      "- new_fingerprint (str, optional, defaults to None)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "kind was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
      "\n",
      "Added in 1.14.2\n",
      "Create a new dataset sorted according to a single or multiple columns.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('rotten_tomatoes', split='validation')\n",
      ">>> ds['label'][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      ">>> sorted_ds = ds.sort('label')\n",
      ">>> sorted_ds['label'][:10]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      ">>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n",
      ">>> another_sorted_ds['label'][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "```\n",
      "shuffle\n",
      "( seed: Optional = Nonegenerator: Optional = Nonekeep_in_memory: bool = Falseload_from_cache_file: Optional = Noneindices_cache_file_name: Optional = Nonewriter_batch_size: Optional = 1000new_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- seed (int, optional)  A seed to initialize the default BitGenerator if generator=None. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      "- generator (numpy.random.Generator, optional)  Numpy random Generator to use to compute the permutation of the dataset rows. If generator=None (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      "- keep_in_memory (bool, default False)  Keep the shuffled indices in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the shuffled indices can be identified, use it instead of recomputing.\n",
      "- indices_cache_file_name (str, optional)  Provide the name of a path for the cache file. It is used to store the shuffled indices instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- new_fingerprint (str, optional, defaults to None)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      "Create a new Dataset where the rows are shuffled.\n",
      "Currently shuffling uses numpy random generators. You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPys default random generator (PCG64).\n",
      "Shuffling takes the list of indices [0:len(my_dataset)] and shuffles it to create an indices mapping. However as soon as your Dataset has an indices mapping, the speed can become 10x slower. This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you arent reading contiguous chunks of data anymore. To restore the speed, youd need to rewrite the entire dataset on your disk again using Dataset.flatten_indices(), which removes the indices mapping.\n",
      "This may take a lot of time depending of the size of your dataset though:\n",
      "```\n",
      "my_dataset[0]  # fast\n",
      "my_dataset = my_dataset.shuffle(seed=42)\n",
      "my_dataset[0]  # up to 10x slower\n",
      "my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
      "my_dataset[0]  # fast again\n",
      "```\n",
      "In this case, we recommend switching to an IterableDataset and leveraging its fast approximate shuffling method IterableDataset.shuffle().\n",
      "It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal:\n",
      "```\n",
      "my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)\n",
      "for example in enumerate(my_iterable_dataset):  # fast\n",
      "    pass\n",
      "\n",
      "shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n",
      "\n",
      "for example in enumerate(shuffled_iterable_dataset):  # as fast as before\n",
      "    pass\n",
      "```\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds['label'][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "# set a seed\n",
      ">>> shuffled_ds = ds.shuffle(seed=42)\n",
      ">>> shuffled_ds['label'][:10]\n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "```\n",
      "train_test_split\n",
      "( test_size: Union = Nonetrain_size: Union = Noneshuffle: bool = Truestratify_by_column: Optional = Noneseed: Optional = Nonegenerator: Optional = Nonekeep_in_memory: bool = Falseload_from_cache_file: Optional = Nonetrain_indices_cache_file_name: Optional = Nonetest_indices_cache_file_name: Optional = Nonewriter_batch_size: Optional = 1000train_new_fingerprint: Optional = Nonetest_new_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- test_size (numpy.random.Generator, optional)  Size of the test split If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n",
      "- train_size (numpy.random.Generator, optional)  Size of the train split If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n",
      "- shuffle (bool, optional, defaults to True)  Whether or not to shuffle the data before splitting.\n",
      "- stratify_by_column (str, optional, defaults to None)  The column name of labels to be used to perform stratified split of data.\n",
      "- seed (int, optional)  A seed to initialize the default BitGenerator if generator=None. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      "- generator (numpy.random.Generator, optional)  Numpy random Generator to use to compute the permutation of the dataset rows. If generator=None (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      "- keep_in_memory (bool, defaults to False)  Keep the splits indices in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the splits indices can be identified, use it instead of recomputing.\n",
      "- train_cache_file_name (str, optional)  Provide the name of a path for the cache file. It is used to store the train split indices instead of the automatically generated cache file name.\n",
      "- test_cache_file_name (str, optional)  Provide the name of a path for the cache file. It is used to store the test split indices instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- train_new_fingerprint (str, optional, defaults to None)  The new fingerprint of the train set after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      "- test_new_fingerprint (str, optional, defaults to None)  The new fingerprint of the test set after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      "Return a dictionary (datasets.DatasetDict) with two random train and test subsets (train and test Dataset splits). Splits are created from the dataset according to test_size, train_size and shuffle.\n",
      "This method is similar to scikit-learn train_test_split.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 852\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 214\n",
      "    })\n",
      "})\n",
      "\n",
      "# set a seed\n",
      ">>> ds = ds.train_test_split(test_size=0.2, seed=42)\n",
      "\n",
      "# stratified split\n",
      ">>> ds = load_dataset(\"imdb\",split=\"train\")\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      ">>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "```\n",
      "shard\n",
      "( num_shards: intindex: intcontiguous: bool = Falsekeep_in_memory: bool = Falseindices_cache_file_name: Optional = Nonewriter_batch_size: Optional = 1000 )\n",
      "Parameters\n",
      "- num_shards (int)  How many shards to split the dataset into.\n",
      "- index (int)  Which shard to select and return. contiguous  (bool, defaults to False): Whether to select contiguous blocks of indices for shards.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- indices_cache_file_name (str, optional)  Provide the name of a path for the cache file. It is used to store the indices of each shard instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "Return the index-nth shard from dataset split into num_shards pieces.\n",
      "This shards deterministically. dset.shard(n, i) will contain all elements of dset whose index mod n = i.\n",
      "dset.shard(n, i, contiguous=True) will instead split dset into contiguous chunks, so it can be easily concatenated back together after processing. If n % i == l, then the first l shards will have length (n // i) + 1, and the remaining shards will have length (n // i). datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)]) will return a dataset with the same order as the original.\n",
      "Be sure to shard before using any randomizing operator (such as shuffle). It is best if the shard operator is used early in the dataset pipeline.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      ">>> ds\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1066\n",
      "})\n",
      ">>> ds.shard(num_shards=2, index=0)\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 533\n",
      "})\n",
      "```\n",
      "to_tf_dataset\n",
      "( batch_size: Optional = Nonecolumns: Union = Noneshuffle: bool = Falsecollate_fn: Optional = Nonedrop_remainder: bool = Falsecollate_fn_args: Optional = Nonelabel_cols: Union = Noneprefetch: bool = Truenum_workers: int = 0num_test_batches: int = 20 )\n",
      "Parameters\n",
      "- batch_size (int, optional)  Size of batches to load from the dataset. Defaults to None, which implies that the dataset wont be batched, but the returned dataset can be batched later with tf_dataset.batch(batch_size).\n",
      "- columns (List[str] or str, optional)  Dataset column(s) to load in the tf.data.Dataset. Column names that are created by the collate_fn and that do not exist in the original dataset can be used.\n",
      "- shuffle(bool, defaults to False)  Shuffle the dataset order when loading. Recommended True for training, False for validation/evaluation.\n",
      "- drop_remainder(bool, defaults to False)  Drop the last incomplete batch when loading. Ensures that all batches yielded by the dataset will have the same length on the batch dimension.\n",
      "- collate_fn(Callable, optional)  A function or callable object (such as a DataCollator) that will collate lists of samples into a batch.\n",
      "- collate_fn_args (Dict, optional)  An optional dict of keyword arguments to be passed to the collate_fn.\n",
      "- label_cols (List[str] or str, defaults to None)  Dataset column(s) to load as labels. Note that many models compute loss internally rather than letting Keras do it, in which case passing the labels here is optional, as long as theyre in the input columns.\n",
      "- prefetch (bool, defaults to True)  Whether to run the dataloader in a separate thread and maintain a small buffer of batches for training. Improves performance by allowing data to be loaded in the background while the model is training.\n",
      "- num_workers (int, defaults to 0)  Number of workers to use for loading the dataset. Only supported on Python versions >= 3.8.\n",
      "- num_test_batches (int, defaults to 20)  Number of batches to use to infer the output signature of the dataset. The higher this number, the more accurate the signature will be, but the longer it will take to create the dataset.\n",
      "Create a tf.data.Dataset from the underlying Dataset. This tf.data.Dataset will load and collate batches from the Dataset, and is suitable for passing to methods like model.fit() or model.predict(). The dataset will yield dicts for both inputs and labels unless the dict would contain only a single key, in which case a raw tf.Tensor is yielded instead.\n",
      "Example:\n",
      "```\n",
      ">>> ds_train = ds[\"train\"].to_tf_dataset(\n",
      "...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      "...    shuffle=True,\n",
      "...    batch_size=16,\n",
      "...    collate_fn=data_collator,\n",
      "... )\n",
      "```\n",
      "push_to_hub\n",
      "( repo_id: strconfig_name: str = 'default'set_default: Optional = Nonesplit: Optional = Nonedata_dir: Optional = Nonecommit_message: Optional = Nonecommit_description: Optional = Noneprivate: Optional = Falsetoken: Optional = Nonerevision: Optional = Nonebranch = 'deprecated'create_pr: Optional = Falsemax_shard_size: Union = Nonenum_shards: Optional = Noneembed_external_files: bool = True )\n",
      "Parameters\n",
      "- repo_id (str)  The ID of the repository to push to in the following format: <user>/<dataset_name> or <org>/<dataset_name>. Also accepts <dataset_name>, which will default to the namespace of the logged-in user.\n",
      "- config_name (str, defaults to default)  The configuration name (or subset) of a dataset. Defaults to default.\n",
      "- set_default (bool, optional)  Whether to set this configuration as the default one. Otherwise, the default configuration is the one named default.\n",
      "- split (str, optional)  The name of the split that will be given to that dataset. Defaults to self.split.\n",
      "- data_dir (str, optional)  Directory name that will contain the uploaded data files. Defaults to the config_name if different from default, else data.\n",
      "Added in 2.17.0\n",
      "- commit_message (str, optional)  Message to commit while pushing. Will default to \"Upload dataset\".\n",
      "- commit_description (str, optional)  Description of the commit that will be created. Additionally, description of the PR if a PR is created (create_pr is True).\n",
      "Added in 2.16.0\n",
      "- private (bool, optional, defaults to False)  Whether the dataset repository should be set to private or not. Only affects repository creation: a repository that already exists will not be affected by that parameter.\n",
      "- token (str, optional)  An optional authentication token for the Hugging Face Hub. If no token is passed, will default to the token saved locally when logging in with huggingface-cli login. Will raise an error if no token is passed and the user is not logged-in.\n",
      "- revision (str, optional)  Branch to push the uploaded files to. Defaults to the \"main\" branch.\n",
      "Added in 2.15.0\n",
      "- branch (str, optional)  The git branch on which to push the dataset. This defaults to the default branch as specified in your repository, which defaults to \"main\".\n",
      "Deprecated in 2.15.0\n",
      "branch was deprecated in favor of revision in version 2.15.0 and will be removed in 3.0.0.\n",
      "- create_pr (bool, optional, defaults to False)  Whether to create a PR with the uploaded files or directly commit.\n",
      "Added in 2.15.0\n",
      "- max_shard_size (int or str, optional, defaults to \"500MB\")  The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit (like \"5MB\").\n",
      "- num_shards (int, optional)  Number of shards to write. By default, the number of shards depends on max_shard_size.\n",
      "Added in 2.8.0\n",
      "- embed_external_files (bool, defaults to True)  Whether to embed file bytes in the shards. In particular, this will do the following before the push for the fields of type:\n",
      "Audio and Image: remove local path information and embed file content in the Parquet files.\n",
      "- Audio and Image: remove local path information and embed file content in the Parquet files.\n",
      "\n",
      "Added in 2.17.0\n",
      "\n",
      "Added in 2.16.0\n",
      "\n",
      "Added in 2.15.0\n",
      "\n",
      "Deprecated in 2.15.0\n",
      "branch was deprecated in favor of revision in version 2.15.0 and will be removed in 3.0.0.\n",
      "\n",
      "Added in 2.15.0\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "- Audio and Image: remove local path information and embed file content in the Parquet files.\n",
      "Pushes the dataset to the hub as a Parquet dataset. The dataset is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      "The resulting Parquet files are self-contained by default. If your dataset contains Image or Audio data, the Parquet files will store the bytes of your images or audio files. You can disable this by setting embed_external_files to False.\n",
      "Example:\n",
      "```\n",
      ">>> dataset.push_to_hub(\"<organization>/<dataset_id>\")\n",
      ">>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
      ">>> dataset.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
      ">>> dataset.push_to_hub(\"<organization>/<dataset_id>\", num_shards=1024)\n",
      "```\n",
      "If your dataset has multiple splits (e.g. train/validation/test):\n",
      "```\n",
      ">>> train_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"train\")\n",
      ">>> val_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"validation\")\n",
      ">>> # later\n",
      ">>> dataset = load_dataset(\"<organization>/<dataset_id>\")\n",
      ">>> train_dataset = dataset[\"train\"]\n",
      ">>> val_dataset = dataset[\"validation\"]\n",
      "```\n",
      "If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
      "```\n",
      ">>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
      ">>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
      ">>> # later\n",
      ">>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
      ">>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
      "```\n",
      "save_to_disk\n",
      "( dataset_path: Unionfs = 'deprecated'max_shard_size: Union = Nonenum_shards: Optional = Nonenum_proc: Optional = Nonestorage_options: Optional = None )\n",
      "Parameters\n",
      "- dataset_path (str)  Path (e.g. dataset/train) or remote URI (e.g. s3://my-bucket/dataset/train) of the dataset directory where the dataset will be saved to.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem where the dataset will be saved to.\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "- max_shard_size (int or str, optional, defaults to \"500MB\")  The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit (like \"50MB\").\n",
      "- num_shards (int, optional)  Number of shards to write. By default the number of shards depends on max_shard_size and num_proc.\n",
      "Added in 2.8.0\n",
      "- num_proc (int, optional)  Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.8.0\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "Added in 2.8.0\n",
      "Saves a dataset to a dataset directory, or in a filesystem using any implementation of fsspec.spec.AbstractFileSystem.\n",
      "For Image and Audio data:\n",
      "All the Image() and Audio() data are stored in the arrow files. If you want to store paths or urls, please use the Value(string) type.\n",
      "Example:\n",
      "```\n",
      ">>> ds.save_to_disk(\"path/to/dataset/directory\")\n",
      ">>> ds.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
      ">>> ds.save_to_disk(\"path/to/dataset/directory\", num_shards=1024)\n",
      "```\n",
      "load_from_disk\n",
      "( dataset_path: strfs = 'deprecated'keep_in_memory: Optional = Nonestorage_options: Optional = None )  Dataset or DatasetDict\n",
      "Parameters\n",
      "- dataset_path (str)  Path (e.g. \"dataset/train\") or remote URI (e.g. \"s3//my-bucket/dataset/train\") of the dataset directory where the dataset will be loaded from.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem where the dataset will be saved to.\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "- keep_in_memory (bool, defaults to None)  Whether to copy the dataset in-memory. If None, the dataset will not be copied in-memory unless explicitly enabled by setting datasets.config.IN_MEMORY_MAX_SIZE to nonzero. See more details in the improve performance section.\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.8.0\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "\n",
      "Added in 2.8.0\n",
      "Returns\n",
      "Dataset or DatasetDict\n",
      "If dataset_path is a path of a dataset directory, the dataset requested.\n",
      "If dataset_path is a path of a dataset dict directory, a datasets.DatasetDict with each split.\n",
      "- If dataset_path is a path of a dataset directory, the dataset requested.\n",
      "- If dataset_path is a path of a dataset dict directory, a datasets.DatasetDict with each split.\n",
      "Loads a dataset that was previously saved using save_to_disk from a dataset directory, or from a filesystem using any implementation of fsspec.spec.AbstractFileSystem.\n",
      "Example:\n",
      "```\n",
      ">>> ds = load_from_disk(\"path/to/dataset/directory\")\n",
      "```\n",
      "flatten_indices\n",
      "( keep_in_memory: bool = Falsecache_file_name: Optional = Nonewriter_batch_size: Optional = 1000features: Optional = Nonedisable_nullable: bool = Falsenum_proc: Optional = Nonenew_fingerprint: Optional = None )\n",
      "Parameters\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- cache_file_name (str, optional, default None)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- features (Optional[datasets.Features], defaults to None)  Use a specific Features to store the cache file instead of the automatically generated one.\n",
      "- disable_nullable (bool, defaults to False)  Allow null values in the table.\n",
      "- num_proc (int, optional, default None)  Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
      "- new_fingerprint (str, optional, defaults to None)  The new fingerprint of the dataset after transform. If None, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      "Create and cache a new Dataset by flattening the indices mapping.\n",
      "to_csv\n",
      "( path_or_buf: Unionbatch_size: Optional = Nonenum_proc: Optional = None**to_csv_kwargs )  int\n",
      "Parameters\n",
      "- path_or_buf (PathLike or FileOrBuffer)  Either a path to a file or a BinaryIO.\n",
      "- batch_size (int, optional)  Size of the batch to load in memory and write at once. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "- num_proc (int, optional)  Number of processes for multiprocessing. By default it doesnt use multiprocessing. batch_size in this case defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE but feel free to make it 5x or 10x of the default value if you have sufficient compute power.\n",
      "- **to_csv_kwargs (additional keyword arguments)  Parameters to pass to pandass pandas.DataFrame.to_csv.\n",
      "Changed in 2.10.0\n",
      "Now, index defaults to False if not specified.\n",
      "If you would like to write the index, pass index=True and also set a name for the index column by passing index_label.\n",
      "\n",
      "Changed in 2.10.0\n",
      "Now, index defaults to False if not specified.\n",
      "If you would like to write the index, pass index=True and also set a name for the index column by passing index_label.\n",
      "Returns\n",
      "int\n",
      "The number of characters or bytes written.\n",
      "The number of characters or bytes written.\n",
      "Exports the dataset to csv\n",
      "Example:\n",
      "```\n",
      ">>> ds.to_csv(\"path/to/dataset/directory\")\n",
      "```\n",
      "to_pandas\n",
      "( batch_size: Optional = Nonebatched: bool = False )\n",
      "Parameters\n",
      "- batched (bool)  Set to True to return a generator that yields the dataset as batches of batch_size rows. Defaults to False (returns the whole datasets once).\n",
      "- batch_size (int, optional)  The size (number of rows) of the batches if batched is True. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "Returns the dataset as a pandas.DataFrame. Can also return a generator for large datasets.\n",
      "Example:\n",
      "```\n",
      ">>> ds.to_pandas()\n",
      "```\n",
      "to_dict\n",
      "( batch_size: Optional = Nonebatched = 'deprecated' )\n",
      "Parameters\n",
      "- batched (bool)  Set to True to return a generator that yields the dataset as batches of batch_size rows. Defaults to False (returns the whole datasets once).\n",
      "Deprecated in 2.11.0\n",
      "Use .iter(batch_size=batch_size) followed by .to_dict() on the individual batches instead.\n",
      "- batch_size (int, optional)  The size (number of rows) of the batches if batched is True. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "\n",
      "Deprecated in 2.11.0\n",
      "Use .iter(batch_size=batch_size) followed by .to_dict() on the individual batches instead.\n",
      "Returns the dataset as a Python dict. Can also return a generator for large datasets.\n",
      "Example:\n",
      "```\n",
      ">>> ds.to_dict()\n",
      "```\n",
      "to_json\n",
      "( path_or_buf: Unionbatch_size: Optional = Nonenum_proc: Optional = None**to_json_kwargs )  int\n",
      "Parameters\n",
      "- path_or_buf (PathLike or FileOrBuffer)  Either a path to a file or a BinaryIO.\n",
      "- batch_size (int, optional)  Size of the batch to load in memory and write at once. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "- num_proc (int, optional)  Number of processes for multiprocessing. By default it doesnt use multiprocessing. batch_size in this case defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE but feel free to make it 5x or 10x of the default value if you have sufficient compute power.\n",
      "- **to_json_kwargs (additional keyword arguments)  Parameters to pass to pandass pandas.DataFrame.to_json.\n",
      "Changed in 2.11.0\n",
      "Now, index defaults to False if orient is \"split\" or \"table\".\n",
      "If you would like to write the index, pass index=True.\n",
      "\n",
      "Changed in 2.11.0\n",
      "Now, index defaults to False if orient is \"split\" or \"table\".\n",
      "If you would like to write the index, pass index=True.\n",
      "Returns\n",
      "int\n",
      "The number of characters or bytes written.\n",
      "The number of characters or bytes written.\n",
      "Export the dataset to JSON Lines or JSON.\n",
      "Example:\n",
      "```\n",
      ">>> ds.to_json(\"path/to/dataset/directory\")\n",
      "```\n",
      "to_parquet\n",
      "( path_or_buf: Unionbatch_size: Optional = None**parquet_writer_kwargs )  int\n",
      "Parameters\n",
      "- path_or_buf (PathLike or FileOrBuffer)  Either a path to a file or a BinaryIO.\n",
      "- batch_size (int, optional)  Size of the batch to load in memory and write at once. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "- **parquet_writer_kwargs (additional keyword arguments)  Parameters to pass to PyArrows pyarrow.parquet.ParquetWriter.\n",
      "Returns\n",
      "int\n",
      "The number of characters or bytes written.\n",
      "The number of characters or bytes written.\n",
      "Exports the dataset to parquet\n",
      "Example:\n",
      "```\n",
      ">>> ds.to_parquet(\"path/to/dataset/directory\")\n",
      "```\n",
      "to_sql\n",
      "( name: strcon: Unionbatch_size: Optional = None**sql_writer_kwargs )  int\n",
      "Parameters\n",
      "- name (str)  Name of SQL table.\n",
      "- con (str or sqlite3.Connection or sqlalchemy.engine.Connection or sqlalchemy.engine.Connection)  A URI string or a SQLite3/SQLAlchemy connection object used to write to a database.\n",
      "- batch_size (int, optional)  Size of the batch to load in memory and write at once. Defaults to datasets.config.DEFAULT_MAX_BATCH_SIZE.\n",
      "- **sql_writer_kwargs (additional keyword arguments)  Parameters to pass to pandass pandas.DataFrame.to_sql.\n",
      "Changed in 2.11.0\n",
      "Now, index defaults to False if not specified.\n",
      "If you would like to write the index, pass index=True and also set a name for the index column by passing index_label.\n",
      "\n",
      "Changed in 2.11.0\n",
      "Now, index defaults to False if not specified.\n",
      "If you would like to write the index, pass index=True and also set a name for the index column by passing index_label.\n",
      "Returns\n",
      "int\n",
      "The number of records written.\n",
      "The number of records written.\n",
      "Exports the dataset to a SQL database.\n",
      "Example:\n",
      "```\n",
      ">>> # con provided as a connection URI string\n",
      ">>> ds.to_sql(\"data\", \"sqlite:///my_own_db.sql\")\n",
      ">>> # con provided as a sqlite3 connection object\n",
      ">>> import sqlite3\n",
      ">>> con = sqlite3.connect(\"my_own_db.sql\")\n",
      ">>> with con:\n",
      "...     ds.to_sql(\"data\", con)\n",
      "```\n",
      "to_iterable_dataset\n",
      "( num_shards: Optional = 1 )\n",
      "Parameters\n",
      "- num_shards (int, default to 1)  Number of shards to define when instantiating the iterable dataset. This is especially useful for big datasets to be able to shuffle properly, and also to enable fast parallel loading using a PyTorch DataLoader or in distributed setups for example. Shards are defined using datasets.Dataset.shard(): it simply slices the data without writing anything on disk.\n",
      "Get an datasets.IterableDataset from a map-style datasets.Dataset. This is equivalent to loading a dataset in streaming mode with datasets.load_dataset(), but much faster since the data is streamed from local files.\n",
      "Contrary to map-style datasets, iterable datasets are lazy and can only be iterated over (e.g. using a for loop). Since they are read sequentially in training loops, iterable datasets are much faster than map-style datasets. All the transformations applied to iterable datasets like filtering or processing are done on-the-fly when you start iterating over the dataset.\n",
      "Still, it is possible to shuffle an iterable dataset using datasets.IterableDataset.shuffle(). This is a fast approximate shuffling that works best if you have multiple shards and if you specify a buffer size that is big enough.\n",
      "To get the best speed performance, make sure your dataset doesnt have an indices mapping. If this is the case, the data are not read contiguously, which can be slow sometimes. You can use ds = ds.flatten_indices() to write your dataset in contiguous chunks of data and have optimal speed before switching to an iterable dataset.\n",
      "Example:\n",
      "Basic usage:\n",
      "```\n",
      ">>> ids = ds.to_iterable_dataset()\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "With lazy filtering and processing:\n",
      "```\n",
      ">>> ids = ds.to_iterable_dataset()\n",
      ">>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "With sharding to enable efficient shuffling:\n",
      "```\n",
      ">>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over\n",
      ">>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "With a PyTorch DataLoader:\n",
      "```\n",
      ">>> import torch\n",
      ">>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      ">>> ids = ids.filter(filter_fn).map(process_fn)\n",
      ">>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "With a PyTorch DataLoader and shuffling:\n",
      "```\n",
      ">>> import torch\n",
      ">>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      ">>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      ">>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling\n",
      "```\n",
      ">>> from datasets.distributed import split_dataset_by_node\n",
      ">>> ids = ds.to_iterable_dataset(num_shards=512)\n",
      ">>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      ">>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\n",
      ">>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\n",
      ">>> for example in ids:\n",
      "...     pass\n",
      "```\n",
      "With shuffling and multiple epochs:\n",
      "```\n",
      ">>> ids = ds.to_iterable_dataset(num_shards=64)\n",
      ">>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
      ">>> for epoch in range(n_epochs):\n",
      "...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating\n",
      "...     for example in ids:\n",
      "...         pass\n",
      "```\n",
      "add_faiss_index\n",
      "( column: strindex_name: Optional = Nonedevice: Optional = Nonestring_factory: Optional = Nonemetric_type: Optional = Nonecustom_index: Optional = Nonebatch_size: int = 1000train_size: Optional = Nonefaiss_verbose: bool = Falsedtype = <class 'numpy.float32'> )\n",
      "Parameters\n",
      "- column (str)  The column of the vectors to add to the index.\n",
      "- index_name (str, optional)  The index_name/identifier of the index. This is the index_name that is used to call get_nearest_examples() or search(). By default it corresponds to column.\n",
      "- device (Union[int, List[int]], optional)  If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs. If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      "- string_factory (str, optional)  This is passed to the index factory of Faiss to create the index. Default index class is IndexFlat.\n",
      "- metric_type (int, optional)  Type of metric. Ex: faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      "- custom_index (faiss.Index, optional)  Custom Faiss index that you already have instantiated and configured for your needs.\n",
      "- batch_size (int)  Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
      "Added in 2.4.0\n",
      "- train_size (int, optional)  If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      "- faiss_verbose (bool, defaults to False)  Enable the verbosity of the Faiss index.\n",
      "- dtype (data-type)  The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
      "Added in 2.4.0\n",
      "Add a dense index using Faiss for fast retrieval. By default the index is done over the vectors of the specified column. You can specify device if you want to run it on GPU (device must be the GPU index). You can find more information about Faiss here:\n",
      "- For string factory\n",
      "Example:\n",
      "```\n",
      ">>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      ">>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n",
      ">>> ds_with_embeddings.add_faiss_index(column='embeddings')\n",
      ">>> # query\n",
      ">>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      ">>> # save index\n",
      ">>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
      "\n",
      ">>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      ">>> # load index\n",
      ">>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
      ">>> # query\n",
      ">>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      "```\n",
      "add_faiss_index_from_external_arrays\n",
      "( external_arrays: arrayindex_name: strdevice: Optional = Nonestring_factory: Optional = Nonemetric_type: Optional = Nonecustom_index: Optional = Nonebatch_size: int = 1000train_size: Optional = Nonefaiss_verbose: bool = Falsedtype = <class 'numpy.float32'> )\n",
      "Parameters\n",
      "- external_arrays (np.array)  If you want to use arrays from outside the lib for the index, you can set external_arrays. It will use external_arrays to create the Faiss index instead of the arrays in the given column.\n",
      "- index_name (str)  The index_name/identifier of the index. This is the index_name that is used to call get_nearest_examples() or search().\n",
      "- device (Optional Union[int, List[int]], optional)  If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs. If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      "- string_factory (str, optional)  This is passed to the index factory of Faiss to create the index. Default index class is IndexFlat.\n",
      "- metric_type (int, optional)  Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      "- custom_index (faiss.Index, optional)  Custom Faiss index that you already have instantiated and configured for your needs.\n",
      "- batch_size (int, optional)  Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
      "Added in 2.4.0\n",
      "- train_size (int, optional)  If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      "- faiss_verbose (bool, defaults to False)  Enable the verbosity of the Faiss index.\n",
      "- dtype (numpy.dtype)  The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
      "Added in 2.4.0\n",
      "Add a dense index using Faiss for fast retrieval. The index is created using the vectors of external_arrays. You can specify device if you want to run it on GPU (device must be the GPU index). You can find more information about Faiss here:\n",
      "- For string factory\n",
      "save_faiss_index\n",
      "( index_name: strfile: Unionstorage_options: Optional = None )\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index. This is the index_name that is used to call .get_nearest or .search.\n",
      "- file (str)  The path to the serialized faiss index on disk or remote URI (e.g. \"s3://my-bucket/index.faiss\").\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.11.0\n",
      "\n",
      "Added in 2.11.0\n",
      "Save a FaissIndex on disk.\n",
      "load_faiss_index\n",
      "( index_name: strfile: Uniondevice: Union = Nonestorage_options: Optional = None )\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index. This is the index_name that is used to call .get_nearest or .search.\n",
      "- file (str)  The path to the serialized faiss index on disk or remote URI (e.g. \"s3://my-bucket/index.faiss\").\n",
      "- device (Optional Union[int, List[int]])  If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs. If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.11.0\n",
      "\n",
      "Added in 2.11.0\n",
      "Load a FaissIndex from disk.\n",
      "If you want to do additional configurations, you can have access to the faiss index object by doing .get_index(index_name).faiss_index to make it fit your needs.\n",
      "add_elasticsearch_index\n",
      "( column: strindex_name: Optional = Nonehost: Optional = Noneport: Optional = Nonees_client: Optional = Nonees_index_name: Optional = Nonees_index_config: Optional = None )\n",
      "Parameters\n",
      "- column (str)  The column of the documents to add to the index.\n",
      "- index_name (str, optional)  The index_name/identifier of the index. This is the index name that is used to call get_nearest_examples() or Dataset.search(). By default it corresponds to column.\n",
      "- host (str, optional, defaults to localhost)  Host of where ElasticSearch is running.\n",
      "- port (str, optional, defaults to 9200)  Port of where ElasticSearch is running.\n",
      "- es_client (elasticsearch.Elasticsearch, optional)  The elasticsearch client used to create the index if host and port are None.\n",
      "- es_index_name (str, optional)  The elasticsearch index name used to create the index.\n",
      "- es_index_config (dict, optional)  The configuration of the elasticsearch index. Default config is:\n",
      "Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n",
      "Example:\n",
      "```\n",
      ">>> es_client = elasticsearch.Elasticsearch()\n",
      ">>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      ">>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n",
      ">>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n",
      "```\n",
      "load_elasticsearch_index\n",
      "( index_name: stres_index_name: strhost: Optional = Noneport: Optional = Nonees_client: Optional = Nonees_index_config: Optional = None )\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index. This is the index name that is used to call get_nearest or search.\n",
      "- es_index_name (str)  The name of elasticsearch index to load.\n",
      "- host (str, optional, defaults to localhost)  Host of where ElasticSearch is running.\n",
      "- port (str, optional, defaults to 9200)  Port of where ElasticSearch is running.\n",
      "- es_client (elasticsearch.Elasticsearch, optional)  The elasticsearch client used to create the index if host and port are None.\n",
      "- es_index_config (dict, optional)  The configuration of the elasticsearch index. Default config is:\n",
      "Load an existing text index using ElasticSearch for fast retrieval.\n",
      "list_indexes\n",
      "( )\n",
      "List the colindex_nameumns/identifiers of all the attached indexes.\n",
      "get_index\n",
      "( index_name: str )\n",
      "Parameters\n",
      "- index_name (str)  Index name.\n",
      "List the index_name/identifiers of all the attached indexes.\n",
      "drop_index\n",
      "( index_name: str )\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index.\n",
      "Drop the index with the specified column.\n",
      "search\n",
      "( index_name: strquery: Unionk: int = 10**kwargs )  (scores, indices)\n",
      "Parameters\n",
      "- index_name (str)  The name/identifier of the index.\n",
      "- query (Union[str, np.ndarray])  The query as a string if index_name is a text index or as a numpy array if index_name is a vector index.\n",
      "- k (int)  The number of examples to retrieve.\n",
      "Returns\n",
      "(scores, indices)\n",
      "A tuple of (scores, indices) where:\n",
      "scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples\n",
      "indices (List[List[int]]): the indices of the retrieved examples\n",
      "A tuple of (scores, indices) where:\n",
      "- scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples\n",
      "- indices (List[List[int]]): the indices of the retrieved examples\n",
      "Find the nearest examples indices in the dataset to the query.\n",
      "search_batch\n",
      "( index_name: strqueries: Unionk: int = 10**kwargs )  (total_scores, total_indices)\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index.\n",
      "- queries (Union[List[str], np.ndarray])  The queries as a list of strings if index_name is a text index or as a numpy array if index_name is a vector index.\n",
      "- k (int)  The number of examples to retrieve per query.\n",
      "Returns\n",
      "(total_scores, total_indices)\n",
      "A tuple of (total_scores, total_indices) where:\n",
      "total_scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples per query\n",
      "total_indices (List[List[int]]): the indices of the retrieved examples per query\n",
      "A tuple of (total_scores, total_indices) where:\n",
      "- total_scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples per query\n",
      "- total_indices (List[List[int]]): the indices of the retrieved examples per query\n",
      "Find the nearest examples indices in the dataset to the query.\n",
      "get_nearest_examples\n",
      "( index_name: strquery: Unionk: int = 10**kwargs )  (scores, examples)\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index.\n",
      "- query (Union[str, np.ndarray])  The query as a string if index_name is a text index or as a numpy array if index_name is a vector index.\n",
      "- k (int)  The number of examples to retrieve.\n",
      "Returns\n",
      "(scores, examples)\n",
      "A tuple of (scores, examples) where:\n",
      "scores (List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples\n",
      "examples (dict): the retrieved examples\n",
      "A tuple of (scores, examples) where:\n",
      "- scores (List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples\n",
      "- examples (dict): the retrieved examples\n",
      "Find the nearest examples in the dataset to the query.\n",
      "get_nearest_examples_batch\n",
      "( index_name: strqueries: Unionk: int = 10**kwargs )  (total_scores, total_examples)\n",
      "Parameters\n",
      "- index_name (str)  The index_name/identifier of the index.\n",
      "- queries (Union[List[str], np.ndarray])  The queries as a list of strings if index_name is a text index or as a numpy array if index_name is a vector index.\n",
      "- k (int)  The number of examples to retrieve per query.\n",
      "Returns\n",
      "(total_scores, total_examples)\n",
      "A tuple of (total_scores, total_examples) where:\n",
      "total_scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples per query\n",
      "total_examples (List[dict]): the retrieved examples per query\n",
      "A tuple of (total_scores, total_examples) where:\n",
      "- total_scores (List[List[float]): the retrieval scores from either FAISS (IndexFlatL2 by default) or ElasticSearch of the retrieved examples per query\n",
      "- total_examples (List[dict]): the retrieved examples per query\n",
      "Find the nearest examples in the dataset to the query.\n",
      "info\n",
      "( )\n",
      "DatasetInfo object containing all the metadata in the dataset.\n",
      "split\n",
      "( )\n",
      "NamedSplit object corresponding to a named dataset split.\n",
      "builder_name\n",
      "( )\n",
      "citation\n",
      "( )\n",
      "config_name\n",
      "( )\n",
      "dataset_size\n",
      "( )\n",
      "description\n",
      "( )\n",
      "download_checksums\n",
      "( )\n",
      "download_size\n",
      "( )\n",
      "features\n",
      "( )\n",
      "homepage\n",
      "( )\n",
      "license\n",
      "( )\n",
      "size_in_bytes\n",
      "( )\n",
      "supervised_keys\n",
      "( )\n",
      "version\n",
      "( )\n",
      "from_csv\n",
      "( path_or_paths: Unionsplit: Optional = Nonefeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsenum_proc: Optional = None**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (path-like or list of path-like)  Path(s) of the CSV file(s).\n",
      "- split (NamedSplit, optional)  Split name to be assigned to the dataset.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- num_proc (int, optional, defaults to None)  Number of processes when downloading and generating the dataset locally. This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to pandas.read_csv.\n",
      "\n",
      "Added in 2.8.0\n",
      "Create Dataset from CSV file(s).\n",
      "Example:\n",
      "```\n",
      ">>> ds = Dataset.from_csv('path/to/dataset.csv')\n",
      "```\n",
      "from_json\n",
      "( path_or_paths: Unionsplit: Optional = Nonefeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsefield: Optional = Nonenum_proc: Optional = None**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (path-like or list of path-like)  Path(s) of the JSON or JSON Lines file(s).\n",
      "- split (NamedSplit, optional)  Split name to be assigned to the dataset.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- field (str, optional)  Field name of the JSON file where the dataset is contained in.\n",
      "- num_proc (int, optional defaults to None)  Number of processes when downloading and generating the dataset locally. This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to JsonConfig.\n",
      "\n",
      "Added in 2.8.0\n",
      "Create Dataset from JSON or JSON Lines file(s).\n",
      "Example:\n",
      "```\n",
      ">>> ds = Dataset.from_json('path/to/dataset.json')\n",
      "```\n",
      "from_parquet\n",
      "( path_or_paths: Unionsplit: Optional = Nonefeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsecolumns: Optional = Nonenum_proc: Optional = None**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (path-like or list of path-like)  Path(s) of the Parquet file(s).\n",
      "- split (NamedSplit, optional)  Split name to be assigned to the dataset.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- columns (List[str], optional)  If not None, only these columns will be read from the file. A column name may be a prefix of a nested field, e.g. a will select a.b, a.c, and a.d.e.\n",
      "- num_proc (int, optional, defaults to None)  Number of processes when downloading and generating the dataset locally. This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to ParquetConfig.\n",
      "\n",
      "Added in 2.8.0\n",
      "Create Dataset from Parquet file(s).\n",
      "Example:\n",
      "```\n",
      ">>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n",
      "```\n",
      "from_text\n",
      "( path_or_paths: Unionsplit: Optional = Nonefeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsenum_proc: Optional = None**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (path-like or list of path-like)  Path(s) of the text file(s).\n",
      "- split (NamedSplit, optional)  Split name to be assigned to the dataset.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- num_proc (int, optional, defaults to None)  Number of processes when downloading and generating the dataset locally. This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to TextConfig.\n",
      "\n",
      "Added in 2.8.0\n",
      "Create Dataset from text file(s).\n",
      "Example:\n",
      "```\n",
      ">>> ds = Dataset.from_text('path/to/dataset.txt')\n",
      "```\n",
      "from_sql\n",
      "( sql: Unioncon: Unionfeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = False**kwargs )\n",
      "Parameters\n",
      "- sql (str or sqlalchemy.sql.Selectable)  SQL query to be executed or a table name.\n",
      "- con (str or sqlite3.Connection or sqlalchemy.engine.Connection or sqlalchemy.engine.Connection)  A URI string used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to SqlConfig.\n",
      "Create Dataset from SQL query or database table.\n",
      "Example:\n",
      "```\n",
      ">>> # Fetch a database table\n",
      ">>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n",
      ">>> # Execute a SQL query on the table\n",
      ">>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n",
      ">>> # Use a Selectable object to specify the query\n",
      ">>> from sqlalchemy import select, text\n",
      ">>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n",
      ">>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n",
      "```\n",
      "The returned dataset can only be cached if con is specified as URI string.\n",
      "prepare_for_task\n",
      "( task: Unionid: int = 0 )\n",
      "Parameters\n",
      "- task (Union[str, TaskTemplate])  The task to prepare the dataset for during training and evaluation. If str, supported tasks include:\n",
      "\"text-classification\"\n",
      "\"question-answering\"\n",
      "If TaskTemplate, must be one of the task templates in datasets.tasks.\n",
      "- \"text-classification\"\n",
      "- \"question-answering\"\n",
      "- id (int, defaults to 0)  The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
      "\n",
      "- \"text-classification\"\n",
      "- \"question-answering\"\n",
      "If TaskTemplate, must be one of the task templates in datasets.tasks.\n",
      "Prepare a dataset for the given task by casting the datasets Features to standardized column names and types as detailed in datasets.tasks.\n",
      "Casts datasets.DatasetInfo.features according to a task-specific schema. Intended for single-use only, so all task templates are removed from datasets.DatasetInfo.task_templates after casting.\n",
      "align_labels_with_mapping\n",
      "( label2id: Dictlabel_column: str )\n",
      "Parameters\n",
      "- label2id (dict)  The label name to ID mapping to align the dataset with.\n",
      "- label_column (str)  The column name of labels to align on.\n",
      "Align the datasets label ID and label name mapping to match an input label2id mapping. This is useful when you want to ensure that a models predicted labels are aligned with the dataset. The alignment in done using the lowercase label names.\n",
      "Example:\n",
      "```\n",
      ">>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
      ">>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
      ">>> # mapping to align with\n",
      ">>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
      ">>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
      "```\n",
      "datasets.concatenate_datasets\n",
      "( dsets: Listinfo: Optional = Nonesplit: Optional = Noneaxis: int = 0 )\n",
      "Parameters\n",
      "- dsets (List[datasets.Dataset])  List of Datasets to concatenate.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "- axis ({0, 1}, defaults to 0)  Axis to concatenate over, where 0 means over rows (vertically) and 1 means over columns (horizontally).\n",
      "Added in 1.6.0\n",
      "\n",
      "Added in 1.6.0\n",
      "Converts a list of Dataset with the same schema into a single Dataset.\n",
      "Example:\n",
      "```\n",
      ">>> ds3 = concatenate_datasets([ds1, ds2])\n",
      "```\n",
      "datasets.interleave_datasets\n",
      "( datasets: Listprobabilities: Optional = Noneseed: Optional = Noneinfo: Optional = Nonesplit: Optional = Nonestopping_strategy: Literal = 'first_exhausted' )  Dataset or IterableDataset\n",
      "Parameters\n",
      "- datasets (List[Dataset] or List[IterableDataset])  List of datasets to interleave.\n",
      "- probabilities (List[float], optional, defaults to None)  If specified, the new dataset is constructed by sampling examples from one source at a time according to these probabilities.\n",
      "- seed (int, optional, defaults to None)  The random seed used to choose a source for each example.\n",
      "- info (DatasetInfo, optional)  Dataset information, like description, citation, etc.\n",
      "Added in 2.4.0\n",
      "- split (NamedSplit, optional)  Name of the dataset split.\n",
      "Added in 2.4.0\n",
      "- stopping_strategy (str, defaults to first_exhausted)  Two strategies are proposed right now, first_exhausted and all_exhausted. By default, first_exhausted is an undersampling strategy, i.e the dataset construction is stopped as soon as one dataset has ran out of samples. If the strategy is all_exhausted, we use an oversampling strategy, i.e the dataset construction is stopped as soon as every samples of every dataset has been added at least once. Note that if the strategy is all_exhausted, the interleaved dataset size can get enormous:\n",
      "with no probabilities, the resulting dataset will have max_length_datasets*nb_dataset samples.\n",
      "with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n",
      "- with no probabilities, the resulting dataset will have max_length_datasets*nb_dataset samples.\n",
      "- with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n",
      "Added in 2.4.0\n",
      "Added in 2.4.0\n",
      "- with no probabilities, the resulting dataset will have max_length_datasets*nb_dataset samples.\n",
      "- with given probabilities, the resulting dataset will have more samples if some datasets have really low probability of visiting.\n",
      "Returns\n",
      "Dataset or IterableDataset\n",
      "Return type depends on the input datasets parameter. Dataset if the input is a list of Dataset, IterableDataset if the input is a list of IterableDataset.\n",
      "Return type depends on the input datasets parameter. Dataset if the input is a list of Dataset, IterableDataset if the input is a list of IterableDataset.\n",
      "Interleave several datasets (sources) into a single dataset. The new dataset is constructed by alternating between the sources to get the examples.\n",
      "You can use this function on a list of Dataset objects, or on a list of IterableDataset objects.\n",
      "- If probabilities is None (default) the new dataset is constructed by cycling between each source to get the examples.\n",
      "- If probabilities is not None, the new dataset is constructed by getting examples from a random source at a time according to the provided probabilities.\n",
      "The resulting dataset ends when one of the source datasets runs out of examples except when oversampling is True, in which case, the resulting dataset ends when all datasets have ran out of examples at least one time.\n",
      "Note for iterable datasets:\n",
      "In a distributed setup or in PyTorch DataLoader workers, the stopping strategy is applied per process. Therefore the first_exhausted strategy on an sharded iterable dataset can generate less samples in total (up to 1 missing sample per subdataset per worker).\n",
      "Example:\n",
      "For regular datasets (map-style):\n",
      "```\n",
      ">>> from datasets import Dataset, interleave_datasets\n",
      ">>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
      ">>> d2 = Dataset.from_dict({\"a\": [10, 11, 12]})\n",
      ">>> d3 = Dataset.from_dict({\"a\": [20, 21, 22]})\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=\"all_exhausted\")\n",
      ">>> dataset[\"a\"]\n",
      "[10, 0, 11, 1, 2, 20, 12, 10, 0, 1, 2, 21, 0, 11, 1, 2, 0, 1, 12, 2, 10, 0, 22]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n",
      ">>> dataset[\"a\"]\n",
      "[10, 0, 11, 1, 2]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3])\n",
      ">>> dataset[\"a\"]\n",
      "[0, 10, 20, 1, 11, 21, 2, 12, 22]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n",
      ">>> dataset[\"a\"]\n",
      "[0, 10, 20, 1, 11, 21, 2, 12, 22]\n",
      ">>> d1 = Dataset.from_dict({\"a\": [0, 1, 2]})\n",
      ">>> d2 = Dataset.from_dict({\"a\": [10, 11, 12, 13]})\n",
      ">>> d3 = Dataset.from_dict({\"a\": [20, 21, 22, 23, 24]})\n",
      ">>> dataset = interleave_datasets([d1, d2, d3])\n",
      ">>> dataset[\"a\"]\n",
      "[0, 10, 20, 1, 11, 21, 2, 12, 22]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], stopping_strategy=\"all_exhausted\")\n",
      ">>> dataset[\"a\"]\n",
      "[0, 10, 20, 1, 11, 21, 2, 12, 22, 0, 13, 23, 1, 10, 24]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42)\n",
      ">>> dataset[\"a\"]\n",
      "[10, 0, 11, 1, 2]\n",
      ">>> dataset = interleave_datasets([d1, d2, d3], probabilities=[0.7, 0.2, 0.1], seed=42, stopping_strategy=\"all_exhausted\")\n",
      ">>> dataset[\"a\"]\n",
      "[10, 0, 11, 1, 2, 20, 12, 13, ..., 0, 1, 2, 0, 24]\n",
      "For datasets in streaming mode (iterable):\n",
      "\n",
      ">>> from datasets import load_dataset, interleave_datasets\n",
      ">>> d1 = load_dataset(\"oscar\", \"unshuffled_deduplicated_en\", split=\"train\", streaming=True)\n",
      ">>> d2 = load_dataset(\"oscar\", \"unshuffled_deduplicated_fr\", split=\"train\", streaming=True)\n",
      ">>> dataset = interleave_datasets([d1, d2])\n",
      ">>> iterator = iter(dataset)\n",
      ">>> next(iterator)\n",
      "{'text': 'Mtendere Village was inspired by the vision...}\n",
      ">>> next(iterator)\n",
      "{'text': \"Mdia de dbat d'ides, de culture...}\n",
      "```\n",
      "datasets.distributed.split_dataset_by_node\n",
      "( dataset: DatasetTyperank: intworld_size: int )  Dataset or IterableDataset\n",
      "Parameters\n",
      "- dataset (Dataset or IterableDataset)  The dataset to split by node.\n",
      "- rank (int)  Rank of the current node.\n",
      "- world_size (int)  Total number of nodes.\n",
      "Returns\n",
      "Dataset or IterableDataset\n",
      "The dataset to be used on the node at rank rank.\n",
      "The dataset to be used on the node at rank rank.\n",
      "Split a dataset for the node at rank rank in a pool of nodes of size world_size.\n",
      "For map-style datasets:\n",
      "Each node is assigned a chunk of data, e.g. rank 0 is given the first chunk of the dataset. To maximize data loading throughput, chunks are made of contiguous data on disk if possible.\n",
      "For iterable datasets:\n",
      "If the dataset has a number of shards that is a factor of world_size (i.e. if dataset.n_shards % world_size == 0), then the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of world_size, skipping the other examples.\n",
      "datasets.enable_caching\n",
      "( )\n",
      "When applying transforms on a dataset, the data are stored in cache files. The caching mechanism allows to reload an existing cache file if its already been computed.\n",
      "Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated after each transform.\n",
      "If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets. More precisely, if the caching is disabled:\n",
      "- cache files are always recreated\n",
      "- cache files are written to a temporary directory that is deleted when session closes\n",
      "- cache files are named using a random hash instead of the dataset fingerprint\n",
      "- use save_to_disk() to save a transformed dataset or it will be deleted when session closes\n",
      "- caching doesnt affect load_dataset(). If you want to regenerate a dataset from scratch you should use the download_mode parameter in load_dataset().\n",
      "datasets.disable_caching\n",
      "( )\n",
      "When applying transforms on a dataset, the data are stored in cache files. The caching mechanism allows to reload an existing cache file if its already been computed.\n",
      "Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated after each transform.\n",
      "If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets. More precisely, if the caching is disabled:\n",
      "- cache files are always recreated\n",
      "- cache files are written to a temporary directory that is deleted when session closes\n",
      "- cache files are named using a random hash instead of the dataset fingerprint\n",
      "- use save_to_disk() to save a transformed dataset or it will be deleted when session closes\n",
      "- caching doesnt affect load_dataset(). If you want to regenerate a dataset from scratch you should use the download_mode parameter in load_dataset().\n",
      "datasets.is_caching_enabled\n",
      "( )\n",
      "When applying transforms on a dataset, the data are stored in cache files. The caching mechanism allows to reload an existing cache file if its already been computed.\n",
      "Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated after each transform.\n",
      "If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets. More precisely, if the caching is disabled:\n",
      "- cache files are always recreated\n",
      "- cache files are written to a temporary directory that is deleted when session closes\n",
      "- cache files are named using a random hash instead of the dataset fingerprint\n",
      "- use save_to_disk()] to save a transformed dataset or it will be deleted when session closes\n",
      "- caching doesnt affect load_dataset(). If you want to regenerate a dataset from scratch you should use the download_mode parameter in load_dataset().\n",
      "DatasetDict\n",
      "Dictionary with split names as keys (train, test for example), and Dataset objects as values. It also has dataset transform methods like map or filter, to process all the splits at once.\n",
      "class datasets.DatasetDict\n",
      "( )\n",
      "A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)\n",
      "data\n",
      "( )\n",
      "The Apache Arrow tables backing each split.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.data\n",
      "```\n",
      "cache_files\n",
      "( )\n",
      "The cache files containing the Apache Arrow table backing each split.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.cache_files\n",
      "{'test': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow'}],\n",
      " 'train': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow'}],\n",
      " 'validation': [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]}\n",
      "```\n",
      "num_columns\n",
      "( )\n",
      "Number of columns in each split of the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.num_columns\n",
      "{'test': 2, 'train': 2, 'validation': 2}\n",
      "```\n",
      "num_rows\n",
      "( )\n",
      "Number of rows in each split of the dataset (same as datasets.Dataset.len()).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.num_rows\n",
      "{'test': 1066, 'train': 8530, 'validation': 1066}\n",
      "```\n",
      "column_names\n",
      "( )\n",
      "Names of the columns in each split of the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.column_names\n",
      "{'test': ['text', 'label'],\n",
      " 'train': ['text', 'label'],\n",
      " 'validation': ['text', 'label']}\n",
      "```\n",
      "shape\n",
      "( )\n",
      "Shape of each split of the dataset (number of columns, number of rows).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.shape\n",
      "{'test': (1066, 2), 'train': (8530, 2), 'validation': (1066, 2)}\n",
      "```\n",
      "unique\n",
      "( column: str )  Dict[str, list]\n",
      "Parameters\n",
      "- column (str)  column name (list all the column names with column_names)\n",
      "Returns\n",
      "Dict[str, list]\n",
      "Dictionary of unique elements in the given column.\n",
      "Dictionary of unique elements in the given column.\n",
      "Return a list of the unique elements in a column for each split.\n",
      "This is implemented in the low-level backend and as such, very fast.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.unique(\"label\")\n",
      "{'test': [1, 0], 'train': [1, 0], 'validation': [1, 0]}\n",
      "```\n",
      "cleanup_cache_files\n",
      "( )\n",
      "Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one. Be careful when running this command that no other process is currently using other cache files.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.cleanup_cache_files()\n",
      "{'test': 0, 'train': 0, 'validation': 0}\n",
      "```\n",
      "map\n",
      "( function: Optional = Nonewith_indices: bool = Falsewith_rank: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000drop_last_batch: bool = Falseremove_columns: Union = Nonekeep_in_memory: bool = Falseload_from_cache_file: Optional = Nonecache_file_names: Optional = Nonewriter_batch_size: Optional = 1000features: Optional = Nonedisable_nullable: bool = Falsefn_kwargs: Optional = Nonenum_proc: Optional = Nonedesc: Optional = None )\n",
      "Parameters\n",
      "- function (callable)  with one of the following signature:\n",
      "function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "function(example: Dict[str, Any], indices: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged.\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], indices: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx): ....\n",
      "- with_rank (bool, defaults to False)  Provide process rank to function. Note that in this case the signature of function should be def function(example[, idx], rank): ....\n",
      "- input_columns ([Union[str, List[str]]], optional, defaults to None)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True, batch_size <= 0 or batch_size == None then provide the full dataset as a single batch to function.\n",
      "- drop_last_batch (bool, defaults to False)  Whether a last batch smaller than the batch_size should be dropped instead of being processed by the function.\n",
      "- remove_columns ([Union[str, List[str]]], optional, defaults to None)  Remove a selection of columns while doing the mapping. Columns will be removed before updating the examples with the output of function, i.e. if function is adding columns with names in remove_columns, these columns will be kept.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- cache_file_names ([Dict[str, str]], optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name. You have to provide one cache_file_name per dataset in the dataset dictionary.\n",
      "- writer_batch_size (int, default 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- features ([datasets.Features], optional, defaults to None)  Use a specific Features to store the cache file instead of the automatically generated one.\n",
      "- disable_nullable (bool, defaults to False)  Disallow null values in the table.\n",
      "- fn_kwargs (Dict, optional, defaults to None)  Keyword arguments to be passed to function\n",
      "- num_proc (int, optional, defaults to None)  Number of processes for multiprocessing. By default it doesnt use multiprocessing.\n",
      "- desc (str, optional, defaults to None)  Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], indices: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged.\n",
      "Apply a function to all the elements in the table (individually or in batches) and update the table (if function does updated examples). The transformation is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "...     return example\n",
      ">>> ds = ds.map(add_prefix)\n",
      ">>> ds[\"train\"][0:3][\"text\"]\n",
      "['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
      " 'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n",
      " 'Review: effective but too-tepid biopic']\n",
      "\n",
      "# process a batch of examples\n",
      ">>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      "# set number of processors\n",
      ">>> ds = ds.map(add_prefix, num_proc=4)\n",
      "```\n",
      "filter\n",
      "( function: Optional = Nonewith_indices: bool = Falsewith_rank: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000keep_in_memory: bool = Falseload_from_cache_file: Optional = Nonecache_file_names: Optional = Nonewriter_batch_size: Optional = 1000fn_kwargs: Optional = Nonenum_proc: Optional = Nonedesc: Optional = None )\n",
      "Parameters\n",
      "- function (Callable)  Callable with one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "- function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx[, rank]): ....\n",
      "- with_rank (bool, defaults to False)  Provide process rank to function. Note that in this case the signature of function should be def function(example[, idx], rank): ....\n",
      "- input_columns ([Union[str, List[str]]], optional, defaults to None)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True batch_size <= 0 or batch_size == None then provide the full dataset as a single batch to function.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if chaching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- cache_file_names ([Dict[str, str]], optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the results of the computation instead of the automatically generated cache file name. You have to provide one cache_file_name per dataset in the dataset dictionary.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "- fn_kwargs (Dict, optional, defaults to None)  Keyword arguments to be passed to function\n",
      "- num_proc (int, optional, defaults to None)  Number of processes for multiprocessing. By default it doesnt use multiprocessing.\n",
      "- desc (str, optional, defaults to None)  Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> bool if batched=False and with_indices=False and with_rank=False\n",
      "- function(example: Dict[str, Any], *extra_args) -> bool if batched=False and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "- function(batch: Dict[str, List]) -> List[bool] if batched=True and with_indices=False and with_rank=False\n",
      "- function(batch: Dict[str, List], *extra_args) -> List[bool] if batched=True and with_indices=True and/or with_rank=True (one extra arg for each)\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "Apply a filter function to all the elements in the table in batches and update the table so that the dataset only includes examples according to the filter function. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4265\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 533\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 533\n",
      "    })\n",
      "})\n",
      "```\n",
      "sort\n",
      "( column_names: Unionreverse: Union = Falsekind = 'deprecated'null_placement: str = 'at_end'keep_in_memory: bool = Falseload_from_cache_file: Optional = Noneindices_cache_file_names: Optional = Nonewriter_batch_size: Optional = 1000 )\n",
      "Parameters\n",
      "- column_names (Union[str, Sequence[str]])  Column name(s) to sort by.\n",
      "- reverse (Union[bool, Sequence[bool]], defaults to False)  If True, sort by descending order rather than ascending. If a single bool is provided, the value is applied to the sorting of all column names. Otherwise a list of bools with the same length and order as column_names must be provided.\n",
      "- kind (str, optional)  Pandas algorithm for sorting selected in {quicksort, mergesort, heapsort, stable}, The default is quicksort. Note that both stable and mergesort use timsort under the covers and, in general, the actual implementation will vary with data type. The mergesort option is retained for backwards compatibility.\n",
      "Deprecated in 2.8.0\n",
      "kind was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
      "- null_placement (str, defaults to at_end)  Put None values at the beginning if at_start or first or at the end if at_end or last\n",
      "- keep_in_memory (bool, defaults to False)  Keep the sorted indices in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the sorted indices can be identified, use it instead of recomputing.\n",
      "- indices_cache_file_names ([Dict[str, str]], optional, defaults to None)  Provide the name of a path for the cache file. It is used to store the indices mapping instead of the automatically generated cache file name. You have to provide one cache_file_name per dataset in the dataset dictionary.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "kind was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
      "Create a new dataset sorted according to a single or multiple columns.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset('rotten_tomatoes')\n",
      ">>> ds['train']['label'][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      ">>> sorted_ds = ds.sort('label')\n",
      ">>> sorted_ds['train']['label'][:10]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      ">>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n",
      ">>> another_sorted_ds['train']['label'][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "```\n",
      "shuffle\n",
      "( seeds: Union = Noneseed: Optional = Nonegenerators: Optional = Nonekeep_in_memory: bool = Falseload_from_cache_file: Optional = Noneindices_cache_file_names: Optional = Nonewriter_batch_size: Optional = 1000 )\n",
      "Parameters\n",
      "- seeds (Dict[str, int] or int, optional)  A seed to initialize the default BitGenerator if generator=None. If None, then fresh, unpredictable entropy will be pulled from the OS. If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state. You can provide one seed per dataset in the dataset dictionary.\n",
      "- seed (int, optional)  A seed to initialize the default BitGenerator if generator=None. Alias for seeds (a ValueError is raised if both are provided).\n",
      "- generators (Dict[str, *optional*, np.random.Generator])  Numpy random Generator to use to compute the permutation of the dataset rows. If generator=None (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy). You have to provide one generator per dataset in the dataset dictionary.\n",
      "- keep_in_memory (bool, defaults to False)  Keep the dataset in memory instead of writing it to a cache file.\n",
      "- load_from_cache_file (Optional[bool], defaults to True if caching is enabled)  If a cache file storing the current computation from function can be identified, use it instead of recomputing.\n",
      "- indices_cache_file_names (Dict[str, str], optional)  Provide the name of a path for the cache file. It is used to store the indices mappings instead of the automatically generated cache file name. You have to provide one cache_file_name per dataset in the dataset dictionary.\n",
      "- writer_batch_size (int, defaults to 1000)  Number of rows per write operation for the cache file writer. This value is a good trade-off between memory usage during the processing, and processing speed. Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running map.\n",
      "Create a new Dataset where the rows are shuffled.\n",
      "The transformation is applied to all the datasets of the dataset dictionary.\n",
      "Currently shuffling uses numpy random generators. You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPys default random generator (PCG64).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds[\"train\"][\"label\"][:10]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "# set a seed\n",
      ">>> shuffled_ds = ds.shuffle(seed=42)\n",
      ">>> shuffled_ds[\"train\"][\"label\"][:10]\n",
      "[0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "```\n",
      "set_format\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects),\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "Set __getitem__ return format (type and columns). The format is set for every dataset in the dataset dictionary.\n",
      "It is possible to call map after calling set_format. Since map may add new columns, then the list of formatted columns gets updated. In this case, if you apply map on a dataset to add a new column, then this column will be formatted:\n",
      "new formatted columns = (all columns - previously unformatted columns)\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      ">>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      ">>> ds[\"train\"].format\n",
      "{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': 'numpy'}\n",
      "```\n",
      "reset_format\n",
      "( )\n",
      "Reset __getitem__ return format to python objects and all columns. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "Same as self.set_format()\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True), batched=True)\n",
      ">>> ds.set_format(type=\"numpy\", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      ">>> ds[\"train\"].format\n",
      "{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': 'numpy'}\n",
      ">>> ds.reset_format()\n",
      ">>> ds[\"train\"].format\n",
      "{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': None}\n",
      "```\n",
      "formatted_as\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects).\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "To be used in a with statement. Set __getitem__ return format (type and columns). The transformation is applied to all the datasets of the dataset dictionary.\n",
      "with_format\n",
      "( type: Optional = Nonecolumns: Optional = Noneoutput_all_columns: bool = False**format_kwargs )\n",
      "Parameters\n",
      "- type (str, optional)  Output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']. None means __getitem__ returns python objects (default).\n",
      "- columns (List[str], optional)  Columns to format in the output. None means __getitem__ returns all columns (default).\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects).\n",
      "- **format_kwargs (additional keyword arguments)  Keywords arguments passed to the convert function like np.array, torch.tensor or tensorflow.ragged.constant.\n",
      "Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly. The format type (for example numpy) is used to format batches when using __getitem__. The format is set for every dataset in the dataset dictionary.\n",
      "Its also possible to use custom transforms for formatting using with_transform().\n",
      "Contrary to set_format(), with_format returns a new DatasetDict object with new Dataset objects.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      ">>> ds[\"train\"].format\n",
      "{'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': None}\n",
      ">>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      ">>> ds[\"train\"].format\n",
      "{'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " 'format_kwargs': {},\n",
      " 'output_all_columns': False,\n",
      " 'type': 'tensorflow'}\n",
      "```\n",
      "with_transform\n",
      "( transform: Optionalcolumns: Optional = Noneoutput_all_columns: bool = False )\n",
      "Parameters\n",
      "- transform (Callable, optional)  User-defined formatting transform, replaces the format defined by set_format(). A formatting function is a callable that takes a batch (as a dict) as input and returns a batch. This function is applied right before returning the objects in __getitem__.\n",
      "- columns (List[str], optional)  Columns to format in the output. If specified, then the input batch of the transform only contains those columns.\n",
      "- output_all_columns (bool, defaults to False)  Keep un-formatted columns as well in the output (as python objects). If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      "Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called. The transform is set for every dataset in the dataset dictionary\n",
      "As set_format(), this can be reset using reset_format().\n",
      "Contrary to set_transform(), with_transform returns a new DatasetDict object with new Dataset objects.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      ">>> def encode(example):\n",
      "...     return tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
      ">>> ds = ds.with_transform(encode)\n",
      ">>> ds[\"train\"][0]\n",
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
      " 'input_ids': tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,\n",
      "        112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,\n",
      "        112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,\n",
      "        170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,\n",
      "        179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,\n",
      "        188,  1566,  7912, 14516,  6997,   119,   102]),\n",
      " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "```\n",
      "flatten\n",
      "( max_depth = 16 )\n",
      "Flatten the Apache Arrow Table of each split (nested features are flatten). Each column with a struct type is flattened into one column per struct field. Other columns are left unchanged.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"squad\")\n",
      ">>> ds[\"train\"].features\n",
      "{'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'id': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None)}\n",
      ">>> ds.flatten()\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "```\n",
      "cast\n",
      "( features: Features )\n",
      "Parameters\n",
      "- features (Features)  New features to cast the dataset to. The name and order of the fields in the features must match the current column names. The type of the data must also be convertible from one type to the other. For non-trivial conversion, e.g. string <-> ClassLabel you should use map() to update the Dataset.\n",
      "Cast the dataset to a new set of features. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "You can also remove a column using Dataset.map() with feature but cast is in-place (doesnt copy the data to a new dataset) and is thus faster.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> new_features = ds[\"train\"].features.copy()\n",
      ">>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      ">>> new_features['text'] = Value('large_string')\n",
      ">>> ds = ds.cast(new_features)\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='large_string', id=None)}\n",
      "```\n",
      "cast_column\n",
      "( column: strfeature )\n",
      "Parameters\n",
      "- column (str)  Column name.\n",
      "- feature (Feature)  Target feature.\n",
      "Cast column to feature for decoding.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      "```\n",
      "remove_columns\n",
      "( column_names: Union )\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to remove.\n",
      "Remove one or several column(s) from each split in the dataset and the features associated to the column(s).\n",
      "The transformation is applied to all the splits of the dataset dictionary.\n",
      "You can also remove a column using Dataset.map() with remove_columns but the present method is in-place (doesnt copy the data to a new dataset) and is thus faster.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.remove_columns(\"label\")\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "```\n",
      "rename_column\n",
      "( original_column_name: strnew_column_name: str )\n",
      "Parameters\n",
      "- original_column_name (str)  Name of the column to rename.\n",
      "- new_column_name (str)  New name for the column.\n",
      "Rename a column in the dataset and move the features associated to the original column under the new column name. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "You can also rename a column using map() with remove_columns but the present method:\n",
      "- takes care of moving the original features under the new column name.\n",
      "- doesnt copy the data to a new dataset and is thus much faster.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.rename_column(\"label\", \"label_new\")\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label_new'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label_new'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label_new'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "```\n",
      "rename_columns\n",
      "( column_mapping: Dict )  DatasetDict\n",
      "Parameters\n",
      "- column_mapping (Dict[str, str])  A mapping of columns to rename to their new names.\n",
      "Returns\n",
      "DatasetDict\n",
      "A copy of the dataset with renamed columns.\n",
      "A copy of the dataset with renamed columns.\n",
      "Rename several columns in the dataset, and move the features associated to the original columns under the new column names. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_new', 'label_new'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_new', 'label_new'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_new', 'label_new'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "```\n",
      "select_columns\n",
      "( column_names: Union )\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to keep.\n",
      "Select one or several column(s) from each split in the dataset and the features associated to the column(s).\n",
      "The transformation is applied to all the splits of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.select_columns(\"text\")\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8530\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1066\n",
      "    })\n",
      "})\n",
      "```\n",
      "class_encode_column\n",
      "( column: strinclude_nulls: bool = False )\n",
      "Parameters\n",
      "- column (str)  The name of the column to cast.\n",
      "- include_nulls (bool, defaults to False)  Whether to include null values in the class labels. If True, the null values will be encoded as the \"None\" class label.\n",
      "Added in 1.14.2\n",
      "\n",
      "Added in 1.14.2\n",
      "Casts the given column as ClassLabel and updates the tables.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"boolq\")\n",
      ">>> ds[\"train\"].features\n",
      "{'answer': Value(dtype='bool', id=None),\n",
      " 'passage': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.class_encode_column(\"answer\")\n",
      ">>> ds[\"train\"].features\n",
      "{'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
      " 'passage': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None)}\n",
      "```\n",
      "push_to_hub\n",
      "( repo_idconfig_name: str = 'default'set_default: Optional = Nonedata_dir: Optional = Nonecommit_message: Optional = Nonecommit_description: Optional = Noneprivate: Optional = Falsetoken: Optional = Nonerevision: Optional = Nonebranch = 'deprecated'create_pr: Optional = Falsemax_shard_size: Union = Nonenum_shards: Optional = Noneembed_external_files: bool = True )\n",
      "Parameters\n",
      "- repo_id (str)  The ID of the repository to push to in the following format: <user>/<dataset_name> or <org>/<dataset_name>. Also accepts <dataset_name>, which will default to the namespace of the logged-in user.\n",
      "- config_name (str)  Configuration name of a dataset. Defaults to default.\n",
      "- set_default (bool, optional)  Whether to set this configuration as the default one. Otherwise, the default configuration is the one named default.\n",
      "- data_dir (str, optional)  Directory name that will contain the uploaded data files. Defaults to the config_name if different from default, else data.\n",
      "Added in 2.17.0\n",
      "- commit_message (str, optional)  Message to commit while pushing. Will default to \"Upload dataset\".\n",
      "- commit_description (str, optional)  Description of the commit that will be created. Additionally, description of the PR if a PR is created (create_pr is True).\n",
      "Added in 2.16.0\n",
      "- private (bool, optional)  Whether the dataset repository should be set to private or not. Only affects repository creation: a repository that already exists will not be affected by that parameter.\n",
      "- token (str, optional)  An optional authentication token for the Hugging Face Hub. If no token is passed, will default to the token saved locally when logging in with huggingface-cli login. Will raise an error if no token is passed and the user is not logged-in.\n",
      "- revision (str, optional)  Branch to push the uploaded files to. Defaults to the \"main\" branch.\n",
      "Added in 2.15.0\n",
      "- branch (str, optional)  The git branch on which to push the dataset. This defaults to the default branch as specified in your repository, which defaults to \"main\".\n",
      "Deprecated in 2.15.0\n",
      "branch was deprecated in favor of revision in version 2.15.0 and will be removed in 3.0.0.\n",
      "- create_pr (bool, optional, defaults to False)  Whether to create a PR with the uploaded files or directly commit.\n",
      "Added in 2.15.0\n",
      "- max_shard_size (int or str, optional, defaults to \"500MB\")  The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit (like \"500MB\" or \"1GB\").\n",
      "- num_shards (Dict[str, int], optional)  Number of shards to write. By default, the number of shards depends on max_shard_size. Use a dictionary to define a different num_shards for each split.\n",
      "Added in 2.8.0\n",
      "- embed_external_files (bool, defaults to True)  Whether to embed file bytes in the shards. In particular, this will do the following before the push for the fields of type:\n",
      "Audio and Image removes local path information and embed file content in the Parquet files.\n",
      "- Audio and Image removes local path information and embed file content in the Parquet files.\n",
      "\n",
      "Added in 2.17.0\n",
      "\n",
      "Added in 2.16.0\n",
      "\n",
      "Added in 2.15.0\n",
      "\n",
      "Deprecated in 2.15.0\n",
      "branch was deprecated in favor of revision in version 2.15.0 and will be removed in 3.0.0.\n",
      "\n",
      "Added in 2.15.0\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "- Audio and Image removes local path information and embed file content in the Parquet files.\n",
      "Pushes the DatasetDict to the hub as a Parquet dataset. The DatasetDict is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      "Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
      "The resulting Parquet files are self-contained by default: if your dataset contains Image or Audio data, the Parquet files will store the bytes of your images or audio files. You can disable this by setting embed_external_files to False.\n",
      "Example:\n",
      "```\n",
      ">>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
      ">>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
      ">>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
      ">>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", num_shards={\"train\": 1024, \"test\": 8})\n",
      "```\n",
      "If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
      "```\n",
      ">>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
      ">>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
      ">>> # later\n",
      ">>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
      ">>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
      "```\n",
      "save_to_disk\n",
      "( dataset_dict_path: Unionfs = 'deprecated'max_shard_size: Union = Nonenum_shards: Optional = Nonenum_proc: Optional = Nonestorage_options: Optional = None )\n",
      "Parameters\n",
      "- dataset_dict_path (str)  Path (e.g. dataset/train) or remote URI (e.g. s3://my-bucket/dataset/train) of the dataset dict directory where the dataset dict will be saved to.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem where the dataset will be saved to.\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "- max_shard_size (int or str, optional, defaults to \"500MB\")  The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit (like \"50MB\").\n",
      "- num_shards (Dict[str, int], optional)  Number of shards to write. By default the number of shards depends on max_shard_size and num_proc. You need to provide the number of shards for each dataset in the dataset dictionary. Use a dictionary to define a different num_shards for each split.\n",
      "Added in 2.8.0\n",
      "- num_proc (int, optional, default None)  Number of processes when downloading and generating the dataset locally. Multiprocessing is disabled by default.\n",
      "Added in 2.8.0\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.8.0\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "Added in 2.8.0\n",
      "\n",
      "Added in 2.8.0\n",
      "Saves a dataset dict to a filesystem using fsspec.spec.AbstractFileSystem.\n",
      "For Image and Audio data:\n",
      "All the Image() and Audio() data are stored in the arrow files. If you want to store paths or urls, please use the Value(string) type.\n",
      "Example:\n",
      "```\n",
      ">>> dataset_dict.save_to_disk(\"path/to/dataset/directory\")\n",
      ">>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
      ">>> dataset_dict.save_to_disk(\"path/to/dataset/directory\", num_shards={\"train\": 1024, \"test\": 8})\n",
      "```\n",
      "load_from_disk\n",
      "( dataset_dict_path: Unionfs = 'deprecated'keep_in_memory: Optional = Nonestorage_options: Optional = None )\n",
      "Parameters\n",
      "- dataset_dict_path (str)  Path (e.g. \"dataset/train\") or remote URI (e.g. \"s3//my-bucket/dataset/train\") of the dataset dict directory where the dataset dict will be loaded from.\n",
      "- fs (fsspec.spec.AbstractFileSystem, optional)  Instance of the remote filesystem where the dataset will be saved to.\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "- keep_in_memory (bool, defaults to None)  Whether to copy the dataset in-memory. If None, the dataset will not be copied in-memory unless explicitly enabled by setting datasets.config.IN_MEMORY_MAX_SIZE to nonzero. See more details in the improve performance section.\n",
      "- storage_options (dict, optional)  Key/value pairs to be passed on to the file-system backend, if any.\n",
      "Added in 2.8.0\n",
      "\n",
      "Deprecated in 2.8.0\n",
      "fs was deprecated in version 2.8.0 and will be removed in 3.0.0. Please use storage_options instead, e.g. storage_options=fs.storage_options\n",
      "\n",
      "Added in 2.8.0\n",
      "Load a dataset that was previously saved using save_to_disk from a filesystem using fsspec.spec.AbstractFileSystem.\n",
      "Example:\n",
      "```\n",
      ">>> ds = load_from_disk('path/to/dataset/directory')\n",
      "```\n",
      "from_csv\n",
      "( path_or_paths: Dictfeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = False**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (dict of path-like)  Path(s) of the CSV file(s).\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to pandas.read_csv.\n",
      "Create DatasetDict from CSV file(s).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import DatasetDict\n",
      ">>> ds = DatasetDict.from_csv({'train': 'path/to/dataset.csv'})\n",
      "```\n",
      "from_json\n",
      "( path_or_paths: Dictfeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = False**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (path-like or list of path-like)  Path(s) of the JSON Lines file(s).\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to JsonConfig.\n",
      "Create DatasetDict from JSON Lines file(s).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import DatasetDict\n",
      ">>> ds = DatasetDict.from_json({'train': 'path/to/dataset.json'})\n",
      "```\n",
      "from_parquet\n",
      "( path_or_paths: Dictfeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = Falsecolumns: Optional = None**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (dict of path-like)  Path(s) of the CSV file(s).\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- columns (List[str], optional)  If not None, only these columns will be read from the file. A column name may be a prefix of a nested field, e.g. a will select a.b, a.c, and a.d.e.\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to ParquetConfig.\n",
      "Create DatasetDict from Parquet file(s).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import DatasetDict\n",
      ">>> ds = DatasetDict.from_parquet({'train': 'path/to/dataset/parquet'})\n",
      "```\n",
      "from_text\n",
      "( path_or_paths: Dictfeatures: Optional = Nonecache_dir: str = Nonekeep_in_memory: bool = False**kwargs )\n",
      "Parameters\n",
      "- path_or_paths (dict of path-like)  Path(s) of the text file(s).\n",
      "- features (Features, optional)  Dataset features.\n",
      "- cache_dir (str, optional, defaults to \"~/.cache/huggingface/datasets\")  Directory to cache data.\n",
      "- keep_in_memory (bool, defaults to False)  Whether to copy the data in-memory.\n",
      "- **kwargs (additional keyword arguments)  Keyword arguments to be passed to TextConfig.\n",
      "Create DatasetDict from text file(s).\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import DatasetDict\n",
      ">>> ds = DatasetDict.from_text({'train': 'path/to/dataset.txt'})\n",
      "```\n",
      "prepare_for_task\n",
      "( task: Unionid: int = 0 )\n",
      "Parameters\n",
      "- task (Union[str, TaskTemplate])  The task to prepare the dataset for during training and evaluation. If str, supported tasks include:\n",
      "\"text-classification\"\n",
      "\"question-answering\"\n",
      "If TaskTemplate, must be one of the task templates in datasets.tasks.\n",
      "- \"text-classification\"\n",
      "- \"question-answering\"\n",
      "- id (int, defaults to 0)  The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
      "\n",
      "- \"text-classification\"\n",
      "- \"question-answering\"\n",
      "If TaskTemplate, must be one of the task templates in datasets.tasks.\n",
      "Prepare a dataset for the given task by casting the datasets Features to standardized column names and types as detailed in datasets.tasks.\n",
      "Casts datasets.DatasetInfo.features according to a task-specific schema. Intended for single-use only, so all task templates are removed from datasets.DatasetInfo.task_templates after casting.\n",
      "IterableDataset\n",
      "The base class IterableDataset implements an iterable Dataset backed by python generators.\n",
      "class datasets.IterableDataset\n",
      "( ex_iterable: _BaseExamplesIterableinfo: Optional = Nonesplit: Optional = Noneformatting: Optional = Noneshuffling: Optional = Nonedistributed: Optional = Nonetoken_per_repo_id: Optional = Noneformat_type = 'deprecated' )\n",
      "A Dataset backed by an iterable.\n",
      "from_generator\n",
      "( generator: Callablefeatures: Optional = Nonegen_kwargs: Optional = None )  IterableDataset\n",
      "Parameters\n",
      "- generator (Callable)  A generator function that yields examples.\n",
      "- features (Features, optional)  Dataset features.\n",
      "- gen_kwargs(dict, optional)  Keyword arguments to be passed to the generator callable. You can define a sharded iterable dataset by passing the list of shards in gen_kwargs. This can be used to improve shuffling and when iterating over the dataset with multiple workers.\n",
      "Returns\n",
      "IterableDataset\n",
      "\n",
      "Create an Iterable Dataset from a generator.\n",
      "Example:\n",
      "```\n",
      ">>> def gen():\n",
      "...     yield {\"text\": \"Good\", \"label\": 0}\n",
      "...     yield {\"text\": \"Bad\", \"label\": 1}\n",
      "...\n",
      ">>> ds = IterableDataset.from_generator(gen)\n",
      "```\n",
      "```\n",
      ">>> def gen(shards):\n",
      "...     for shard in shards:\n",
      "...         with open(shard) as f:\n",
      "...             for line in f:\n",
      "...                 yield {\"line\": line}\n",
      "...\n",
      ">>> shards = [f\"data{i}.txt\" for i in range(32)]\n",
      ">>> ds = IterableDataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n",
      ">>> ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer\n",
      ">>> from torch.utils.data import DataLoader\n",
      ">>> dataloader = DataLoader(ds.with_format(\"torch\"), num_workers=4)  # give each worker a subset of 32/4=8 shards\n",
      "```\n",
      "remove_columns\n",
      "( column_names: Union )  IterableDataset\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to remove.\n",
      "Returns\n",
      "IterableDataset\n",
      "A copy of the dataset object without the columns to remove.\n",
      "A copy of the dataset object without the columns to remove.\n",
      "Remove one or several column(s) in the dataset and the features associated to them. The removal is done on-the-fly on the examples when iterating over the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> next(iter(ds))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      ">>> ds = ds.remove_columns(\"label\")\n",
      ">>> next(iter(ds))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "select_columns\n",
      "( column_names: Union )  IterableDataset\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to select.\n",
      "Returns\n",
      "IterableDataset\n",
      "A copy of the dataset object with selected columns.\n",
      "A copy of the dataset object with selected columns.\n",
      "Select one or several column(s) in the dataset and the features associated to them. The selection is done on-the-fly on the examples when iterating over the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> next(iter(ds))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'label': 1}\n",
      ">>> ds = ds.select_columns(\"text\")\n",
      ">>> next(iter(ds))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "cast_column\n",
      "( column: strfeature: Union )  IterableDataset\n",
      "Parameters\n",
      "- column (str)  Column name.\n",
      "- feature (Feature)  Target feature.\n",
      "Returns\n",
      "IterableDataset\n",
      "\n",
      "Cast column to feature for decoding.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset, Audio\n",
      ">>> ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\", streaming=True)\n",
      ">>> ds.features\n",
      "{'audio': Audio(sampling_rate=8000, mono=True, decode=True, id=None),\n",
      " 'english_transcription': Value(dtype='string', id=None),\n",
      " 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n",
      " 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n",
      " 'path': Value(dtype='string', id=None),\n",
      " 'transcription': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
      ">>> ds.features\n",
      "{'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None),\n",
      " 'english_transcription': Value(dtype='string', id=None),\n",
      " 'intent_class': ClassLabel(num_classes=14, names=['abroad', 'address', 'app_error', 'atm_limit', 'balance', 'business_loan',  'card_issues', 'cash_deposit', 'direct_debit', 'freeze', 'high_value_payment', 'joint_account', 'latest_transactions', 'pay_bill'], id=None),\n",
      " 'lang_id': ClassLabel(num_classes=14, names=['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR',  'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN'], id=None),\n",
      " 'path': Value(dtype='string', id=None),\n",
      " 'transcription': Value(dtype='string', id=None)}\n",
      "```\n",
      "cast\n",
      "( features: Features )  IterableDataset\n",
      "Parameters\n",
      "- features (Features)  New features to cast the dataset to. The name of the fields in the features must match the current column names. The type of the data must also be convertible from one type to the other. For non-trivial conversion, e.g. string <-> ClassLabel you should use map() to update the Dataset.\n",
      "Returns\n",
      "IterableDataset\n",
      "A copy of the dataset with casted features.\n",
      "A copy of the dataset with casted features.\n",
      "Cast the dataset to a new set of features.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> new_features = ds.features.copy()\n",
      ">>> new_features[\"label\"] = ClassLabel(names=[\"bad\", \"good\"])\n",
      ">>> new_features[\"text\"] = Value(\"large_string\")\n",
      ">>> ds = ds.cast(new_features)\n",
      ">>> ds.features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='large_string', id=None)}\n",
      "```\n",
      "__iter__\n",
      "( )\n",
      "iter\n",
      "( batch_size: intdrop_last_batch: bool = False )\n",
      "Parameters\n",
      "- batch_size (int)  size of each batch to yield.\n",
      "- drop_last_batch (bool, default False)  Whether a last batch smaller than the batch_size should be dropped\n",
      "Iterate through the batches of size batch_size.\n",
      "map\n",
      "( function: Optional = Nonewith_indices: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000drop_last_batch: bool = Falseremove_columns: Union = Nonefeatures: Optional = Nonefn_kwargs: Optional = None )\n",
      "Parameters\n",
      "- function (Callable, optional, defaults to None)  Function applied on-the-fly on the examples when you iterate on the dataset. It must have one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx[, rank]): ....\n",
      "- input_columns (Optional[Union[str, List[str]]], defaults to None)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True. batch_size <= 0 or batch_size == None then provide the full dataset as a single batch to function.\n",
      "- drop_last_batch (bool, defaults to False)  Whether a last batch smaller than the batch_size should be dropped instead of being processed by the function.\n",
      "- remove_columns ([List[str]], optional, defaults to None)  Remove a selection of columns while doing the mapping. Columns will be removed before updating the examples with the output of function, i.e. if function is adding columns with names in remove_columns, these columns will be kept.\n",
      "- features ([Features], optional, defaults to None)  Feature types of the resulting dataset.\n",
      "- fn_kwargs (Dict, optional, default None)  Keyword arguments to be passed to function.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "Apply a function to all the examples in the iterable dataset (individually or in batches) and update them. If your function returns a column that already exists, then it overwrites it. The function is applied on-the-fly on the examples when iterating over the dataset.\n",
      "You can specify whether the function should be batched or not with the batched parameter:\n",
      "- If batched is False, then the function takes 1 example in and should return 1 example. An example is a dictionary, e.g. {\"text\": \"Hello there !\"}.\n",
      "- If batched is True and batch_size is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples. A batch is a dictionary, e.g. a batch of 1 example is {text: [Hello there !]}.\n",
      "- If batched is True and batch_size is n > 1, then the function takes a batch of n examples as input and can return a batch with n examples, or with an arbitrary number of examples. Note that the last batch may have less than n examples. A batch is a dictionary, e.g. a batch of n examples is {\"text\": [\"Hello there !\"] * n}.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "...     return example\n",
      ">>> ds = ds.map(add_prefix)\n",
      ">>> list(ds.take(3))\n",
      "[{'label': 1,\n",
      " 'text': 'Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
      " {'label': 1,\n",
      " 'text': 'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
      " {'label': 1, 'text': 'Review: effective but too-tepid biopic'}]\n",
      "```\n",
      "rename_column\n",
      "( original_column_name: strnew_column_name: str )  IterableDataset\n",
      "Parameters\n",
      "- original_column_name (str)  Name of the column to rename.\n",
      "- new_column_name (str)  New name for the column.\n",
      "Returns\n",
      "IterableDataset\n",
      "A copy of the dataset with a renamed column.\n",
      "A copy of the dataset with a renamed column.\n",
      "Rename a column in the dataset, and move the features associated to the original column under the new column name.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> next(iter(ds))\n",
      "{'label': 1,\n",
      " 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      ">>> ds = ds.rename_column(\"text\", \"movie_review\")\n",
      ">>> next(iter(ds))\n",
      "{'label': 1,\n",
      " 'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "filter\n",
      "( function: Optional = Nonewith_indices = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000fn_kwargs: Optional = None )\n",
      "Parameters\n",
      "- function (Callable)  Callable with one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "- function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "- function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "- function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "- function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx): ....\n",
      "- input_columns (str or List[str], optional)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, default 1000)  Number of examples per batch provided to function if batched=True.\n",
      "- fn_kwargs (Dict, optional, default None)  Keyword arguments to be passed to function.\n",
      "\n",
      "- function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "- function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "- function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "- function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "Apply a filter function to all the elements so that the dataset only includes examples according to the filter function. The filtering is done on-the-fly when iterating over the dataset.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> ds = ds.filter(lambda x: x[\"label\"] == 0)\n",
      ">>> list(ds.take(3))\n",
      "[{'label': 0, 'movie_review': 'simplistic , silly and tedious .'},\n",
      " {'label': 0,\n",
      " 'movie_review': \"it's so laddish and juvenile , only teenage boys could possibly find it funny .\"},\n",
      " {'label': 0,\n",
      " 'movie_review': 'exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]\n",
      "```\n",
      "shuffle\n",
      "( seed = Nonegenerator: Optional = Nonebuffer_size: int = 1000 )\n",
      "Parameters\n",
      "- seed (int, optional, defaults to None)  Random seed that will be used to shuffle the dataset. It is used to sample from the shuffle buffer and also to shuffle the data shards.\n",
      "- generator (numpy.random.Generator, optional)  Numpy random Generator to use to compute the permutation of the dataset rows. If generator=None (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      "- buffer_size (int, defaults to 1000)  Size of the buffer.\n",
      "Randomly shuffles the elements of this dataset.\n",
      "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
      "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1000, then shuffle will initially select a random element from only the first 1000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element buffer.\n",
      "If the dataset is made of several shards, it also does shuffle the order of the shards. However if the order has been fixed by using skip() or take() then the order of the shards is kept unchanged.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> list(ds.take(3))\n",
      "[{'label': 1,\n",
      " 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
      " {'label': 1,\n",
      " 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
      " {'label': 1, 'text': 'effective but too-tepid biopic'}]\n",
      ">>> shuffled_ds = ds.shuffle(seed=42)\n",
      ">>> list(shuffled_ds.take(3))\n",
      "[{'label': 1,\n",
      " 'text': \"a sports movie with action that's exciting on the field and a story you care about off it .\"},\n",
      " {'label': 1,\n",
      " 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},\n",
      " {'label': 1,\n",
      " 'text': \"sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune .\"}]\n",
      "```\n",
      "skip\n",
      "( n )\n",
      "Parameters\n",
      "- n (int)  Number of elements to skip.\n",
      "Create a new IterableDataset that skips the first n elements.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> list(ds.take(3))\n",
      "[{'label': 1,\n",
      " 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
      " {'label': 1,\n",
      " 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
      " {'label': 1, 'text': 'effective but too-tepid biopic'}]\n",
      ">>> ds = ds.skip(1)\n",
      ">>> list(ds.take(3))\n",
      "[{'label': 1,\n",
      " 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
      " {'label': 1, 'text': 'effective but too-tepid biopic'},\n",
      " {'label': 1,\n",
      " 'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]\n",
      "```\n",
      "take\n",
      "( n )\n",
      "Parameters\n",
      "- n (int)  Number of elements to take.\n",
      "Create a new IterableDataset with only the first n elements.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\n",
      ">>> small_ds = ds.take(2)\n",
      ">>> list(small_ds)\n",
      "[{'label': 1,\n",
      " 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
      " {'label': 1,\n",
      " 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'}]\n",
      "```\n",
      "info\n",
      "( )\n",
      "DatasetInfo object containing all the metadata in the dataset.\n",
      "split\n",
      "( )\n",
      "NamedSplit object corresponding to a named dataset split.\n",
      "builder_name\n",
      "( )\n",
      "citation\n",
      "( )\n",
      "config_name\n",
      "( )\n",
      "dataset_size\n",
      "( )\n",
      "description\n",
      "( )\n",
      "download_checksums\n",
      "( )\n",
      "download_size\n",
      "( )\n",
      "features\n",
      "( )\n",
      "homepage\n",
      "( )\n",
      "license\n",
      "( )\n",
      "size_in_bytes\n",
      "( )\n",
      "supervised_keys\n",
      "( )\n",
      "version\n",
      "( )\n",
      "IterableDatasetDict\n",
      "Dictionary with split names as keys (train, test for example), and IterableDataset objects as values.\n",
      "class datasets.IterableDatasetDict\n",
      "( )\n",
      "map\n",
      "( function: Optional = Nonewith_indices: bool = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: int = 1000drop_last_batch: bool = Falseremove_columns: Union = Nonefn_kwargs: Optional = None )\n",
      "Parameters\n",
      "- function (Callable, optional, defaults to None)  Function applied on-the-fly on the examples when you iterate on the dataset. It must have one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx[, rank]): ....\n",
      "- input_columns ([Union[str, List[str]]], optional, defaults to None)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function.\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True.\n",
      "- drop_last_batch (bool, defaults to False)  Whether a last batch smaller than the batch_size should be dropped instead of being processed by the function.\n",
      "- remove_columns ([List[str]], optional, defaults to None)  Remove a selection of columns while doing the mapping. Columns will be removed before updating the examples with the output of function, i.e. if function is adding columns with names in remove_columns, these columns will be kept.\n",
      "- fn_kwargs (Dict, optional, defaults to None)  Keyword arguments to be passed to function\n",
      "\n",
      "- function(example: Dict[str, Any]) -> Dict[str, Any] if batched=False and with_indices=False\n",
      "- function(example: Dict[str, Any], idx: int) -> Dict[str, Any] if batched=False and with_indices=True\n",
      "- function(batch: Dict[str, List]) -> Dict[str, List] if batched=True and with_indices=False\n",
      "- function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List] if batched=True and with_indices=True\n",
      "For advanced usage, the function can also return a pyarrow.Table. Moreover if your function returns nothing (None), then map will run your function and return the dataset unchanged. If no function is provided, default to identity function: lambda x: x.\n",
      "Apply a function to all the examples in the iterable dataset (individually or in batches) and update them. If your function returns a column that already exists, then it overwrites it. The function is applied on-the-fly on the examples when iterating over the dataset. The transformation is applied to all the datasets of the dataset dictionary.\n",
      "You can specify whether the function should be batched or not with the batched parameter:\n",
      "- If batched is False, then the function takes 1 example in and should return 1 example. An example is a dictionary, e.g. {\"text\": \"Hello there !\"}.\n",
      "- If batched is True and batch_size is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples. A batch is a dictionary, e.g. a batch of 1 example is {\"text\": [\"Hello there !\"]}.\n",
      "- If batched is True and batch_size is n > 1, then the function takes a batch of n examples as input and can return a batch with n examples, or with an arbitrary number of examples. Note that the last batch may have less than n examples. A batch is a dictionary, e.g. a batch of n examples is {\"text\": [\"Hello there !\"] * n}.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "...     return example\n",
      ">>> ds = ds.map(add_prefix)\n",
      ">>> next(iter(ds[\"train\"]))\n",
      "{'label': 1,\n",
      " 'text': 'Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "filter\n",
      "( function: Optional = Nonewith_indices = Falseinput_columns: Union = Nonebatched: bool = Falsebatch_size: Optional = 1000fn_kwargs: Optional = None )\n",
      "Parameters\n",
      "- function (Callable)  Callable with one of the following signatures:\n",
      "function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "- function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "- function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "- function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "- function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "- with_indices (bool, defaults to False)  Provide example indices to function. Note that in this case the signature of function should be def function(example, idx): ....\n",
      "- input_columns (str or List[str], optional)  The columns to be passed into function as positional arguments. If None, a dict mapping to all formatted columns is passed as one argument.\n",
      "- batched (bool, defaults to False)  Provide batch of examples to function\n",
      "- batch_size (int, optional, defaults to 1000)  Number of examples per batch provided to function if batched=True.\n",
      "- fn_kwargs (Dict, optional, defaults to None)  Keyword arguments to be passed to function\n",
      "\n",
      "- function(example: Dict[str, Any]) -> bool if with_indices=False, batched=False\n",
      "- function(example: Dict[str, Any], indices: int) -> bool if with_indices=True, batched=False\n",
      "- function(example: Dict[str, List]) -> List[bool] if with_indices=False, batched=True\n",
      "- function(example: Dict[str, List], indices: List[int]) -> List[bool] if with_indices=True, batched=True\n",
      "If no function is provided, defaults to an always True function: lambda x: True.\n",
      "Apply a filter function to all the elements so that the dataset only includes examples according to the filter function. The filtering is done on-the-fly when iterating over the dataset. The filtering is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds = ds.filter(lambda x: x[\"label\"] == 0)\n",
      ">>> list(ds[\"train\"].take(3))\n",
      "[{'label': 0, 'text': 'Review: simplistic , silly and tedious .'},\n",
      " {'label': 0,\n",
      " 'text': \"Review: it's so laddish and juvenile , only teenage boys could possibly find it funny .\"},\n",
      " {'label': 0,\n",
      " 'text': 'Review: exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .'}]\n",
      "```\n",
      "shuffle\n",
      "( seed = Nonegenerator: Optional = Nonebuffer_size: int = 1000 )\n",
      "Parameters\n",
      "- seed (int, optional, defaults to None)  Random seed that will be used to shuffle the dataset. It is used to sample from the shuffle buffer and also to shuffle the data shards.\n",
      "- generator (numpy.random.Generator, optional)  Numpy random Generator to use to compute the permutation of the dataset rows. If generator=None (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      "- buffer_size (int, defaults to 1000)  Size of the buffer.\n",
      "Randomly shuffles the elements of this dataset. The shuffling is applied to all the datasets of the dataset dictionary.\n",
      "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
      "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1000, then shuffle will initially select a random element from only the first 1000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1000 element buffer.\n",
      "If the dataset is made of several shards, it also does shuffle the order of the shards. However if the order has been fixed by using skip() or take() then the order of the shards is kept unchanged.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> list(ds[\"train\"].take(3))\n",
      "[{'label': 1,\n",
      " 'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
      " {'label': 1,\n",
      " 'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
      " {'label': 1, 'text': 'effective but too-tepid biopic'}]\n",
      ">>> ds = ds.shuffle(seed=42)\n",
      ">>> list(ds[\"train\"].take(3))\n",
      "[{'label': 1,\n",
      " 'text': \"a sports movie with action that's exciting on the field and a story you care about off it .\"},\n",
      " {'label': 1,\n",
      " 'text': 'at its best , the good girl is a refreshingly adult take on adultery . . .'},\n",
      " {'label': 1,\n",
      " 'text': \"sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man's ruin may be another's fortune .\"}]\n",
      "```\n",
      "with_format\n",
      "( type: Optional = None )\n",
      "Parameters\n",
      "- type (str, optional, defaults to None)  If set to torch, the returned dataset will be a subclass of torch.utils.data.IterableDataset to be used in a DataLoader.\n",
      "Return a dataset with the specified format. This method only supports the torch format for now. The format is set to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> from transformers import AutoTokenizer\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      ">>> def encode(example):\n",
      "...     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
      ">>> ds = ds.map(encode, batched=True, remove_columns=[\"text\"])\n",
      ">>> ds = ds.with_format(\"torch\")\n",
      "```\n",
      "cast\n",
      "( features: Features )  IterableDatasetDict\n",
      "Parameters\n",
      "- features (Features)  New features to cast the dataset to. The name of the fields in the features must match the current column names. The type of the data must also be convertible from one type to the other. For non-trivial conversion, e.g. string <-> ClassLabel you should use map to update the Dataset.\n",
      "Returns\n",
      "IterableDatasetDict\n",
      "A copy of the dataset with casted features.\n",
      "A copy of the dataset with casted features.\n",
      "Cast the dataset to a new set of features. The type casting is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> new_features = ds[\"train\"].features.copy()\n",
      ">>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      ">>> new_features['text'] = Value('large_string')\n",
      ">>> ds = ds.cast(new_features)\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='large_string', id=None)}\n",
      "```\n",
      "cast_column\n",
      "( column: strfeature: Union )\n",
      "Parameters\n",
      "- column (str)  Column name.\n",
      "- feature (Feature)  Target feature.\n",
      "Cast column to feature for decoding. The type casting is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      ">>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      ">>> ds[\"train\"].features\n",
      "{'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      "```\n",
      "remove_columns\n",
      "( column_names: Union )  IterableDatasetDict\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to remove.\n",
      "Returns\n",
      "IterableDatasetDict\n",
      "A copy of the dataset object without the columns to remove.\n",
      "A copy of the dataset object without the columns to remove.\n",
      "Remove one or several column(s) in the dataset and the features associated to them. The removal is done on-the-fly on the examples when iterating over the dataset. The removal is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds = ds.remove_columns(\"label\")\n",
      ">>> next(iter(ds[\"train\"]))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "rename_column\n",
      "( original_column_name: strnew_column_name: str )  IterableDatasetDict\n",
      "Parameters\n",
      "- original_column_name (str)  Name of the column to rename.\n",
      "- new_column_name (str)  New name for the column.\n",
      "Returns\n",
      "IterableDatasetDict\n",
      "A copy of the dataset with a renamed column.\n",
      "A copy of the dataset with a renamed column.\n",
      "Rename a column in the dataset, and move the features associated to the original column under the new column name. The renaming is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds = ds.rename_column(\"text\", \"movie_review\")\n",
      ">>> next(iter(ds[\"train\"]))\n",
      "{'label': 1,\n",
      " 'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "rename_columns\n",
      "( column_mapping: Dict )  IterableDatasetDict\n",
      "Parameters\n",
      "- column_mapping (Dict[str, str])  A mapping of columns to rename to their new names.\n",
      "Returns\n",
      "IterableDatasetDict\n",
      "A copy of the dataset with renamed columns\n",
      "A copy of the dataset with renamed columns\n",
      "Rename several columns in the dataset, and move the features associated to the original columns under the new column names. The renaming is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds = ds.rename_columns({\"text\": \"movie_review\", \"label\": \"rating\"})\n",
      ">>> next(iter(ds[\"train\"]))\n",
      "{'movie_review': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
      " 'rating': 1}\n",
      "```\n",
      "select_columns\n",
      "( column_names: Union )  IterableDatasetDict\n",
      "Parameters\n",
      "- column_names (Union[str, List[str]])  Name of the column(s) to keep.\n",
      "Returns\n",
      "IterableDatasetDict\n",
      "A copy of the dataset object with only selected columns.\n",
      "A copy of the dataset object with only selected columns.\n",
      "Select one or several column(s) in the dataset and the features associated to them. The selection is done on-the-fly on the examples when iterating over the dataset. The selection is applied to all the datasets of the dataset dictionary.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", streaming=True)\n",
      ">>> ds = ds.select(\"text\")\n",
      ">>> next(iter(ds[\"train\"]))\n",
      "{'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}\n",
      "```\n",
      "Features\n",
      "class datasets.Features\n",
      "( *args**kwargs )\n",
      "A special dictionary that defines the internal structure of a dataset.\n",
      "Instantiated with a dictionary of type dict[str, FieldType], where keys are the desired column names, and values are the type of that column.\n",
      "FieldType can be one of the following:\n",
      "- a Value feature specifies a single typed value, e.g. int64 or string.\n",
      "- a ClassLabel feature specifies a field with a predefined set of classes which can have labels associated to them and will be stored as integers in the dataset.\n",
      "- a python dict which specifies that the field is a nested field containing a mapping of sub-fields to sub-fields features. Its possible to have nested fields of nested fields in an arbitrary manner.\n",
      "- a python list or a Sequence specifies that the field contains a list of objects. The python list or Sequence should be provided with a single sub-feature as an example of the feature type hosted in this list.\n",
      "A Sequence with a internal dictionary feature will be automatically converted into a dictionary of lists. This behavior is implemented to have a compatilbity layer with the TensorFlow Datasets library but may be un-wanted in some cases. If you dont want this behavior, you can use a python list instead of the Sequence.\n",
      "- a Array2D, Array3D, Array4D or Array5D feature for multidimensional arrays.\n",
      "- an Audio feature to store the absolute path to an audio file or a dictionary with the relative path to an audio file (path key) and its bytes content (bytes key). This feature extracts the audio data.\n",
      "- an Image feature to store the absolute path to an image file, an np.ndarray object, a PIL.Image.Image object or a dictionary with the relative path to an image file (path key) and its bytes content (bytes key). This feature extracts the image data.\n",
      "- Translation and TranslationVariableLanguages, the two features specific to Machine Translation.\n",
      "a Value feature specifies a single typed value, e.g. int64 or string.\n",
      "a ClassLabel feature specifies a field with a predefined set of classes which can have labels associated to them and will be stored as integers in the dataset.\n",
      "a python dict which specifies that the field is a nested field containing a mapping of sub-fields to sub-fields features. Its possible to have nested fields of nested fields in an arbitrary manner.\n",
      "a python list or a Sequence specifies that the field contains a list of objects. The python list or Sequence should be provided with a single sub-feature as an example of the feature type hosted in this list.\n",
      "A Sequence with a internal dictionary feature will be automatically converted into a dictionary of lists. This behavior is implemented to have a compatilbity layer with the TensorFlow Datasets library but may be un-wanted in some cases. If you dont want this behavior, you can use a python list instead of the Sequence.\n",
      "a Array2D, Array3D, Array4D or Array5D feature for multidimensional arrays.\n",
      "an Audio feature to store the absolute path to an audio file or a dictionary with the relative path to an audio file (path key) and its bytes content (bytes key). This feature extracts the audio data.\n",
      "an Image feature to store the absolute path to an image file, an np.ndarray object, a PIL.Image.Image object or a dictionary with the relative path to an image file (path key) and its bytes content (bytes key). This feature extracts the image data.\n",
      "Translation and TranslationVariableLanguages, the two features specific to Machine Translation.\n",
      "copy\n",
      "( )\n",
      "Make a deep copy of Features.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
      ">>> copy_of_features = ds.features.copy()\n",
      ">>> copy_of_features\n",
      "{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " 'text': Value(dtype='string', id=None)}\n",
      "```\n",
      "decode_batch\n",
      "( batch: dicttoken_per_repo_id: Optional = None )\n",
      "Parameters\n",
      "- batch (dict[str, list[Any]])  Dataset batch data.\n",
      "- token_per_repo_id (dict, optional)  To access and decode audio or image files from private repositories on the Hub, you can pass a dictionary repo_id (str) -> token (bool or str)\n",
      "Decode batch with custom feature decoding.\n",
      "decode_column\n",
      "( column: listcolumn_name: str )\n",
      "Parameters\n",
      "- column (list[Any])  Dataset column data.\n",
      "- column_name (str)  Dataset column name.\n",
      "Decode column with custom feature decoding.\n",
      "decode_example\n",
      "( example: dicttoken_per_repo_id: Optional = None )\n",
      "Parameters\n",
      "- example (dict[str, Any])  Dataset row data.\n",
      "- token_per_repo_id (dict, optional)  To access and decode audio or image files from private repositories on the Hub, you can pass a dictionary repo_id (str) -> token (bool or str).\n",
      "Decode example with custom feature decoding.\n",
      "encode_batch\n",
      "( batch )\n",
      "Parameters\n",
      "- batch (dict[str, list[Any]])  Data in a Dataset batch.\n",
      "Encode batch into a format for Arrow.\n",
      "encode_column\n",
      "( columncolumn_name: str )\n",
      "Parameters\n",
      "- column (list[Any])  Data in a Dataset column.\n",
      "- column_name (str)  Dataset column name.\n",
      "Encode column into a format for Arrow.\n",
      "encode_example\n",
      "( example )\n",
      "Parameters\n",
      "- example (dict[str, Any])  Data in a Dataset row.\n",
      "Encode example into a format for Arrow.\n",
      "flatten\n",
      "( max_depth = 16 )  Features\n",
      "Returns\n",
      "Features\n",
      "The flattened features.\n",
      "The flattened features.\n",
      "Flatten the features. Every dictionary column is removed and is replaced by all the subfields it contains. The new fields are named by concatenating the name of the original column and the subfield name like this: <original>.<subfield>.\n",
      "If a column contains nested dictionaries, then all the lower-level subfields names are also concatenated to form new columns: <original>.<subfield>.<subsubfield>, etc.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"squad\", split=\"train\")\n",
      ">>> ds.features.flatten()\n",
      "{'answers.answer_start': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
      " 'answers.text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
      " 'context': Value(dtype='string', id=None),\n",
      " 'id': Value(dtype='string', id=None),\n",
      " 'question': Value(dtype='string', id=None),\n",
      " 'title': Value(dtype='string', id=None)}\n",
      "```\n",
      "from_arrow_schema\n",
      "( pa_schema: Schema )\n",
      "Parameters\n",
      "- pa_schema (pyarrow.Schema)  Arrow Schema.\n",
      "Construct Features from Arrow Schema. It also checks the schema metadata for Hugging Face Datasets features. Non-nullable fields are not supported and set to nullable.\n",
      "from_dict\n",
      "( dic )  Features\n",
      "Parameters\n",
      "- dic (dict[str, Any])  Python dictionary.\n",
      "Returns\n",
      "Features\n",
      "\n",
      "Construct [Features] from dict.\n",
      "Regenerate the nested feature object from a deserialized dict. We use the _type key to infer the dataclass name of the feature FieldType.\n",
      "It allows for a convenient constructor syntax to define features from deserialized JSON dictionaries. This function is used in particular when deserializing a [DatasetInfo] that was dumped to a JSON object. This acts as an analogue to [Features.from_arrow_schema] and handles the recursive field-by-field instantiation, but doesnt require any mapping to/from pyarrow, except for the fact that it takes advantage of the mapping of pyarrow primitive dtypes that [Value] automatically performs.\n",
      "Example:\n",
      "```\n",
      ">>> Features.from_dict({'_type': {'dtype': 'string', 'id': None, '_type': 'Value'}})\n",
      "{'_type': Value(dtype='string', id=None)}\n",
      "```\n",
      "reorder_fields_as\n",
      "( other: Features )\n",
      "Parameters\n",
      "- other ([Features])  The other [Features] to align with.\n",
      "Reorder Features fields to match the field order of other [Features].\n",
      "The order of the fields is important since it matters for the underlying arrow data. Re-ordering the fields allows to make the underlying arrow data type match.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features, Sequence, Value\n",
      ">>> # let's say we have to features with a different order of nested fields (for a and b for example)\n",
      ">>> f1 = Features({\"root\": Sequence({\"a\": Value(\"string\"), \"b\": Value(\"string\")})})\n",
      ">>> f2 = Features({\"root\": {\"b\": Sequence(Value(\"string\")), \"a\": Sequence(Value(\"string\"))}})\n",
      ">>> assert f1.type != f2.type\n",
      ">>> # re-ordering keeps the base structure (here Sequence is defined at the root level), but make the fields order match\n",
      ">>> f1.reorder_fields_as(f2)\n",
      "{'root': Sequence(feature={'b': Value(dtype='string', id=None), 'a': Value(dtype='string', id=None)}, length=-1, id=None)}\n",
      ">>> assert f1.reorder_fields_as(f2).type == f2.type\n",
      "```\n",
      "class datasets.Sequence\n",
      "( feature: Anylength: int = -1id: Optional = None )\n",
      "Parameters\n",
      "- length (int)  Length of the sequence.\n",
      "Construct a list of feature from a single type or a dict of types. Mostly here for compatiblity with tfds.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features, Sequence, Value, ClassLabel\n",
      ">>> features = Features({'post': Sequence(feature={'text': Value(dtype='string'), 'upvotes': Value(dtype='int32'), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'])})})\n",
      ">>> features\n",
      "{'post': Sequence(feature={'text': Value(dtype='string', id=None), 'upvotes': Value(dtype='int32', id=None), 'label': ClassLabel(num_classes=2, names=['hot', 'cold'], id=None)}, length=-1, id=None)}\n",
      "```\n",
      "class datasets.ClassLabel\n",
      "( num_classes: dataclasses.InitVar[typing.Optional[int]] = Nonenames: List = Nonenames_file: dataclasses.InitVar[typing.Optional[str]] = Noneid: Optional = None )\n",
      "Parameters\n",
      "- num_classes (int, optional)  Number of classes. All labels must be < num_classes.\n",
      "- names (list of str, optional)  String names for the integer classes. The order in which the names are provided is kept.\n",
      "- names_file (str, optional)  Path to a file with names for the integer classes, one per line.\n",
      "Feature type for integer class labels.\n",
      "There are 3 ways to define a ClassLabel, which correspond to the 3 arguments:\n",
      "- num_classes: Create 0 to (num_classes-1) labels.\n",
      "- names: List of label strings.\n",
      "- names_file: File containing the list of labels.\n",
      "Under the hood the labels are stored as integers. You can use negative integers to represent unknown/missing labels.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'])})\n",
      ">>> features\n",
      "{'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'], id=None)}\n",
      "```\n",
      "cast_storage\n",
      "( storage: Union )  pa.Int64Array\n",
      "Parameters\n",
      "- storage (Union[pa.StringArray, pa.IntegerArray])  PyArrow array to cast.\n",
      "Returns\n",
      "pa.Int64Array\n",
      "Array in the ClassLabel arrow storage type.\n",
      "Array in the ClassLabel arrow storage type.\n",
      "Cast an Arrow array to the ClassLabel arrow storage type. The Arrow types that can be converted to the ClassLabel pyarrow storage type are:\n",
      "- pa.string()\n",
      "- pa.int()\n",
      "int2str\n",
      "( values: Union )\n",
      "Conversion integer => class name string.\n",
      "Regarding unknown/missing labels: passing negative integers raises ValueError.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
      ">>> ds.features[\"label\"].int2str(0)\n",
      "'neg'\n",
      "```\n",
      "str2int\n",
      "( values: Union )\n",
      "Conversion class name string => integer.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
      ">>> ds.features[\"label\"].str2int('neg')\n",
      "0\n",
      "```\n",
      "class datasets.Value\n",
      "( dtype: strid: Optional = None )\n",
      "The Value dtypes are as follows:\n",
      "- null\n",
      "- bool\n",
      "- int8\n",
      "- int16\n",
      "- int32\n",
      "- int64\n",
      "- uint8\n",
      "- uint16\n",
      "- uint32\n",
      "- uint64\n",
      "- float16\n",
      "- float32 (alias float)\n",
      "- float64 (alias double)\n",
      "- time32[(s|ms)]\n",
      "- time64[(us|ns)]\n",
      "- timestamp[(s|ms|us|ns)]\n",
      "- timestamp[(s|ms|us|ns), tz=(tzstring)]\n",
      "- date32\n",
      "- date64\n",
      "- duration[(s|ms|us|ns)]\n",
      "- decimal128(precision, scale)\n",
      "- decimal256(precision, scale)\n",
      "- binary\n",
      "- large_binary\n",
      "- string\n",
      "- large_string\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'stars': Value(dtype='int32')})\n",
      ">>> features\n",
      "{'stars': Value(dtype='int32', id=None)}\n",
      "```\n",
      "class datasets.Translation\n",
      "( languages: Listid: Optional = None )\n",
      "Parameters\n",
      "- languages (dict)  A dictionary for each example mapping string language codes to string translations.\n",
      "FeatureConnector for translations with fixed languages per example. Here for compatiblity with tfds.\n",
      "Example:\n",
      "```\n",
      ">>> # At construction time:\n",
      ">>> datasets.features.Translation(languages=['en', 'fr', 'de'])\n",
      ">>> # During data generation:\n",
      ">>> yield {\n",
      "...         'en': 'the cat',\n",
      "...         'fr': 'le chat',\n",
      "...         'de': 'die katze'\n",
      "... }\n",
      "```\n",
      "flatten\n",
      "( )\n",
      "Flatten the Translation feature into a dictionary.\n",
      "class datasets.TranslationVariableLanguages\n",
      "( languages: Optional = Nonenum_languages: Optional = Noneid: Optional = None ) \n",
      "language or translation (variable-length 1D tf.Tensor of tf.string)\n",
      "- language or translation (variable-length 1D tf.Tensor of tf.string)\n",
      "Parameters\n",
      "- languages (dict)  A dictionary for each example mapping string language codes to one or more string translations. The languages present may vary from example to example.\n",
      "Returns\n",
      "- language or translation (variable-length 1D tf.Tensor of tf.string)\n",
      "Language codes sorted in ascending order or plain text translations, sorted to align with language codes.\n",
      "Language codes sorted in ascending order or plain text translations, sorted to align with language codes.\n",
      "FeatureConnector for translations with variable languages per example. Here for compatiblity with tfds.\n",
      "Example:\n",
      "```\n",
      ">>> # At construction time:\n",
      ">>> datasets.features.TranslationVariableLanguages(languages=['en', 'fr', 'de'])\n",
      ">>> # During data generation:\n",
      ">>> yield {\n",
      "...         'en': 'the cat',\n",
      "...         'fr': ['le chat', 'la chatte,']\n",
      "...         'de': 'die katze'\n",
      "... }\n",
      ">>> # Tensor returned :\n",
      ">>> {\n",
      "...         'language': ['en', 'de', 'fr', 'fr'],\n",
      "...         'translation': ['the cat', 'die katze', 'la chatte', 'le chat'],\n",
      "... }\n",
      "```\n",
      "flatten\n",
      "( )\n",
      "Flatten the TranslationVariableLanguages feature into a dictionary.\n",
      "class datasets.Array2D\n",
      "( shape: tupledtype: strid: Optional = None )\n",
      "Parameters\n",
      "- shape (tuple)  The size of each dimension.\n",
      "- dtype (str)  The value of the data type.\n",
      "Create a two-dimensional array.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'x': Array2D(shape=(1, 3), dtype='int32')})\n",
      "```\n",
      "class datasets.Array3D\n",
      "( shape: tupledtype: strid: Optional = None )\n",
      "Parameters\n",
      "- shape (tuple)  The size of each dimension.\n",
      "- dtype (str)  The value of the data type.\n",
      "Create a three-dimensional array.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'x': Array3D(shape=(1, 2, 3), dtype='int32')})\n",
      "```\n",
      "class datasets.Array4D\n",
      "( shape: tupledtype: strid: Optional = None )\n",
      "Parameters\n",
      "- shape (tuple)  The size of each dimension.\n",
      "- dtype (str)  The value of the data type.\n",
      "Create a four-dimensional array.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'x': Array4D(shape=(1, 2, 2, 3), dtype='int32')})\n",
      "```\n",
      "class datasets.Array5D\n",
      "( shape: tupledtype: strid: Optional = None )\n",
      "Parameters\n",
      "- shape (tuple)  The size of each dimension.\n",
      "- dtype (str)  The value of the data type.\n",
      "Create a five-dimensional array.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import Features\n",
      ">>> features = Features({'x': Array5D(shape=(1, 2, 2, 3, 3), dtype='int32')})\n",
      "```\n",
      "class datasets.Audio\n",
      "( sampling_rate: Optional = Nonemono: bool = Truedecode: bool = Trueid: Optional = None )\n",
      "Parameters\n",
      "- sampling_rate (int, optional)  Target sampling rate. If None, the native sampling rate is used.\n",
      "- mono (bool, defaults to True)  Whether to convert the audio signal to mono by averaging samples across channels.\n",
      "- decode (bool, defaults to True)  Whether to decode the audio data. If False, returns the underlying dictionary in the format {\"path\": audio_path, \"bytes\": audio_bytes}.\n",
      "Audio Feature to extract audio data from an audio file.\n",
      "Input: The Audio feature accepts as input:\n",
      "- A str: Absolute path to the audio file (i.e. random access is allowed).\n",
      "- A dict with the keys:\n",
      "path: String with relative path of the audio file to the archive file.\n",
      "bytes: Bytes content of the audio file.\n",
      "This is useful for archived files with sequential access.\n",
      "- path: String with relative path of the audio file to the archive file.\n",
      "- bytes: Bytes content of the audio file.\n",
      "- A dict with the keys:\n",
      "path: String with relative path of the audio file to the archive file.\n",
      "array: Array containing the audio sample\n",
      "sampling_rate: Integer corresponding to the sampling rate of the audio sample.\n",
      "This is useful for archived files with sequential access.\n",
      "- path: String with relative path of the audio file to the archive file.\n",
      "- array: Array containing the audio sample\n",
      "- sampling_rate: Integer corresponding to the sampling rate of the audio sample.\n",
      "A str: Absolute path to the audio file (i.e. random access is allowed).\n",
      "A dict with the keys:\n",
      "- path: String with relative path of the audio file to the archive file.\n",
      "- bytes: Bytes content of the audio file.\n",
      "This is useful for archived files with sequential access.\n",
      "A dict with the keys:\n",
      "- path: String with relative path of the audio file to the archive file.\n",
      "- array: Array containing the audio sample\n",
      "- sampling_rate: Integer corresponding to the sampling rate of the audio sample.\n",
      "This is useful for archived files with sequential access.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_dataset, Audio\n",
      ">>> ds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n",
      ">>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
      ">>> ds[0][\"audio\"]\n",
      "{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n",
      "     3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n",
      " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n",
      " 'sampling_rate': 16000}\n",
      "```\n",
      "cast_storage\n",
      "( storage: Union )  pa.StructArray\n",
      "Parameters\n",
      "- storage (Union[pa.StringArray, pa.StructArray])  PyArrow array to cast.\n",
      "Returns\n",
      "pa.StructArray\n",
      "Array in the Audio arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()})\n",
      "Array in the Audio arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()})\n",
      "Cast an Arrow array to the Audio arrow storage type. The Arrow types that can be converted to the Audio pyarrow storage type are:\n",
      "- pa.string() - it must contain the path data\n",
      "- pa.binary() - it must contain the audio bytes\n",
      "- pa.struct({\"bytes\": pa.binary()})\n",
      "- pa.struct({\"path\": pa.string()})\n",
      "- pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}) - order doesnt matter\n",
      "decode_example\n",
      "( value: dicttoken_per_repo_id: Optional = None )  dict\n",
      "Parameters\n",
      "- value (dict)  A dictionary with keys:\n",
      "path: String with relative audio file path.\n",
      "bytes: Bytes of the audio file.\n",
      "- path: String with relative audio file path.\n",
      "- bytes: Bytes of the audio file.\n",
      "- token_per_repo_id (dict, optional)  To access and decode audio files from private repositories on the Hub, you can pass a dictionary repo_id (str) -> token (bool or str)\n",
      "\n",
      "- path: String with relative audio file path.\n",
      "- bytes: Bytes of the audio file.\n",
      "Returns\n",
      "dict\n",
      "\n",
      "Decode example audio file into audio data.\n",
      "embed_storage\n",
      "( storage: StructArray )  pa.StructArray\n",
      "Parameters\n",
      "- storage (pa.StructArray)  PyArrow array to embed.\n",
      "Returns\n",
      "pa.StructArray\n",
      "Array in the Audio arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Array in the Audio arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Embed audio files into the Arrow array.\n",
      "encode_example\n",
      "( value: Union )  dict\n",
      "Parameters\n",
      "- value (str or dict)  Data passed as input to Audio feature.\n",
      "Returns\n",
      "dict\n",
      "\n",
      "Encode example into a format for Arrow.\n",
      "flatten\n",
      "( )\n",
      "If in the decodable state, raise an error, otherwise flatten the feature into a dictionary.\n",
      "class datasets.Image\n",
      "( decode: bool = Trueid: Optional = None )\n",
      "Parameters\n",
      "- decode (bool, defaults to True)  Whether to decode the image data. If False, returns the underlying dictionary in the format {\"path\": image_path, \"bytes\": image_bytes}.\n",
      "Image Feature to read image data from an image file.\n",
      "Input: The Image feature accepts as input:\n",
      "- A str: Absolute path to the image file (i.e. random access is allowed).\n",
      "- A dict with the keys:\n",
      "path: String with relative path of the image file to the archive file.\n",
      "bytes: Bytes of the image file.\n",
      "This is useful for archived files with sequential access.\n",
      "- path: String with relative path of the image file to the archive file.\n",
      "- bytes: Bytes of the image file.\n",
      "- An np.ndarray: NumPy array representing an image.\n",
      "- A PIL.Image.Image: PIL image object.\n",
      "A str: Absolute path to the image file (i.e. random access is allowed).\n",
      "A dict with the keys:\n",
      "- path: String with relative path of the image file to the archive file.\n",
      "- bytes: Bytes of the image file.\n",
      "This is useful for archived files with sequential access.\n",
      "An np.ndarray: NumPy array representing an image.\n",
      "A PIL.Image.Image: PIL image object.\n",
      "Examples:\n",
      "```\n",
      ">>> from datasets import load_dataset, Image\n",
      ">>> ds = load_dataset(\"beans\", split=\"train\")\n",
      ">>> ds.features[\"image\"]\n",
      "Image(decode=True, id=None)\n",
      ">>> ds[0][\"image\"]\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500 at 0x15E52E7F0>\n",
      ">>> ds = ds.cast_column('image', Image(decode=False))\n",
      "{'bytes': None,\n",
      " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/b0a21163f78769a2cf11f58dfc767fb458fc7cea5c05dccc0144a2c0f0bc1292/train/healthy/healthy_train.85.jpg'}\n",
      "```\n",
      "cast_storage\n",
      "( storage: Union )  pa.StructArray\n",
      "Parameters\n",
      "- storage (Union[pa.StringArray, pa.StructArray, pa.ListArray])  PyArrow array to cast.\n",
      "Returns\n",
      "pa.StructArray\n",
      "Array in the Image arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Array in the Image arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Cast an Arrow array to the Image arrow storage type. The Arrow types that can be converted to the Image pyarrow storage type are:\n",
      "- pa.string() - it must contain the path data\n",
      "- pa.binary() - it must contain the image bytes\n",
      "- pa.struct({\"bytes\": pa.binary()})\n",
      "- pa.struct({\"path\": pa.string()})\n",
      "- pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}) - order doesnt matter\n",
      "- pa.list(*) - it must contain the image array data\n",
      "decode_example\n",
      "( value: dicttoken_per_repo_id = None )\n",
      "Parameters\n",
      "- value (str or dict)  A string with the absolute image file path, a dictionary with keys:\n",
      "path: String with absolute or relative image file path.\n",
      "bytes: The bytes of the image file.\n",
      "- path: String with absolute or relative image file path.\n",
      "- bytes: The bytes of the image file.\n",
      "- token_per_repo_id (dict, optional)  To access and decode image files from private repositories on the Hub, you can pass a dictionary repo_id (str) -> token (bool or str).\n",
      "\n",
      "- path: String with absolute or relative image file path.\n",
      "- bytes: The bytes of the image file.\n",
      "Decode example image file into image data.\n",
      "embed_storage\n",
      "( storage: StructArray )  pa.StructArray\n",
      "Parameters\n",
      "- storage (pa.StructArray)  PyArrow array to embed.\n",
      "Returns\n",
      "pa.StructArray\n",
      "Array in the Image arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Array in the Image arrow storage type, that is pa.struct({\"bytes\": pa.binary(), \"path\": pa.string()}).\n",
      "Embed image files into the Arrow array.\n",
      "encode_example\n",
      "( value: Union )\n",
      "Parameters\n",
      "- value (str, np.ndarray, PIL.Image.Image or dict)  Data passed as input to Image feature.\n",
      "Encode example into a format for Arrow.\n",
      "flatten\n",
      "( )\n",
      "If in the decodable state, return the feature itself, otherwise flatten the feature into a dictionary.\n",
      "MetricInfo\n",
      "class datasets.MetricInfo\n",
      "( description: strcitation: strfeatures: Featuresinputs_description: str = <factory>homepage: str = <factory>license: str = <factory>codebase_urls: List = <factory>reference_urls: List = <factory>streamable: bool = Falseformat: Optional = Nonemetric_name: Optional = Noneconfig_name: Optional = Noneexperiment_id: Optional = None )\n",
      "Information about a metric.\n",
      "MetricInfo documents a metric, including its name, version, and features. See the constructor arguments and properties for a full list.\n",
      "Note: Not all fields are known on construction and may be updated later.\n",
      "from_directory\n",
      "( metric_info_dir )\n",
      "Create MetricInfo from the JSON file in metric_info_dir.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import MetricInfo\n",
      ">>> metric_info = MetricInfo.from_directory(\"/path/to/directory/\")\n",
      "```\n",
      "write_to_directory\n",
      "( metric_info_dirpretty_print = False )\n",
      "Write MetricInfo as JSON to metric_info_dir. Also save the license separately in LICENCE. If pretty_print is True, the JSON will be pretty-printed with the indent level of 4.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_metric\n",
      ">>> metric = load_metric(\"accuracy\")\n",
      ">>> metric.info.write_to_directory(\"/path/to/directory/\")\n",
      "```\n",
      "Metric\n",
      "The base class Metric implements a Metric backed by one or several Dataset.\n",
      "class datasets.Metric\n",
      "( config_name: Optional = Nonekeep_in_memory: bool = Falsecache_dir: Optional = Nonenum_process: int = 1process_id: int = 0seed: Optional = Noneexperiment_id: Optional = Nonemax_concurrent_cache_files: int = 10000timeout: Union = 100**kwargs )\n",
      "Parameters\n",
      "- config_name (str)  This is used to define a hash specific to a metrics computation script and prevents the metrics data to be overridden when the metric loading script is modified.\n",
      "- keep_in_memory (bool)  keep all predictions and references in memory. Not possible in distributed settings.\n",
      "- cache_dir (str)  Path to a directory in which temporary prediction/references data will be stored. The data directory should be located on a shared file-system in distributed setups.\n",
      "- num_process (int)  specify the total number of nodes in a distributed settings. This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "- process_id (int)  specify the id of the current process in a distributed setup (between 0 and num_process-1) This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "- seed (int, optional)  If specified, this will temporarily set numpys random seed when datasets.Metric.compute() is run.\n",
      "- experiment_id (str)  A specific experiment id. This is used if several distributed evaluations share the same file system. This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).\n",
      "- max_concurrent_cache_files (int)  Max number of concurrent metrics cache files (default 10000).\n",
      "- timeout (Union[int, float])  Timeout in second for distributed setting synchronization.\n",
      "A Metric is the base class and common API for all metrics.\n",
      "Deprecated in 2.5.0\n",
      "Use the new library  Evaluate instead: https://huggingface.co/docs/evaluate\n",
      "add\n",
      "( prediction = Nonereference = None**kwargs )\n",
      "Parameters\n",
      "- prediction (list/array/tensor, optional)  Predictions.\n",
      "- reference (list/array/tensor, optional)  References.\n",
      "Add one prediction and reference for the metrics stack.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_metric\n",
      ">>> metric = load_metric(\"accuracy\")\n",
      ">>> metric.add(predictions=model_predictions, references=labels)\n",
      "```\n",
      "add_batch\n",
      "( predictions = Nonereferences = None**kwargs )\n",
      "Parameters\n",
      "- predictions (list/array/tensor, optional)  Predictions.\n",
      "- references (list/array/tensor, optional)  References.\n",
      "Add a batch of predictions and references for the metrics stack.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_metric\n",
      ">>> metric = load_metric(\"accuracy\")\n",
      ">>> metric.add_batch(predictions=model_prediction, references=labels)\n",
      "```\n",
      "compute\n",
      "( predictions = Nonereferences = None**kwargs )\n",
      "Parameters\n",
      "- predictions (list/array/tensor, optional)  Predictions.\n",
      "- references (list/array/tensor, optional)  References.\n",
      "- **kwargs (optional)  Keyword arguments that will be forwarded to the metrics _compute method (see details in the docstring).\n",
      "Compute the metrics.\n",
      "Usage of positional arguments is not allowed to prevent mistakes.\n",
      "Example:\n",
      "```\n",
      ">>> from datasets import load_metric\n",
      ">>> metric = load_metric(\"accuracy\")\n",
      ">>> accuracy = metric.compute(predictions=model_prediction, references=labels)\n",
      "```\n",
      "download_and_prepare\n",
      "( download_config: Optional = Nonedl_manager: Optional = None )\n",
      "Parameters\n",
      "- download_config (DownloadConfig, optional)  Specific download configuration parameters.\n",
      "- dl_manager (DownloadManager, optional)  Specific download manager to use.\n",
      "Downloads and prepares dataset for reading.\n",
      "Filesystems\n",
      "class datasets.filesystems.S3FileSystem\n",
      "( *args**kwargs )\n",
      "Parameters\n",
      "- anon (bool, default to False)  Whether to use anonymous connection (public buckets only). If False, uses the key/secret given, or botos credential resolver (client_kwargs, environment, variables, config files, EC2 IAM server, in that order).\n",
      "- key (str)  If not anonymous, use this access key ID, if specified.\n",
      "- secret (str)  If not anonymous, use this secret access key, if specified.\n",
      "- token (str)  If not anonymous, use this security token, if specified.\n",
      "- use_ssl (bool, defaults to True)  Whether to use SSL in connections to S3; may be faster without, but insecure. If use_ssl is also set in client_kwargs, the value set in client_kwargs will take priority.\n",
      "- s3_additional_kwargs (dict)  Parameters that are used when calling S3 API methods. Typically used for things like ServerSideEncryption.\n",
      "- client_kwargs (dict)  Parameters for the botocore client.\n",
      "- requester_pays (bool, defaults to False)  Whether RequesterPays buckets are supported.\n",
      "- default_block_size (int)  If given, the default block size value used for open(), if no specific value is given at all time. The built-in default is 5MB.\n",
      "- default_fill_cache (bool, defaults to True)  Whether to use cache filling with open by default. Refer to S3File.open.\n",
      "- default_cache_type (str, defaults to bytes)  If given, the default cache_type value used for open(). Set to none if no caching is desired. See fsspecs documentation for other available cache_type values.\n",
      "- version_aware (bool, defaults to False)  Whether to support bucket versioning. If enable this will require the user to have the necessary IAM permissions for dealing with versioned objects.\n",
      "- cache_regions (bool, defaults to False)  Whether to cache bucket regions. Whenever a new bucket is used, it will first find out which region it belongs to and then use the client for that region.\n",
      "- asynchronous (bool, defaults to False)  Whether this instance is to be used from inside coroutines.\n",
      "- config_kwargs (dict)  Parameters passed to botocore.client.Config. **kwargs  Other parameters for core session.\n",
      "- session (aiobotocore.session.AioSession)  Session to be used for all connections. This session will be used inplace of creating a new session inside S3FileSystem. For example: aiobotocore.session.AioSession(profile='test_user').\n",
      "- skip_instance_cache (bool)  Control reuse of instances. Passed on to fsspec.\n",
      "- use_listings_cache (bool)  Control reuse of directory listings. Passed on to fsspec.\n",
      "- listings_expiry_time (int or float)  Control reuse of directory listings. Passed on to fsspec.\n",
      "- max_paths (int)  Control reuse of directory listings. Passed on to fsspec.\n",
      "datasets.filesystems.S3FileSystem is a subclass of s3fs.S3FileSystem.\n",
      "Users can use this class to access S3 as if it were a file system. It exposes a filesystem-like API (ls, cp, open, etc.) on top of S3 storage. Provide credentials either explicitly (key=, secret=) or with botos credential methods. See botocore documentation for more information. If no credentials are available, use anon=True.\n",
      "Examples:\n",
      "Listing files from public S3 bucket.\n",
      "```\n",
      ">>> import datasets\n",
      ">>> s3 = datasets.filesystems.S3FileSystem(anon=True)\n",
      ">>> s3.ls('public-datasets/imdb/train')\n",
      "['dataset_info.json.json','dataset.arrow','state.json']\n",
      "```\n",
      "Listing files from private S3 bucket using aws_access_key_id and aws_secret_access_key.\n",
      "```\n",
      ">>> import datasets\n",
      ">>> s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
      ">>> s3.ls('my-private-datasets/imdb/train')\n",
      "['dataset_info.json.json','dataset.arrow','state.json']\n",
      "```\n",
      "Using S3Filesystem with botocore.session.Session and custom aws_profile.\n",
      "```\n",
      ">>> import botocore\n",
      ">>> from datasets.filesystems import S3Filesystem\n",
      "\n",
      ">>> s3_session = botocore.session.Session(profile_name='my_profile_name')\n",
      ">>> s3 = S3FileSystem(session=s3_session)\n",
      "```\n",
      "Loading dataset from S3 using S3Filesystem and load_from_disk().\n",
      "```\n",
      ">>> from datasets import load_from_disk\n",
      ">>> from datasets.filesystems import S3Filesystem\n",
      "\n",
      ">>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
      ">>> dataset = load_from_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)\n",
      ">>> print(len(dataset))\n",
      "25000\n",
      "```\n",
      "Saving dataset to S3 using S3Filesystem and Dataset.save_to_disk().\n",
      "```\n",
      ">>> from datasets import load_dataset\n",
      ">>> from datasets.filesystems import S3Filesystem\n",
      "\n",
      ">>> dataset = load_dataset(\"imdb\")\n",
      ">>> s3 = S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)\n",
      ">>> dataset.save_to_disk('s3://my-private-datasets/imdb/train', storage_options=s3.storage_options)\n",
      "```\n",
      "datasets.filesystems.extract_path_from_uri\n",
      "( dataset_path: str )\n",
      "Parameters\n",
      "- dataset_path (str)  Path (e.g. dataset/train) or remote uri (e.g. s3://my-bucket/dataset/train) of the dataset directory.\n",
      "Preprocesses dataset_path and removes remote filesystem (e.g. removing s3://).\n",
      "datasets.filesystems.is_remote_filesystem\n",
      "( fs: AbstractFileSystem )\n",
      "Parameters\n",
      "- fs (fsspec.spec.AbstractFileSystem)  An abstract super-class for pythonic file-systems, e.g. fsspec.filesystem('file') or datasets.filesystems.S3FileSystem.\n",
      "Checks if fs is a remote filesystem.\n",
      "Fingerprint\n",
      "class datasets.fingerprint.Hasher\n",
      "( )\n",
      "Hasher that accepts python objects as inputs.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import InferenceClient\n",
    "import gradio as gr\n",
    "import json\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.get(\"http://huggingface.co/docs\")\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Extended regular expression to find URLs within various contexts including angle brackets\n",
    "    url_pattern = r'https?://[\\w\\-._~:/?#\\[\\]@!$&\\'()*+,;=%]+'\n",
    "    # Extract URLs that might be within angle brackets\n",
    "    bracketed_urls = re.findall(r'<(' + url_pattern + ')>', text)\n",
    "    # Extract normal URLs not in brackets\n",
    "    normal_urls = re.findall(url_pattern, text)\n",
    "    # Combine both lists, avoiding duplicates\n",
    "    all_urls = list(set(bracketed_urls + normal_urls))\n",
    "    return all_urls\n",
    "\n",
    "with open('hf.json', 'r') as f:\n",
    "    links = json.load(f)\n",
    "\n",
    "for i in range(len(links)):\n",
    "    links[i] = links[i][:-1]\n",
    "    \n",
    "client = InferenceClient(\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    token='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'\n",
    "    # \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    ")\n",
    "\n",
    "generate_kwargs = dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=1000,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True\n",
    "        # seed=42,\n",
    "    )\n",
    "\n",
    "formatted_prompt = f'''<s>[INST] This is the list of available documentations {str(links)}\\n\\nTell me the url or urls of the most suitable documentation for the given task.\\nTask: Tell me how to create a huggingface dataset and use mapping on it. [/INST]'''\n",
    "\n",
    "response = client.text_generation(formatted_prompt, **generate_kwargs, details=True, return_full_text=False)['generated_text']\n",
    "\n",
    "urls = extract_urls(response)\n",
    "print(urls)\n",
    "\n",
    "docs = ''\n",
    "for url in urls:\n",
    "    text= ''\n",
    "    tag = None\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        x = driver.find_element(By.CLASS_NAME, 'prose-doc')\n",
    "    except:\n",
    "        continue\n",
    "    driver.implicitly_wait(0.4)\n",
    "    try:\n",
    "        if driver.find_elements(By.CSS_SELECTOR, '.absolute.leading-tight.bg-black.text-gray-200.rounded-xl.bottom-12.ring-offset-2'):\n",
    "            buttons = driver.find_elements(By.CSS_SELECTOR, '.absolute.leading-tight.px-3.bg-black.text-gray-200.rounded-xl.bottom-12.ring-offset-2')\n",
    "            for button in buttons:\n",
    "                button.click()\n",
    "    except:\n",
    "        pass\n",
    "    driver.implicitly_wait(0.4)\n",
    "    elements = driver.find_element(By.CLASS_NAME, 'prose-doc').find_elements(By.CSS_SELECTOR, \"p, h1, h2, h3, h4, h5, h6, ul, pre\")\n",
    "    for element in elements: \n",
    "        if element.tag_name == 'ul':\n",
    "            list_items = element.find_elements(By.TAG_NAME, \"li\")\n",
    "            for item in list_items:\n",
    "                text += '- ' + item.text + '\\n'\n",
    "        elif element.tag_name =='pre':\n",
    "            t = element\n",
    "            text = text + f'```\\n{element.text}\\n```\\n'\n",
    "        else:\n",
    "            text += element.text + '\\n'\n",
    "    # print(text)\n",
    "    docs = docs + text + '\\n\\n'\n",
    "\n",
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
