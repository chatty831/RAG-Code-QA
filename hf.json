[
    [
        "https://huggingface.co/docs/hub/index",
        "\ud83e\udd17 Hugging Face Hub",
        "The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. Are you ready to join the path towards open source Machine Learning? \ud83e\udd17"
    ],
    [
        "https://huggingface.co/docs/hub/repositories",
        "Repositories",
        "Models, Spaces, and Datasets are hosted on the Hugging Face Hub as Git repositories, which means that version control and collaboration are core elements of the Hub. In a nutshell, a repository (also known as a repo) is a place where code and assets can be stored to back up your work, share it with the community, and work in a team."
    ],
    [
        "https://huggingface.co/docs/hub/repositories-getting-started",
        "Getting Started with Repositories",
        "This beginner-friendly guide will help you get the basic skills you need to create and manage your repository on the Hub. Each section builds on the previous one, so feel free to choose where to start!"
    ],
    [
        "https://huggingface.co/docs/hub/repositories-settings",
        "Repository Settings",
        "You can choose a repository\u2019s visibility when you create it, and any repository that you own can have its visibility toggled between public and private in the Settings tab. Unless your repository is owned by an organization, you are the only user that can make changes to your repo or upload any code. Setting your visibility to private will:"
    ],
    [
        "https://huggingface.co/docs/hub/repositories-pull-requests-discussions",
        "Pull Requests & Discussions",
        "Hub Pull requests and Discussions allow users to do community contributions to repositories. Pull requests and discussions work the same for all the repo types."
    ],
    [
        "https://huggingface.co/docs/hub/notifications",
        "Notifications",
        "Notifications allow you to know when new activities (Pull Requests or discussions) happen on models, datasets, and Spaces belonging to users or organizations you are watching."
    ],
    [
        "https://huggingface.co/docs/hub/collections",
        "Collections",
        "Use Collections to group repositories from the Hub (Models, Datasets, Spaces and Papers) on a dedicated page."
    ],
    [
        "https://huggingface.co/docs/hub/webhooks",
        "Webhooks",
        "Webhooks are now publicly available!"
    ],
    [
        "https://huggingface.co/docs/hub/repositories-recommendations",
        "Repository size recommendations",
        "There are some limitations to be aware of when dealing with a large amount of data in your repo. Given the time it takes to stream the data, getting an upload/push to fail at the end of the process or encountering a degraded experience, be it on hf.co or when working locally, can be very annoying."
    ],
    [
        "https://huggingface.co/docs/hub/repositories-next-steps",
        "Next Steps",
        "These next sections highlight features and additional information that you may find useful to make the most out of the Git repositories on the Hugging Face Hub."
    ],
    [
        "https://huggingface.co/docs/hub/repositories-licenses",
        "Licenses",
        "You are able to add a license to any repo that you create on the Hugging Face Hub to let other users know about the permissions that you want to attribute to your code or data. The license can be specified in your repository\u2019s README.md file, known as a card on the Hub, in the card\u2019s metadata section. Remember to seek out and respect a project\u2019s license if you\u2019re considering using their code or data."
    ],
    [
        "https://huggingface.co/docs/hub/models",
        "Models",
        "The Hugging Face Hub hosts many models for a variety of machine learning tasks. Models are stored in repositories, so they benefit from all the features possessed by every repo on the Hugging Face Hub. Additionally, model repos have attributes that make exploring and using models as easy as possible. These docs will take you through everything you\u2019ll need to know to find models on the Hub, upload your models, and make the most of everything the Model Hub offers!"
    ],
    [
        "https://huggingface.co/docs/hub/models-the-hub",
        "The Model Hub",
        "The Model Hub is where the members of the Hugging Face community can host all of their model checkpoints for simple storage, discovery, and sharing. Download pre-trained models with the huggingface_hub client library, with \ud83e\udd17 Transformers for fine-tuning and other usages or with any of the over 15 integrated libraries. You can even leverage the Inference API to use models in production settings."
    ],
    [
        "https://huggingface.co/docs/hub/model-cards",
        "Model Cards",
        "New! Try our experimental Model Card Creator App"
    ],
    [
        "https://huggingface.co/docs/hub/models-gated",
        "Gated Models",
        "To give more control over how models are used, the Hub allows model authors to enable access requests for their models. Users must agree to share their contact information (username and email address) with the model authors to access the model files when enabled. Model authors can configure this request with additional fields. A model with access requests enabled is called a gated model. Access requests are always granted to individual users rather than to entire organizations. A common use case of gated models is to provide access to early research models before the wider release."
    ],
    [
        "https://huggingface.co/docs/hub/models-uploading",
        "Uploading Models",
        "To upload models to the Hub, you\u2019ll need to create an account at Hugging Face. Models on the Hub are Git-based repositories, which give you versioning, branches, discoverability and sharing features, integration with dozens of libraries, and more! You have control over what you want to upload to your repository, which could include checkpoints, configs, and any other files."
    ],
    [
        "https://huggingface.co/docs/hub/models-downloading",
        "Downloading Models",
        "If a model on the Hub is tied to a supported library, loading the model can be done in just a few lines. For information on accessing the model, you can click on the \u201cUse in Library\u201d button on the model page to see how to do so. For example, distilbert/distilgpt2 shows how to do so with \ud83e\udd17 Transformers below."
    ],
    [
        "https://huggingface.co/docs/hub/models-libraries",
        "Integrated Libraries",
        "The Hub has support for dozens of libraries in the Open Source ecosystem. Thanks to the huggingface_hub Python library, it\u2019s easy to enable sharing your models on the Hub. The Hub supports many libraries, and we\u2019re working on expanding this support. We\u2019re happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward."
    ],
    [
        "https://huggingface.co/docs/hub/models-widgets",
        "Model Widgets",
        "Many model repos have a widget that allows anyone to run inferences directly in the browser!"
    ],
    [
        "https://huggingface.co/docs/hub/models-inference",
        "Inference API docs",
        "Please refer to Inference API Documentation for detailed information."
    ],
    [
        "https://huggingface.co/docs/hub/models-download-stats",
        "Models Download Stats",
        "Counting the number of downloads for models is not a trivial task, as a single model repository might contain multiple files, including multiple model weight files (e.g., with sharded models) and different formats depending on the library (GGUF, PyTorch, TensorFlow, etc.). To avoid double counting downloads (e.g., counting a single download of a model as multiple downloads), the Hub uses a set of query files that are employed for download counting. No information is sent from the user, and no additional calls are made for this. The count is done server-side as the Hub serves files for downloads."
    ],
    [
        "https://huggingface.co/docs/hub/models-faq",
        "Frequently Asked Questions",
        "It\u2019s up to the person who uploaded the model to include the training information! A user can specify the dataset used for training a model. If the datasets used for the model are on the Hub, the uploader may have included them in the model card\u2019s metadata. In that case, the datasets would be linked with a handy card on the right side of the model page:"
    ],
    [
        "https://huggingface.co/docs/hub/models-advanced",
        "Advanced Topics",
        ""
    ],
    [
        "https://huggingface.co/docs/hub/datasets",
        "Datasets",
        "The Hugging Face Hub is home to a growing collection of datasets that span a variety of domains and tasks. These docs will guide you through interacting with the datasets on the Hub, uploading new datasets, exploring the datasets contents, and using datasets in your projects."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-overview",
        "Datasets Overview",
        "The Hugging Face Hub hosts a large number of community-curated datasets for a diverse range of tasks such as translation, automatic speech recognition, and image classification. Alongside the information contained in the dataset card, many datasets, such as GLUE, include a Dataset Viewer to showcase the data."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-cards",
        "Dataset Cards",
        "Each dataset may be documented by the README.md file in the repository. This file is called a dataset card, and the Hugging Face Hub will render its contents on the dataset\u2019s main page. To inform users about how to responsibly use the data, it\u2019s a good idea to include information about any potential biases within the dataset. Generally, dataset cards help users understand the contents of the dataset and give context for how the dataset should be used."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-gated",
        "Gated Datasets",
        "To give more control over how datasets are used, the Hub allows datasets authors to enable access requests for their datasets. Users must agree to share their contact information (username and email address) with the datasets authors to access the datasets files when enabled. Datasets authors can configure this request with additional fields. A dataset with access requests enabled is called a gated dataset. Access requests are always granted to individual users rather than to entire organizations. A common use case of gated datasets is to provide access to early research datasets before the wider release."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-adding",
        "Uploading Datasets",
        "The Hub is home to an extensive collection of community-curated and research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!"
    ],
    [
        "https://huggingface.co/docs/hub/datasets-downloading",
        "Downloading Datasets",
        "If a dataset on the Hub is tied to a supported library, loading the dataset can be done in just a few lines. For information on accessing the dataset, you can click on the \u201cUse in dataset library\u201d button on the dataset page to see how to do so. For example, samsum shows how to do so with \ud83e\udd17 Datasets below."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-libraries",
        "Integrated Libraries",
        "The Datasets Hub has support for several libraries in the Open Source ecosystem. Thanks to the huggingface_hub Python library, it\u2019s easy to enable sharing your datasets on the Hub. We\u2019re happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-viewer",
        "Dataset Viewer",
        "The dataset page includes a table with the contents of the dataset, arranged by pages of 100 rows. You can navigate between pages using the buttons at the bottom of the table."
    ],
    [
        "https://huggingface.co/docs/hub/datasets-download-stats",
        "Datasets Download Stats",
        "The Hub provides download stats for all datasets loadable via the datasets library. To determine the number of downloads, the Hub counts every time load_dataset is called in Python, excluding Hugging Face\u2019s CI tooling on GitHub. No information is sent from the user, and no additional calls are made for this. The count is done server-side as we serve files for downloads. This means that:"
    ],
    [
        "https://huggingface.co/docs/hub/datasets-data-files-configuration",
        "Data files Configuration",
        "There are no constraints on how to structure dataset repositories."
    ],
    [
        "https://huggingface.co/docs/hub/spaces",
        "Spaces",
        "Hugging Face Spaces offer a simple way to host ML demo apps directly on your profile or your organization\u2019s profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-overview",
        "Spaces Overview",
        "Hugging Face Spaces make it easy for you to create and deploy ML-powered demos in minutes. Watch the following video for a quick introduction to Spaces:"
    ],
    [
        "https://huggingface.co/docs/hub/spaces-gpus",
        "Spaces GPU Upgrades",
        "You can upgrade your Space to use a GPU accelerator using the Settings button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!"
    ],
    [
        "https://huggingface.co/docs/hub/spaces-storage",
        "Spaces Persistent Storage",
        "Every Space comes with a small amount of disk storage. This disk space is ephemeral, meaning its content will be lost if your Space restarts or is stopped. If you need to persist data with a longer lifetime than the Space itself, you can:"
    ],
    [
        "https://huggingface.co/docs/hub/spaces-sdks-gradio",
        "Gradio Spaces",
        "Gradio provides an easy and intuitive interface for running a model from a list of inputs and displaying the outputs in formats such as images, audio, 3D objects, and more. Gradio now even has a Plot output component for creating data visualizations with Matplotlib, Bokeh, and Plotly! For more details, take a look at the Getting started guide from the Gradio team."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-sdks-streamlit",
        "Streamlit Spaces",
        "Streamlit gives users freedom to build a full-featured web app with Python in a reactive way. Your code is rerun each time the state of the app changes. Streamlit is also great for data visualization and supports several charting libraries such as Bokeh, Plotly, and Altair. Read this blog post about building and hosting Streamlit apps in Spaces."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-sdks-static",
        "Static HTML Spaces",
        "Spaces also accommodate custom HTML for your app instead of using Streamlit or Gradio. Set sdk: static inside the YAML block at the top of your Spaces README.md file. Then you can place your HTML code within an index.html file."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-sdks-docker",
        "Docker Spaces",
        "Spaces accommodate custom Docker containers for apps outside the scope of Streamlit and Gradio. Docker Spaces allow users to go beyond the limits of what was previously possible with the standard SDKs. From FastAPI and Go endpoints to Phoenix apps and ML Ops tools, Docker Spaces can help in many different setups."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-embed",
        "Embed your Space",
        "Once your Space is up and running you might wish to embed it in a website or in your blog. Embedding or sharing your Space is a great way to allow your audience to interact with your work and demonstrations without requiring any setup on their side. To embed a Space its visibility needs to be public."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-run-with-docker",
        "Run Spaces with Docker",
        "You can use Docker to run most Spaces locally. To view instructions to download and run Spaces\u2019 Docker images, click on the \u201cRun with Docker\u201d button on the top-right corner of your Space page:"
    ],
    [
        "https://huggingface.co/docs/hub/spaces-config-reference",
        "Spaces Configuration Reference",
        "Spaces are configured through the YAML block at the top of the README.md file at the root of the repository. All the accepted parameters are listed below."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-oauth",
        "Sign-In with HF button",
        "You can enable a built-in sign-in flow in your Space by seamlessly creating and associating an OAuth/OpenID connect app so users can log in with their HF account."
    ],
    [
        "https://huggingface.co/docs/hub/spaces-changelog",
        "Spaces Changelog",
        ""
    ],
    [
        "https://huggingface.co/docs/hub/spaces-advanced",
        "Advanced Topics",
        ""
    ],
    [
        "https://huggingface.co/docs/hub/other",
        "Other",
        ""
    ],
    [
        "https://huggingface.co/docs/hub/organizations",
        "Organizations",
        "The Hugging Face Hub offers Organizations, which can be used to group accounts and manage datasets, models, and Spaces. The Hub also allows admins to set user roles to control access to repositories and manage their organization\u2019s payment method and billing info."
    ],
    [
        "https://huggingface.co/docs/hub/billing",
        "Billing",
        "At Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), and we monetize by providing simple access to compute for AI, with services like AutoTrain, Spaces and Inference Endpoints, directly accessible from the Hub, and billed by Hugging Face to the credit card on file."
    ],
    [
        "https://huggingface.co/docs/hub/security",
        "Security",
        "The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering private repositories for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning."
    ],
    [
        "https://huggingface.co/docs/hub/moderation",
        "Moderation",
        "Check out the Code of Conduct and the Content Guidelines."
    ],
    [
        "https://huggingface.co/docs/hub/paper-pages",
        "Paper Pages",
        "Paper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). Paper pages also enable the community to discuss about the paper."
    ],
    [
        "https://huggingface.co/docs/hub/search",
        "Search",
        "You can now easily search anything on the Hub with Full-text search. We index model cards, dataset cards, and Spaces app.py files."
    ],
    [
        "https://huggingface.co/docs/hub/doi",
        "Digital Object Identifier (DOI)",
        "The Hugging Face Hub offers the possibility to generate DOI for your models or datasets. DOIs (Digital Object Identifiers) are strings uniquely identifying a digital object, anything from articles to figures, including datasets and models. DOIs are tied to object metadata, including the object\u2019s URL, version, creation date, description, etc. They are a commonly accepted reference to digital resources across research and academic communities; they are analogous to a book\u2019s ISBN."
    ],
    [
        "https://huggingface.co/docs/hub/api",
        "Hub API Endpoints",
        "We have open endpoints that you can use to retrieve information from the Hub as well as perform certain actions such as creating model, dataset or Space repos. We offer a wrapper Python library, huggingface_hub, that allows easy access to these endpoints. We also provide webhooks to receive real-time incremental info about repos. Enjoy!"
    ],
    [
        "https://huggingface.co/docs/hub/oauth",
        "Sign-In with HF",
        "You can use the HF OAuth / OpenID connect flow to create a \u201cSign in with HF\u201d flow in any website or App."
    ],
    [
        "https://huggingface.co/docs/transformers/index",
        "\ud83e\udd17 Transformers",
        "State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX."
    ],
    [
        "https://huggingface.co/docs/transformers/quicktour",
        "Quick tour",
        "Get up and running with \ud83e\udd17 Transformers! Whether you\u2019re a developer or an everyday user, this quick tour will help you get started and show you how to use the pipeline() for inference, load a pretrained model and preprocessor with an AutoClass, and quickly train a model with PyTorch or TensorFlow. If you\u2019re a beginner, we recommend checking out our tutorials or course next for more in-depth explanations of the concepts introduced here."
    ],
    [
        "https://huggingface.co/docs/transformers/installation",
        "Installation",
        "Install \ud83e\udd17 Transformers for whichever deep learning library you\u2019re working with, setup your cache, and optionally configure \ud83e\udd17 Transformers to run offline."
    ],
    [
        "https://huggingface.co/docs/transformers/pipeline_tutorial",
        "Run inference with pipelines",
        "The pipeline() makes it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks. Even if you don\u2019t have experience with a specific modality or aren\u2019t familiar with the underlying code behind the models, you can still use them for inference with the pipeline()! This tutorial will teach you to:"
    ],
    [
        "https://huggingface.co/docs/transformers/autoclass_tutorial",
        "Write portable code with AutoClass",
        "With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of \ud83e\udd17 Transformers core philosophy to make the library easy, simple and flexible to use, an AutoClass automatically infers and loads the correct architecture from a given checkpoint. The from_pretrained() method lets you quickly load a pretrained model for any architecture so you don\u2019t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different."
    ],
    [
        "https://huggingface.co/docs/transformers/preprocessing",
        "Preprocess data",
        "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. \ud83e\udd17 Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you\u2019ll learn that for:"
    ],
    [
        "https://huggingface.co/docs/transformers/training",
        "Fine-tune a pretrained model",
        "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. \ud83e\udd17 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:"
    ],
    [
        "https://huggingface.co/docs/transformers/run_scripts",
        "Train with a script",
        "Along with the \ud83e\udd17 Transformers notebooks, there are also example scripts demonstrating how to train a model for a task with PyTorch, TensorFlow, or JAX/Flax."
    ],
    [
        "https://huggingface.co/docs/transformers/accelerate",
        "Set up distributed training with \ud83e\udd17 Accelerate",
        "As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the \ud83e\udd17 Accelerate library to help users easily train a \ud83e\udd17 Transformers model on any type of distributed setup, whether it is multiple GPU\u2019s on one machine or multiple GPU\u2019s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment."
    ],
    [
        "https://huggingface.co/docs/transformers/peft",
        "Load and train adapters with \ud83e\udd17 PEFT",
        "Parameter-Efficient Fine Tuning (PEFT) methods freeze the pretrained model parameters during fine-tuning and add a small number of trainable parameters (the adapters) on top of it. The adapters are trained to learn task-specific information. This approach has been shown to be very memory-efficient with lower compute usage while producing results comparable to a fully fine-tuned model."
    ],
    [
        "https://huggingface.co/docs/transformers/model_sharing",
        "Share your model",
        "The last two tutorials showed how you can fine-tune a model with PyTorch, Keras, and \ud83e\udd17 Accelerate for distributed setups. The next step is to share your model with the community! At Hugging Face, we believe in openly sharing knowledge and resources to democratize artificial intelligence for everyone. We encourage you to consider sharing your model with the community to help others save time and resources."
    ],
    [
        "https://huggingface.co/docs/transformers/transformers_agents",
        "Agents",
        "Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change."
    ],
    [
        "https://huggingface.co/docs/transformers/llm_tutorial",
        "Generation with LLMs",
        "LLMs, or Large Language Models, are the key component behind text generation. In a nutshell, they consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict one token at a time, you need to do something more elaborate to generate new sentences other than just calling the model \u2014 you need to do autoregressive generation."
    ],
    [
        "https://huggingface.co/docs/transformers/fast_tokenizers",
        "Use fast tokenizers from \ud83e\udd17 Tokenizers",
        "The PreTrainedTokenizerFast depends on the \ud83e\udd17 Tokenizers library. The tokenizers obtained from the \ud83e\udd17 Tokenizers library can be loaded very simply into \ud83e\udd17 Transformers."
    ],
    [
        "https://huggingface.co/docs/transformers/multilingual",
        "Run inference with multilingual models",
        "There are several multilingual models in \ud83e\udd17 Transformers, and their inference usage differs from monolingual models. Not all multilingual model usage is different though. Some models, like google-bert/bert-base-multilingual-uncased, can be used just like a monolingual model. This guide will show you how to use multilingual models whose usage differs for inference."
    ],
    [
        "https://huggingface.co/docs/transformers/create_a_model",
        "Use model-specific APIs",
        "An AutoClass automatically infers the model architecture and downloads pretrained configuration and weights. Generally, we recommend using an AutoClass to produce checkpoint-agnostic code. But users who want more control over specific model parameters can create a custom \ud83e\udd17 Transformers model from just a few base classes. This could be particularly useful for anyone who is interested in studying, training or experimenting with a \ud83e\udd17 Transformers model. In this guide, dive deeper into creating a custom model without an AutoClass. Learn how to:"
    ],
    [
        "https://huggingface.co/docs/transformers/custom_models",
        "Share a custom model",
        "The \ud83e\udd17 Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder of the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs."
    ],
    [
        "https://huggingface.co/docs/transformers/chat_templating",
        "Templates for chat models",
        "An increasingly common use case for LLMs is chat. In a chat context, rather than continuing a single string of text (as is the case with a standard language model), the model instead continues a conversation that consists of one or more messages, each of which includes a role, like \u201cuser\u201d or \u201cassistant\u201d, as well as message text."
    ],
    [
        "https://huggingface.co/docs/transformers/trainer",
        "Trainer",
        "The Trainer is a complete training and evaluation loop for PyTorch models implemented in the Transformers library. You only need to pass it the necessary pieces for training (model, tokenizer, dataset, evaluation function, training hyperparameters, etc.), and the Trainer class takes care of the rest. This makes it easier to start training faster without manually writing your own training loop. But at the same time, Trainer is very customizable and offers a ton of training options so you can tailor it to your exact training needs."
    ],
    [
        "https://huggingface.co/docs/transformers/sagemaker",
        "Run training on Amazon SageMaker",
        "The documentation has been moved to hf.co/docs/sagemaker. This page will be removed in transformers 5.0."
    ],
    [
        "https://huggingface.co/docs/transformers/serialization",
        "Export to ONNX",
        "Deploying \ud83e\udd17 Transformers models in production environments often requires, or can benefit from exporting the models into a serialized format that can be loaded and executed on specialized runtimes and hardware."
    ],
    [
        "https://huggingface.co/docs/transformers/tflite",
        "Export to TFLite",
        "TensorFlow Lite is a lightweight framework for deploying machine learning models on resource-constrained devices, such as mobile phones, embedded systems, and Internet of Things (IoT) devices. TFLite is designed to optimize and run models efficiently on these devices with limited computational power, memory, and power consumption. A TensorFlow Lite model is represented in a special efficient portable format identified by the .tflite file extension."
    ],
    [
        "https://huggingface.co/docs/transformers/torchscript",
        "Export to TorchScript",
        "This is the very beginning of our experiments with TorchScript and we are still exploring its capabilities with variable-input-size models. It is a focus of interest to us and we will deepen our analysis in upcoming releases, with more code examples, a more flexible implementation, and benchmarks comparing Python-based codes with compiled TorchScript."
    ],
    [
        "https://huggingface.co/docs/transformers/benchmarks",
        "Benchmarks",
        "Hugging Face\u2019s Benchmarking tools are deprecated and it is advised to use external Benchmarking libraries to measure the speed and memory complexity of Transformer models."
    ],
    [
        "https://huggingface.co/docs/transformers/notebooks",
        "Notebooks with examples",
        "You can find here a list of the official notebooks provided by Hugging Face."
    ],
    [
        "https://huggingface.co/docs/transformers/community",
        "Community resources",
        "This page regroups resources around \ud83e\udd17 Transformers developed by the community."
    ],
    [
        "https://huggingface.co/docs/transformers/custom_tools",
        "Custom Tools and Prompts",
        "If you are not aware of what tools and agents are in the context of transformers, we recommend you read the Transformers Agents page first."
    ],
    [
        "https://huggingface.co/docs/transformers/troubleshooting",
        "Troubleshoot",
        "Sometimes errors occur, but we are here to help! This guide covers some of the most common issues we\u2019ve seen and how you can resolve them. However, this guide isn\u2019t meant to be a comprehensive collection of every \ud83e\udd17 Transformers issue. For more help with troubleshooting your issue, try:"
    ],
    [
        "https://huggingface.co/docs/transformers/performance",
        "Overview",
        "Training large transformer models and deploying them to production present various challenges.\nDuring training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment phase, the model can struggle to handle the required throughput in a production environment."
    ],
    [
        "https://huggingface.co/docs/transformers/quantization",
        "Quantization",
        "Quantization techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits. For example, if your model weights are stored as 32-bit floating points and they\u2019re quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory-usage. Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_gpu_one",
        "Methods and tools for efficient training on a single GPU",
        "This guide demonstrates practical techniques that you can use to increase the efficiency of your model\u2019s training by optimizing memory utilization, speeding up the training, or both. If you\u2019d like to understand how GPU is utilized during training, please refer to the Model training anatomy conceptual guide first. This guide focuses on practical techniques."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_gpu_many",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/fsdp",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_cpu",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_cpu_many",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_tpu_tf",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_train_special",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_hardware",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/hpo_train",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_infer_cpu",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_infer_gpu_one",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/big_models",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/debugging",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/tf_xla",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/perf_torch_compile",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/contributing",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/add_new_model",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/add_tensorflow_model",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/add_new_pipeline",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/testing",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/pr_checks",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/philosophy",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/glossary",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/task_summary",
        "Multiple GPUs and parallelism",
        "If training a model on a single GPU is too slow or if the model\u2019s weights do not fit in a single GPU\u2019s memory, transitioning to a multi-GPU setup may be a viable option. Prior to making this transition, thoroughly explore all the strategies covered in the Methods and tools for efficient training on a single GPU as they are universally applicable to model training on any number of GPUs. Once you have employed those strategies and found them insufficient for your case on a single GPU, consider moving to multiple GPUs."
    ],
    [
        "https://huggingface.co/docs/transformers/tasks_explained",
        "How \ud83e\udd17 Transformers solve tasks",
        "In What \ud83e\udd17 Transformers can do, you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what\u2019s happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, decoder, or encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We\u2019ll also explain how a modern CNN works."
    ],
    [
        "https://huggingface.co/docs/transformers/model_summary",
        "The Transformer model family",
        "Since its introduction in 2017, the original Transformer model has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for predicting the folded structure of proteins, training a cheetah to run, and time series forecasting. With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they\u2019re based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it\u2019ll help you understand Transformers you haven\u2019t encountered before."
    ],
    [
        "https://huggingface.co/docs/transformers/tokenizer_summary",
        "Summary of the tokenizers",
        "On this page, we will have a closer look at tokenization."
    ],
    [
        "https://huggingface.co/docs/transformers/attention",
        "Attention mechanisms",
        "Most transformer models use full attention in the sense that the attention matrix is square. It can be a big computational bottleneck when you have long texts. Longformer and reformer are models that try to be more efficient and use a sparse version of the attention matrix to speed up training."
    ],
    [
        "https://huggingface.co/docs/transformers/pad_truncation",
        "Padding and truncation",
        "Batched inputs are often different lengths, so they can\u2019t be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special padding token to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences."
    ],
    [
        "https://huggingface.co/docs/transformers/bertology",
        "BERTology",
        "There is a growing field of study concerned with investigating the inner working of large-scale transformers like BERT (that some call \u201cBERTology\u201d). Some good examples of this field are:"
    ],
    [
        "https://huggingface.co/docs/transformers/perplexity",
        "Perplexity of fixed-length models",
        "Perplexity (PPL) is one of the most common metrics for evaluating language models. Before diving in, we should note that the metric applies specifically to classical language models (sometimes called autoregressive or causal language models) and is not well defined for masked language models like BERT (see summary of the models)."
    ],
    [
        "https://huggingface.co/docs/transformers/pipeline_webserver",
        "Pipelines for webserver inference",
        "The key thing to understand is that we can use an iterator, just like you would on a dataset, since a webserver is basically a system that waits for requests and treats them as they come in."
    ],
    [
        "https://huggingface.co/docs/transformers/model_memory_anatomy",
        "Model training anatomy",
        "To understand performance optimization techniques that one can apply to improve efficiency of model training speed and memory utilization, it\u2019s helpful to get familiar with how GPU is utilized during training, and how compute intensity varies depending on an operation performed."
    ],
    [
        "https://huggingface.co/docs/transformers/llm_tutorial_optimization",
        "Getting the most out of LLMs",
        "Large Language Models (LLMs) such as GPT3/4, Falcon, and Llama are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries. Deploying these models in real-world tasks remains challenging, however:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/agent",
        "Agents and Tools",
        "Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change."
    ],
    [
        "https://huggingface.co/docs/transformers/model_doc/auto",
        "Auto Classes",
        "In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/backbones",
        "Backbones",
        "A backbone is a model used for feature extraction for higher level computer vision tasks such as object detection and image classification. Transformers provides an AutoBackbone class for initializing a Transformers backbone from pretrained model weights, and two utility classes:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/callback",
        "Callbacks",
        "Callbacks are objects that can customize the behavior of the training loop in the PyTorch Trainer (this feature is not yet implemented in TensorFlow) that can inspect the training loop state (for progress reporting, logging on TensorBoard or other ML platforms\u2026) and take decisions (like early stopping)."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/configuration",
        "Configuration",
        "The base class PretrainedConfig implements the common methods for loading/saving a configuration either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace\u2019s AWS S3 repository)."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/data_collator",
        "Data Collator",
        "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/keras_callbacks",
        "Keras callbacks",
        "When training a Transformers model with Keras, there are some library-specific callbacks available to automate common tasks:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/logging",
        "Logging",
        "\ud83e\udd17 Transformers has a centralized logging system, so that you can setup the verbosity of the library easily."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/model",
        "Models",
        "The base classes PreTrainedModel, TFPreTrainedModel, and FlaxPreTrainedModel implement the common methods for loading/saving a model either from a local file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace\u2019s AWS S3 repository)."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/text_generation",
        "Text Generation",
        "Each framework has a generate method for text generation implemented in their respective GenerationMixin class:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/onnx",
        "ONNX",
        "\ud83e\udd17 Transformers provides a transformers.onnx package that enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/optimizer_schedules",
        "Optimization",
        "The .optimization module provides:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/output",
        "Model outputs",
        "All models have outputs that are instances of subclasses of ModelOutput. Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/pipelines",
        "Pipelines",
        "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/processors",
        "Processors",
        "Processors can mean two different things in the Transformers library:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/quantization",
        "Quantization",
        "Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn\u2019t be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/tokenizer",
        "Tokenizer",
        "A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a \u201cFast\u201d implementation based on the Rust library \ud83e\udd17 Tokenizers. The \u201cFast\u201d implementations allows:"
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/trainer",
        "Trainer",
        "The Trainer class provides an API for feature-complete training in PyTorch, and it supports distributed training on multiple GPUs/TPUs, mixed precision for NVIDIA GPUs, AMD GPUs, and torch.amp for PyTorch. Trainer goes hand-in-hand with the TrainingArguments class, which offers a wide range of options to customize how a model is trained. Together, these two classes provide a complete training API."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/deepspeed",
        "DeepSpeed",
        "DeepSpeed, powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the Trainer class and most of the setup is automatically taken care of for you."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/feature_extractor",
        "Feature Extractor",
        "A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction from sequences, e.g., pre-processing audio files to generate Log-Mel Spectrogram features, feature extraction from images, e.g., cropping image files, but also padding, normalization, and conversion to NumPy, PyTorch, and TensorFlow tensors."
    ],
    [
        "https://huggingface.co/docs/transformers/main_classes/image_processor",
        "Image Processor",
        "An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/modeling_utils",
        "Custom Layers and Utilities",
        "This page lists all the custom layers used by the library, as well as the utility functions it provides for modeling."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/pipelines_utils",
        "Utilities for pipelines",
        "This page lists all the utility functions the library provides for pipelines."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/tokenization_utils",
        "Utilities for Tokenizers",
        "This page lists all the utility functions used by the tokenizers, mainly the class PreTrainedTokenizerBase that implements the common methods between PreTrainedTokenizer and PreTrainedTokenizerFast and the mixin SpecialTokensMixin."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/trainer_utils",
        "Utilities for Trainer",
        "This page lists all the utility functions used by Trainer."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/generation_utils",
        "Utilities for Generation",
        "This page lists all the utility functions used by generate()."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/image_processing_utils",
        "Utilities for Image Processors",
        "This page lists all the utility functions used by the image processors, mainly the functional transformations used to process the images."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/audio_utils",
        "Utilities for Audio processing",
        "This page lists all the utility functions that can be used by the audio FeatureExtractor in order to compute special features from a raw audio using common algorithms such as Short Time Fourier Transform or log mel spectrogram."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/file_utils",
        "General Utilities",
        "This page lists all of Transformers general utility functions that are found in the file utils.py."
    ],
    [
        "https://huggingface.co/docs/transformers/internal/time_series_utils",
        "Utilities for Time Series",
        "This page lists all the utility functions and classes that can be used for Time Series based models."
    ],
    [
        "https://huggingface.co/docs/diffusers/index",
        "\ud83e\udde8 Diffusers",
        ""
    ],
    [
        "https://huggingface.co/docs/diffusers/quicktour",
        "Quicktour",
        "Diffusion models are trained to denoise random Gaussian noise step-by-step to generate a sample of interest, such as an image or audio. This has sparked a tremendous amount of interest in generative AI, and you have probably seen examples of diffusion generated images on the internet. \ud83e\udde8 Diffusers is a library aimed at making diffusion models widely accessible to everyone."
    ],
    [
        "https://huggingface.co/docs/diffusers/stable_diffusion",
        "Effective and efficient diffusion",
        "Getting the DiffusionPipeline to generate images in a certain style or include what you want can be tricky. Often times, you have to run the DiffusionPipeline several times before you end up with an image you\u2019re happy with. But generating something out of nothing is a computationally intensive process, especially if you\u2019re running inference over and over again."
    ],
    [
        "https://huggingface.co/docs/diffusers/installation",
        "Installation",
        "\ud83e\udd17 Diffusers is tested on Python 3.8+, PyTorch 1.7.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:"
    ],
    [
        "https://huggingface.co/docs/diffusers/tutorials/tutorial_overview",
        "Overview",
        "Welcome to \ud83e\udde8 Diffusers! If you\u2019re new to diffusion models and generative AI, and want to learn more, then you\u2019ve come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/write_own_pipeline",
        "Understanding pipelines, models and schedulers",
        "\ud83e\udde8 Diffusers is designed to be a user-friendly and flexible toolbox for building diffusion systems tailored to your use-case. At the core of the toolbox are models and schedulers. While the DiffusionPipeline bundles these components together for convenience, you can also unbundle the pipeline and use the models and schedulers separately to create new diffusion systems."
    ],
    [
        "https://huggingface.co/docs/diffusers/tutorials/autopipeline",
        "AutoPipeline",
        "\ud83e\udd17 Diffusers is able to complete many different tasks, and you can often reuse the same pretrained weights for multiple tasks such as text-to-image, image-to-image, and inpainting. If you\u2019re new to the library and diffusion models though, it may be difficult to know which pipeline to use for a task. For example, if you\u2019re using the runwayml/stable-diffusion-v1-5 checkpoint for text-to-image, you might not know that you could also use it for image-to-image and inpainting by loading the checkpoint with the StableDiffusionImg2ImgPipeline and StableDiffusionInpaintPipeline classes respectively."
    ],
    [
        "https://huggingface.co/docs/diffusers/tutorials/basic_training",
        "Train a diffusion model",
        "Unconditional image generation is a popular application of diffusion models that generates images that look like those in the dataset used for training. Typically, the best results are obtained from finetuning a pretrained model on a specific dataset. You can find many of these checkpoints on the Hub, but if you can\u2019t find one you like, you can always train your own!"
    ],
    [
        "https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference",
        "Load LoRAs for inference",
        "There are many adapter types (with LoRAs being the most popular) trained in different styles to achieve different effects. You can even combine multiple adapters to create new and unique images."
    ],
    [
        "https://huggingface.co/docs/diffusers/tutorials/fast_diffusion",
        "Accelerate inference of text-to-image diffusion models",
        "Diffusion models are slower than their GAN counterparts because of the iterative and sequential reverse diffusion process. There are several techniques that can address this limitation such as progressive timestep distillation (LCM LoRA), model compression (SSD-1B), and reusing adjacent features of the denoiser (DeepCache)."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/loading_overview",
        "Overview",
        "\ud83e\udde8 Diffusers offers many pipelines, models, and schedulers for generative tasks. To make loading these components as simple as possible, we provide a single and unified method - from_pretrained() - that loads any of these components from either the Hugging Face Hub or your local machine. Whenever you load a pipeline or model, the latest files are automatically downloaded and cached so you can quickly reuse them next time without redownloading the files."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/loading",
        "Load pipelines, models, and schedulers",
        "Having an easy way to use a diffusion system for inference is essential to \ud83e\udde8 Diffusers. Diffusion systems often consist of multiple components like parameterized models, tokenizers, and schedulers that interact in complex ways. That is why we designed the DiffusionPipeline to wrap the complexity of the entire diffusion system into an easy-to-use API, while remaining flexible enough to be adapted for other use cases, such as loading each component individually as building blocks to assemble your own diffusion system."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/schedulers",
        "Load and compare different schedulers",
        "Diffusion pipelines are inherently a collection of diffusion models and schedulers that are partly independent from each other. This means that one is able to switch out parts of the pipeline to better customize a pipeline to one\u2019s use case. The best example of this is the Schedulers."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview",
        "Load community pipelines and components",
        "Community pipelines are any DiffusionPipeline class that are different from the original implementation as specified in their paper (for example, the StableDiffusionControlNetPipeline corresponds to the Text-to-Image Generation with ControlNet Conditioning paper). They provide additional functionality or extend the original implementation of a pipeline."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors",
        "Load safetensors",
        "safetensors is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or pickled into a .bin file with Python\u2019s pickle utility. However, pickle is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to pickle, making it ideal for sharing model weights."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/other-formats",
        "Load different Stable Diffusion formats",
        "Stable Diffusion models are available in different formats depending on the framework they\u2019re trained and saved with, and where you download them from. Converting these formats for use in \ud83e\udd17 Diffusers allows you to use all the features supported by the library, such as using different schedulers for inference, building your custom pipeline, and a variety of techniques and methods for optimizing inference speed."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/loading_adapters",
        "Load adapters",
        "There are several training techniques for personalizing diffusion models to generate images of a specific subject or images in certain styles. Each of these training methods produces a different type of adapter. Some of the adapters generate an entirely new model, while other adapters only modify a smaller set of embeddings or weights. This means the loading process for each adapter is also different."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/push_to_hub",
        "Push files to the Hub",
        "\ud83e\udd17 Diffusers provides a PushToHubMixin for uploading your model, scheduler, or pipeline to the Hub. It is an easy way to store your files on the Hub, and also allows you to share your work with others. Under the hood, the PushToHubMixin:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview",
        "Overview",
        "A pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like StableDiffusionXLPipeline or StableDiffusionControlNetPipeline, with specific capabilities. All pipeline types inherit from the base DiffusionPipeline class; pass it any checkpoint, and it\u2019ll automatically detect the pipeline type and load the necessary components."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/unconditional_image_generation",
        "Unconditional image generation",
        "Unconditional image generation generates images that look like a random sample from the training data the model was trained on because the denoising process is not guided by any additional context like text or image."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation",
        "Text-to-image",
        "When you think of diffusion models, text-to-image is usually one of the first things that come to mind. Text-to-image generates an image from a text description (for example, \u201cAstronaut in a jungle, cold color palette, muted colors, detailed, 8k\u201d) which is also known as a prompt."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/img2img",
        "Image-to-image",
        "Image-to-image is similar to text-to-image, but in addition to a prompt, you can also pass an initial image as a starting point for the diffusion process. The initial image is encoded to latent space and noise is added to it. Then the latent diffusion model takes a prompt and the noisy latent image, predicts the added noise, and removes the predicted noise from the initial latent image to get the new latent image. Lastly, a decoder decodes the new latent image back into an image."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/inpaint",
        "Inpainting",
        "Inpainting replaces or edits specific areas of an image. This makes it a useful tool for image restoration like removing defects and artifacts, or even replacing an image area with something entirely new. Inpainting relies on a mask to determine which regions of an image to fill in; the area to inpaint is represented by white pixels and the area to keep is represented by black pixels. The white pixels are filled in by the prompt."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/depth2img",
        "Depth-to-image",
        "The StableDiffusionDepth2ImgPipeline lets you pass a text prompt and an initial image to condition the generation of new images. In addition, you can also pass a depth_map to preserve the image structure. If no depth_map is provided, the pipeline automatically predicts the depth via an integrated depth-estimation model."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/textual_inversion_inference",
        "Textual inversion",
        "The StableDiffusionPipeline supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the Stable Diffusion Conceptualizer."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/distributed_inference",
        "Distributed inference with multiple GPUs",
        "On distributed setups, you can run inference across multiple GPUs with \ud83e\udd17 Accelerate or PyTorch Distributed, which is useful for generating with multiple prompts in parallel."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/reusing_seeds",
        "Improve image quality with deterministic generation",
        "A common way to improve the quality of generated images is with deterministic batch generation, generate a batch of images and select one image to improve with a more detailed prompt in a second round of inference. The key is to pass a list of torch.Generator\u2019s to the pipeline for batched image generation, and tie each Generator to a seed so you can reuse it for an image."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/control_brightness",
        "Control image brightness",
        "The Stable Diffusion pipeline is mediocre at generating images that are either very bright or dark as explained in the Common Diffusion Noise Schedules and Sample Steps are Flawed paper. The solutions proposed in the paper are currently implemented in the DDIMScheduler which you can use to improve the lighting in your images."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/weighted_prompts",
        "Prompt weighting",
        "Prompt weighting provides a way to emphasize or de-emphasize certain parts of a prompt, allowing for more control over the generated image. A prompt can include several concepts, which gets turned into contextualized text embeddings. The embeddings are used by the model to condition its cross-attention layers to generate an image (read the Stable Diffusion blog post to learn more about how it works)."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/freeu",
        "Improve generation quality with FreeU",
        "The UNet is responsible for denoising during the reverse diffusion process, and there are two distinct features in its architecture:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/pipeline_overview",
        "Overview",
        "A pipeline is an end-to-end class that provides a quick and easy way to use a diffusion system for inference by bundling independently trained models and schedulers together. Certain combinations of models and schedulers define specific pipeline types, like StableDiffusionXLPipeline or StableDiffusionControlNetPipeline, with specific capabilities. All pipeline types inherit from the base DiffusionPipeline class; pass it any checkpoint, and it\u2019ll automatically detect the pipeline type and load the necessary components."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/sdxl",
        "Stable Diffusion XL",
        "Stable Diffusion XL (SDXL) is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/sdxl_turbo",
        "SDXL Turbo",
        "SDXL Turbo is an adversarial time-distilled Stable Diffusion XL (SDXL) model capable of running inference in as little as 1 step."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/kandinsky",
        "Kandinsky",
        "The Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/controlnet",
        "ControlNet",
        "ControlNet is a type of model for controlling image diffusion models by conditioning the model with an additional input image. There are many types of conditioning inputs (canny edge, user sketching, human pose, depth, and more) you can use to control a diffusion model. This is hugely useful because it affords you greater control over image generation, making it easier to generate specific images without experimenting with different text prompts or denoising values as much."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/shap-e",
        "Shap-E",
        "Shap-E is a conditional model for generating 3D assets which could be used for video game development, interior design, and architecture. It is trained on a large dataset of 3D assets, and post-processed to render more views of each object and produce 16K instead of 4K point clouds. The Shap-E model is trained in two steps:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/diffedit",
        "DiffEdit",
        "Image editing typically requires providing a mask of the area to be edited. DiffEdit automatically generates the mask for you based on a text query, making it easier overall to create a mask without image editing software. The DiffEdit algorithm works in three steps:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/distilled_sd",
        "Distilled Stable Diffusion inference",
        "Stable Diffusion inference can be a computationally intensive process because it must iteratively denoise the latents to generate an image. To reduce the computational burden, you can use a distilled version of the Stable Diffusion model from Nota AI. The distilled version of their Stable Diffusion model eliminates some of the residual and attention blocks from the UNet, reducing the model size by 51% and improving latency on CPU/GPU by 43%."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/callback",
        "Pipeline callbacks",
        "The denoising loop of a pipeline can be modified with custom defined functions using the callback_on_step_end parameter. The callback function is executed at the end of each step, and modifies the pipeline attributes and variables for the next step. This is really useful for dynamically adjusting certain pipeline attributes or modifying tensor variables. This versatility allows for interesting use-cases such as changing the prompt embeddings at each timestep, assigning different weights to the prompt embeddings, and editing the guidance scale. With callbacks, you can implement new features without modifying the underlying code!"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/reproducibility",
        "Create reproducible pipelines",
        "Reproducibility is important for testing, replicating results, and can even be used to improve image quality. However, the randomness in diffusion models is a desired property because it allows the pipeline to generate different images every time it is run. While you can\u2019t expect to get the exact same results across platforms, you can expect results to be reproducible across releases and platforms within a certain tolerance range. Even then, tolerance varies depending on the diffusion pipeline and checkpoint."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_examples",
        "Community pipelines",
        "For more context about the design choices behind community pipelines, please have a look at this issue."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/contribute_pipeline",
        "Contribute a community pipeline",
        "\ud83d\udca1 Take a look at GitHub Issue #841 for more context about why we\u2019re adding community pipelines to help everyone easily share their work without being slowed down."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm_lora",
        "Latent Consistency Model-LoRA",
        "Latent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/inference_with_lcm",
        "Latent Consistency Model",
        "Latent Consistency Models (LCM) enable quality image generation in typically 2-4 steps making it possible to use diffusion models in almost real-time settings."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/svd",
        "Stable Video Diffusion",
        "Stable Video Diffusion (SVD) is a powerful image-to-video generation model that can generate 2-4 second high resolution (576x1024) videos conditioned on an input image."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/overview",
        "Overview",
        "\ud83e\udd17 Diffusers provides a collection of training scripts for you to train your own diffusion models. You can find all of our training scripts in diffusers/examples."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/create_dataset",
        "Create a dataset for training",
        "There are many datasets on the Hub to train a model on, but if you can\u2019t find one you\u2019re interested in or want to use your own, you can create a dataset with the \ud83e\udd17 Datasets library. The dataset structure depends on the task you want to train your model on. The most basic dataset structure is a directory of images for tasks like unconditional image generation. Another dataset structure may be a directory of images and a text file containing their corresponding text captions for tasks like text-to-image generation."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/adapt_a_model",
        "Adapt a model to a new task",
        "Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/unconditional_training",
        "Unconditional image generation",
        "Unconditional image generation models are not conditioned on text or images during training. It only generates images that resemble its training data distribution."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/text2image",
        "Text-to-image",
        "The text-to-image script is experimental, and it\u2019s easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/sdxl",
        "Stable Diffusion XL",
        "This script is experimental, and it\u2019s easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/kandinsky",
        "Kandinsky 2.2",
        "This script is experimental, and it\u2019s easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/wuerstchen",
        "Wuerstchen",
        "The Wuerstchen model drastically reduces computational costs by compressing the latent space by 42x, without compromising image quality and accelerating inference. During training, Wuerstchen uses two models (VQGAN + autoencoder) to compress the latents, and then a third model (text-conditioned latent diffusion model) is conditioned on this highly compressed space to generate an image."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/controlnet",
        "ControlNet",
        "ControlNet models are adapters trained on top of another pretrained model. It allows for a greater degree of control over image generation by conditioning the model with an additional input image. The input image can be a canny edge, depth map, human pose, and many more."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/t2i_adapters",
        "T2I-Adapters",
        "T2I-Adapter is a lightweight adapter model that provides an additional conditioning input image (line art, canny, sketch, depth, pose) to better control image generation. It is similar to a ControlNet, but it is a lot smaller (~77M parameters and ~300MB file size) because its only inserts weights into the UNet instead of copying and training it."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/instructpix2pix",
        "InstructPix2Pix",
        "InstructPix2Pix is a Stable Diffusion model trained to edit images from human-provided instructions. For example, your prompt can be \u201cturn the clouds rainy\u201d and the model will edit the input image accordingly. This model is conditioned on the text prompt (or editing instruction) and the input image."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/text_inversion",
        "Textual Inversion",
        "Textual Inversion is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/dreambooth",
        "DreamBooth",
        "DreamBooth is a training technique that updates the entire diffusion model by training on just a few images of a subject or style. It works by associating a special word in the prompt with the example images."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/lora",
        "LoRA",
        "This is experimental and the API may change in the future."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/custom_diffusion",
        "Custom Diffusion",
        "Custom Diffusion is a training technique for personalizing image generation models. Like Textual Inversion, DreamBooth, and LoRA, Custom Diffusion only requires a few (~4-5) example images. This technique works by only training weights in the cross-attention layers, and it uses a special word to represent the newly learned concept. Custom Diffusion is unique because it can also learn multiple concepts at the same time."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/lcm_distill",
        "Latent Consistency Distillation",
        "Latent Consistency Models (LCMs) are able to generate high-quality images in just a few steps, representing a big leap forward because many pipelines require at least 25+ steps. LCMs are produced by applying the latent consistency distillation method to any Stable Diffusion model. This method works by applying one-stage guided distillation to the latent space, and incorporating a skipping-step method to consistently skip timesteps to accelerate the distillation process (refer to section 4.1, 4.2, and 4.3 of the paper for more details)."
    ],
    [
        "https://huggingface.co/docs/diffusers/training/ddpo",
        "Reinforcement learning training with DDPO",
        "You can fine-tune Stable Diffusion on a reward function via reinforcement learning with the \ud83e\udd17 TRL library and \ud83e\udd17 Diffusers. This is done with the Denoising Diffusion Policy Optimization (DDPO) algorithm introduced by Black et al. in Training Diffusion Models with Reinforcement Learning, which is implemented in \ud83e\udd17 TRL with the DDPOTrainer."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/other-modalities",
        "Other Modalities",
        "Diffusers is in the process of expanding to modalities other than images."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/opt_overview",
        "Overview",
        "Generating high-quality outputs is computationally intensive, especially during each iterative step where you go from a noisy output to a less noisy output. One of \ud83e\udd17 Diffuser\u2019s goals is to make this technology widely accessible to everyone, which includes enabling fast inference on consumer and specialized hardware."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/fp16",
        "Speed up inference",
        "There are several ways to optimize \ud83e\udd17 Diffusers for inference speed. As a general rule of thumb, we recommend using either xFormers or torch.nn.functional.scaled_dot_product_attention in PyTorch 2.0 for their memory-efficient attention."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/memory",
        "Reduce memory usage",
        "A barrier to using diffusion models is the large amount of memory required. To overcome this challenge, there are several memory-reducing techniques you can use to run even some of the largest models on free-tier or consumer GPUs. Some of these techniques can even be combined to further reduce memory usage."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/torch2.0",
        "PyTorch 2.0",
        "\ud83e\udd17 Diffusers supports the latest optimizations from PyTorch 2.0 which include:"
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/xformers",
        "xFormers",
        "We recommend xFormers for both inference and training. In our tests, the optimizations performed in the attention blocks allow for both faster speed and reduced memory consumption."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/tome",
        "Token merging",
        "Token merging (ToMe) merges redundant tokens/patches progressively in the forward pass of a Transformer-based network which can speed-up the inference latency of StableDiffusionPipeline."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/deepcache",
        "DeepCache",
        "DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture."
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/stable_diffusion_jax_how_to",
        "DeepCache",
        "DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/onnx",
        "DeepCache",
        "DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/open_vino",
        "DeepCache",
        "DeepCache accelerates StableDiffusionPipeline and StableDiffusionXLPipeline by strategically caching and reusing high-level features while efficiently updating low-level features by taking advantage of the U-Net architecture."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/coreml",
        "Core ML",
        "Core ML is the model format and machine learning library supported by Apple frameworks. If you are interested in running Stable Diffusion models inside your macOS or iOS/iPadOS apps, this guide will show you how to convert existing PyTorch checkpoints into the Core ML format and use them for inference with Python or Swift."
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/mps",
        "Metal Performance Shaders (MPS)",
        "\ud83e\udd17 Diffusers is compatible with Apple silicon (M1/M2 chips) using the PyTorch mps device, which uses the Metal framework to leverage the GPU on MacOS devices. You\u2019ll need to have:"
    ],
    [
        "https://huggingface.co/docs/diffusers/optimization/habana",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/conceptual/philosophy",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/using-diffusers/controlling_generation",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/conceptual/contribution",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/conceptual/ethical_guidelines",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/conceptual/evaluation",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/configuration",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/logging",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/outputs",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/ip_adapter",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/lora",
        "Habana Gaudi",
        "\ud83e\udd17 Diffusers is compatible with Habana Gaudi through \ud83e\udd17 Optimum. Follow the installation guide to install the SynapseAI and Gaudi drivers, and then install Optimum Habana:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/single_file",
        "Single files",
        "Diffusers supports loading pretrained pipeline (or model) weights stored in a single file, such as a ckpt or safetensors file. These single file types are typically produced from community trained models. There are three classes for loading single file weights:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/textual_inversion",
        "Textual Inversion",
        "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images. The file produced from training is extremely small (a few KBs) and the new embeddings can be loaded into the text encoder."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/unet",
        "UNet",
        "Some training methods - like LoRA and Custom Diffusion - typically target the UNet\u2019s attention layers, but these training methods can also target other non-attention layers. Instead of training all of a model\u2019s parameters, only a subset of the parameters are trained, which is faster and more efficient. This class is useful if you\u2019re only loading weights into a UNet. If you need to load weights into the text encoder or a text encoder and UNet, try using the load_lora_weights() function instead."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/loaders/peft",
        "PEFT",
        "Diffusers supports loading adapters such as LoRA with the PEFT library with the PeftAdapterMixin class. This allows modeling classes in Diffusers like UNet2DConditionModel to load an adapter."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/overview",
        "Overview",
        "\ud83e\udd17 Diffusers provides pretrained models for popular algorithms and modules to create custom diffusion systems. The primary function of models is to denoise an input sample as modeled by the distribution\np\n\u03b8\n(x\nt\u22121\n\u2223x\nt\n)."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/unet",
        "UNet1DModel",
        "The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in \ud83e\udd17 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in \ud83e\udd17 Diffusers, depending on it\u2019s number of dimensions and whether it is a conditional model or not. This is a 1D UNet model."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/unet2d",
        "UNet2DModel",
        "The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in \ud83e\udd17 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in \ud83e\udd17 Diffusers, depending on it\u2019s number of dimensions and whether it is a conditional model or not. This is a 2D UNet model."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/unet2d-cond",
        "UNet2DConditionModel",
        "The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in \ud83e\udd17 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in \ud83e\udd17 Diffusers, depending on it\u2019s number of dimensions and whether it is a conditional model or not. This is a 2D UNet conditional model."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/unet3d-cond",
        "UNet3DConditionModel",
        "The UNet model was originally introduced by Ronneberger et al. for biomedical image segmentation, but it is also commonly used in \ud83e\udd17 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in \ud83e\udd17 Diffusers, depending on it\u2019s number of dimensions and whether it is a conditional model or not. This is a 3D UNet conditional model."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/unet-motion",
        "UNetMotionModel",
        "The UNet model was originally introduced by Ronneberger et al for biomedical image segmentation, but it is also commonly used in \ud83e\udd17 Diffusers because it outputs images that are the same size as the input. It is one of the most important components of a diffusion system because it facilitates the actual diffusion process. There are several variants of the UNet model in \ud83e\udd17 Diffusers, depending on it\u2019s number of dimensions and whether it is a conditional model or not. This is a 2D UNet model."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/uvit2d",
        "UViT2DModel",
        "The U-ViT model is a vision transformer (ViT) based UNet. This model incorporates elements from ViT (considers all inputs such as time, conditions and noisy image patches as tokens) and a UNet (long skip connections between the shallow and deep layers). The skip connection is important for predicting pixel-level features. An additional 3x3 convolutional block is applied prior to the final output to improve image quality."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/vq",
        "VQModel",
        "The VQ-VAE model was introduced in Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray Kavukcuoglu. The model is used in \ud83e\udd17 Diffusers to decode latent representations into images. Unlike AutoencoderKL, the VQModel works in a quantized latent space."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/autoencoderkl",
        "AutoencoderKL",
        "The variational autoencoder (VAE) model with KL loss was introduced in Auto-Encoding Variational Bayes by Diederik P. Kingma and Max Welling. The model is used in \ud83e\udd17 Diffusers to encode images into latents and to decode latent representations into images."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/asymmetricautoencoderkl",
        "AsymmetricAutoencoderKL",
        "Improved larger variational autoencoder (VAE) model with KL loss for inpainting task: Designing a Better Asymmetric VQGAN for StableDiffusion by Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/autoencoder_tiny",
        "Tiny AutoEncoder",
        "Tiny AutoEncoder for Stable Diffusion (TAESD) was introduced in madebyollin/taesd by Ollin Boer Bohan. It is a tiny distilled version of Stable Diffusion\u2019s VAE that can quickly decode the latents in a StableDiffusionPipeline or StableDiffusionXLPipeline almost instantly."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/consistency_decoder_vae",
        "ConsistencyDecoderVAE",
        "Consistency decoder can be used to decode the latents from the denoising UNet in the StableDiffusionPipeline. This decoder was introduced in the DALL-E 3 technical report."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/transformer2d",
        "Transformer2D",
        "A Transformer model for image-like data from CompVis that is based on the Vision Transformer introduced by Dosovitskiy et al. The Transformer2DModel accepts discrete (classes of vector embeddings) or continuous (actual embeddings) inputs."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/transformer_temporal",
        "Transformer Temporal",
        "A Transformer model for video-like data."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/prior_transformer",
        "Prior Transformer",
        "The Prior Transformer was originally introduced in Hierarchical Text-Conditional Image Generation with CLIP Latents by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/models/controlnet",
        "ControlNet",
        "The ControlNet model was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, Maneesh Agrawala. It provides a greater degree of control over text-to-image generation by conditioning the model on additional inputs such as edge maps, depth maps, segmentation maps, and keypoints for pose detection."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/overview",
        "Overview",
        "Pipelines provide a simple way to run state-of-the-art diffusion models in inference by bundling all of the necessary components (multiple independently-trained models, schedulers, and processors) into a single end-to-end class. Pipelines are flexible and they can be adapted to use different schedulers or even model components."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/amused",
        "aMUSEd",
        "aMUSEd was introduced in aMUSEd: An Open MUSE Reproduction by Suraj Patil, William Berman, Robin Rombach, and Patrick von Platen."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/animatediff",
        "AnimateDiff",
        "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning by Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/attend_and_excite",
        "Attend-and-Excite",
        "Attend-and-Excite for Stable Diffusion was proposed in Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models and provides textual attention control over image generation."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/audioldm",
        "AudioLDM",
        "AudioLDM was proposed in AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al. Inspired by Stable Diffusion, AudioLDM is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional sound effects, human speech and music."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/audioldm2",
        "AudioLDM 2",
        "AudioLDM 2 was proposed in AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining by Haohe Liu et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional sound effects, human speech and music."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/auto_pipeline",
        "AutoPipeline",
        "AutoPipeline is designed to:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/blip_diffusion",
        "BLIP-Diffusion",
        "BLIP-Diffusion was proposed in BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. It enables zero-shot subject-driven generation and control-guided zero-shot generation."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/consistency_models",
        "Consistency Models",
        "Consistency Models were proposed in Consistency Models by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/controlnet",
        "ControlNet",
        "ControlNet was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, and Maneesh Agrawala."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/controlnet_sdxl",
        "ControlNet with Stable Diffusion XL",
        "ControlNet was introduced in Adding Conditional Control to Text-to-Image Diffusion Models by Lvmin Zhang, Anyi Rao, and Maneesh Agrawala."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/dance_diffusion",
        "Dance Diffusion",
        "Dance Diffusion is by Zach Evans."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/ddim",
        "DDIM",
        "Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/ddpm",
        "DDPM",
        "Denoising Diffusion Probabilistic Models (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the \ud83e\udd17 Diffusers library, DDPM refers to the discrete denoising scheduler from the paper as well as the pipeline."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if",
        "DeepFloyd IF",
        "DeepFloyd IF is a novel state-of-the-art open-source text-to-image model with a high degree of photorealism and language understanding. The model is a modular composed of a frozen text encoder and three cascaded pixel diffusion modules:"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/diffedit",
        "DiffEdit",
        "DiffEdit: Diffusion-based semantic image editing with mask guidance is by Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/dit",
        "DiT",
        "Scalable Diffusion Models with Transformers (DiT) is by William Peebles and Saining Xie."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/i2vgenxl",
        "I2VGen-XL",
        "I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models by Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/pix2pix",
        "InstructPix2Pix",
        "InstructPix2Pix: Learning to Follow Image Editing Instructions is by Tim Brooks, Aleksander Holynski and Alexei A. Efros."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/kandinsky",
        "Kandinsky 2.1",
        "Kandinsky 2.1 is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/kandinsky_v22",
        "Kandinsky 2.2",
        "Kandinsky 2.2 is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/kandinsky3",
        "Kandinsky 3",
        "Kandinsky 3 is created by Vladimir Arkhipkin,Anastasia Maltseva,Igor Pavlov,Andrei Filatov,Arseniy Shakhmatov,Andrey Kuznetsov,Denis Dimitrov, Zein Shaheen"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/latent_consistency_models",
        "Latent Consistency Models",
        "Latent Consistency Models (LCMs) were proposed in Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/latent_diffusion",
        "Latent Diffusion",
        "Latent Diffusion was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/panorama",
        "MultiDiffusion",
        "MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation is by Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/musicldm",
        "MusicLDM",
        "MusicLDM was proposed in MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies by Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov. MusicLDM takes a text prompt as input and predicts the corresponding music sample."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/paint_by_example",
        "Paint by Example",
        "Paint by Example: Exemplar-based Image Editing with Diffusion Models is by Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/pia",
        "Personalized Image Animator (PIA)",
        "PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models by Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, Kai Chen"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/pixart",
        "PixArt-\u03b1",
        ""
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/self_attention_guidance",
        "Self-Attention Guidance",
        "Improving Sample Quality of Diffusion Models Using Self-Attention Guidance is by Susung Hong et al."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/semantic_stable_diffusion",
        "Semantic Guidance",
        "Semantic Guidance for Diffusion Models was proposed in SEGA: Instructing Text-to-Image Models using Semantic Guidance and provides strong semantic control over image generation. Small changes to the text prompt usually result in entirely different output images. However, with SEGA a variety of changes to the image are enabled that can be controlled easily and intuitively, while staying true to the original image composition."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/shap_e",
        "Shap-E",
        "The Shap-E model was proposed in Shap-E: Generating Conditional 3D Implicit Functions by Alex Nichol and Heewoo Jun from OpenAI."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview",
        "Overview",
        "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img",
        "Text-to-image",
        "The Stable Diffusion model was created by researchers and engineers from CompVis, Stability AI, Runway, and LAION. The StableDiffusionPipeline is capable of generating photorealistic images given any text input. It\u2019s trained on 512x512 images from a subset of the LAION-5B dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs. Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in High-Resolution Image Synthesis with Latent Diffusion Models by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img",
        "Image-to-image",
        "The Stable Diffusion model can also be applied to image-to-image generation by passing a text prompt and an initial image to condition the generation of new images."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint",
        "Inpainting",
        "The Stable Diffusion model can also be applied to inpainting which lets you edit specific parts of an image by providing a mask and a text prompt using Stable Diffusion."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/depth2img",
        "Depth-to-image",
        "The Stable Diffusion model can also infer depth based on an image using MiDaS. This allows you to pass a text prompt and an initial image to condition the generation of new images as well as a depth_map to preserve the image structure."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation",
        "Image variation",
        "The Stable Diffusion model can also generate variations from an input image. It uses a fine-tuned version of a Stable Diffusion model by Justin Pinkney from Lambda."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_safe",
        "Safe Stable Diffusion",
        "Safe Stable Diffusion was proposed in Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models and mitigates inappropriate degeneration from Stable Diffusion models because they\u2019re trained on unfiltered web-crawled datasets. For instance Stable Diffusion may unexpectedly generate nudity, violence, images depicting self-harm, and otherwise offensive content. Safe Stable Diffusion is an extension of Stable Diffusion that drastically reduces this type of content."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_2",
        "Stable Diffusion 2",
        "Stable Diffusion 2 is a text-to-image latent diffusion model built upon the work of the original Stable Diffusion, and it was led by Robin Rombach and Katherine Crowson from Stability AI and LAION."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl",
        "Stable Diffusion XL",
        "Stable Diffusion XL (SDXL) was proposed in SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis by Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/sdxl_turbo",
        "SDXL Turbo",
        "Stable Diffusion XL (SDXL) Turbo was proposed in Adversarial Diffusion Distillation by Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale",
        "Latent upscaler",
        "The Stable Diffusion latent upscaler model was created by Katherine Crowson in collaboration with Stability AI. It is used to enhance the output image resolution by a factor of 2 (see this demo notebook for a demonstration of the original implementation)."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale",
        "Super-resolution",
        "The Stable Diffusion upscaler diffusion model was created by the researchers and engineers from CompVis, Stability AI, and LAION. It is used to enhance the resolution of input images by a factor of 4."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/k_diffusion",
        "K-Diffusion",
        "k-diffusion is a popular library created by Katherine Crowson. We provide StableDiffusionKDiffusionPipeline and StableDiffusionXLKDiffusionPipeline that allow you to run Stable DIffusion with samplers from k-diffusion."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/ldm3d_diffusion",
        "LDM3D Text-to-(RGB, Depth), Text-to-(RGB-pano, Depth-pano), LDM3D Upscaler",
        "LDM3D was proposed in LDM3D: Latent Diffusion Model for 3D by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal. LDM3D generates an image and a depth map from a given text prompt unlike the existing text-to-image diffusion models such as Stable Diffusion which only generates an image. With almost the same number of parameters, LDM3D achieves to create a latent space that can compress both the RGB images and the depth maps."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/adapter",
        "Stable Diffusion T2I-Adapter",
        "T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models by Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/gligen",
        "GLIGEN (Grounded Language-to-Image Generation)",
        "The GLIGEN model was created by researchers and engineers from University of Wisconsin-Madison, Columbia University, and Microsoft. The StableDiffusionGLIGENPipeline and StableDiffusionGLIGENTextImagePipeline can generate photorealistic images conditioned on grounding inputs. Along with text and bounding boxes with StableDiffusionGLIGENPipeline, if input images are given, StableDiffusionGLIGENTextImagePipeline can insert objects described by text at the region defined by bounding boxes. Otherwise, it\u2019ll generate an image described by the caption/prompt and insert objects described by text at the region defined by bounding boxes. It\u2019s trained on COCO2014D and COCO2014CD datasets, and the model uses a frozen CLIP ViT-L/14 text encoder to condition itself on grounding inputs."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/stable_unclip",
        "Stable unCLIP",
        "Stable unCLIP checkpoints are finetuned from Stable Diffusion 2.1 checkpoints to condition on CLIP image embeddings. Stable unCLIP still conditions on text embeddings. Given the two separate conditionings, stable unCLIP can be used for text guided image variation. When combined with an unCLIP prior, it can also be used for full text to image generation."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/text_to_video",
        "Text-to-video",
        "\ud83e\uddea This pipeline is for research purposes only."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/text_to_video_zero",
        "Text2Video-Zero",
        "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators is by Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/unclip",
        "unCLIP",
        "Hierarchical Text-Conditional Image Generation with CLIP Latents is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in \ud83e\udd17 Diffusers comes from kakaobrain\u2019s karlo."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/unidiffuser",
        "UniDiffuser",
        "The UniDiffuser model was proposed in One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale by Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/value_guided_sampling",
        "Value-guided sampling",
        "\ud83e\uddea This is an experimental pipeline for reinforcement learning!"
    ],
    [
        "https://huggingface.co/docs/diffusers/api/pipelines/wuerstchen",
        "Wuerstchen",
        "Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models is by Pablo Pernias, Dominic Rampas, Mats L. Richter and Christopher Pal and Marc Aubreville."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/overview",
        "Overview",
        "\ud83e\udd17 Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model\u2019s output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward n timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be discrete in which case the timestep is an int or continuous in which case the timestep is a float."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/cm_stochastic_iterative",
        "CMStochasticIterativeScheduler",
        "Consistency Models by Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever introduced a multistep and onestep scheduler (Algorithm 1) that is capable of generating good samples in one or a small number of steps."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/consistency_decoder",
        "ConsistencyDecoderScheduler",
        "This scheduler is a part of the ConsistencyDecoderPipeline and was introduced in DALL-E 3."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/ddim_inverse",
        "DDIMInverseScheduler",
        "DDIMInverseScheduler is the inverted scheduler from Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon. The implementation is mostly based on the DDIM inversion definition from Null-text Inversion for Editing Real Images using Guided Diffusion Models."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/ddim",
        "DDIMScheduler",
        "Denoising Diffusion Implicit Models (DDIM) by Jiaming Song, Chenlin Meng and Stefano Ermon."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/ddpm",
        "DDPMScheduler",
        "Denoising Diffusion Probabilistic Models (DDPM) by Jonathan Ho, Ajay Jain and Pieter Abbeel proposes a diffusion based model of the same name. In the context of the \ud83e\udd17 Diffusers library, DDPM refers to the discrete denoising scheduler from the paper as well as the pipeline."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/deis",
        "DEISMultistepScheduler",
        "Diffusion Exponential Integrator Sampler (DEIS) is proposed in Fast Sampling of Diffusion Models with Exponential Integrator by Qinsheng Zhang and Yongxin Chen. DEISMultistepScheduler is a fast high order solver for diffusion ordinary differential equations (ODEs)."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver_inverse",
        "DPMSolverMultistepInverse",
        "DPMSolverMultistepInverse is the inverted scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/multistep_dpm_solver",
        "DPMSolverMultistepScheduler",
        "DPMSolverMultistep is a multistep scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/dpm_sde",
        "DPMSolverSDEScheduler",
        "The DPMSolverSDEScheduler is inspired by the stochastic sampler from the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/singlestep_dpm_solver",
        "DPMSolverSinglestepScheduler",
        "DPMSolverSinglestepScheduler is a single step scheduler from DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps and DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models by Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/euler_ancestral",
        "EulerAncestralDiscreteScheduler",
        "A scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original k-diffusion implementation by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/euler",
        "EulerDiscreteScheduler",
        "The Euler scheduler (Algorithm 2) is from the Elucidating the Design Space of Diffusion-Based Generative Models paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original k-diffusion implementation by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/heun",
        "HeunDiscreteScheduler",
        "The Heun scheduler (Algorithm 1) is from the Elucidating the Design Space of Diffusion-Based Generative Models paper by Karras et al. The scheduler is ported from the k-diffusion library and created by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/ipndm",
        "IPNDMScheduler",
        "IPNDMScheduler is a fourth-order Improved Pseudo Linear Multistep scheduler. The original implementation can be found at crowsonkb/v-diffusion-pytorch."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/stochastic_karras_ve",
        "KarrasVeScheduler",
        "KarrasVeScheduler is a stochastic sampler tailored to variance-expanding (VE) models. It is based on the Elucidating the Design Space of Diffusion-Based Generative Models and Score-based generative modeling through stochastic differential equations papers."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete_ancestral",
        "KDPM2AncestralDiscreteScheduler",
        "The KDPM2DiscreteScheduler with ancestral sampling is inspired by the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/dpm_discrete",
        "KDPM2DiscreteScheduler",
        "The KDPM2DiscreteScheduler is inspired by the Elucidating the Design Space of Diffusion-Based Generative Models paper, and the scheduler is ported from and created by Katherine Crowson."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/lcm",
        "LCMScheduler",
        "Multistep and onestep scheduler (Algorithm 3) introduced alongside latent consistency models in the paper Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. This scheduler should be able to generate good samples from LatentConsistencyModelPipeline in 1-8 steps."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/lms_discrete",
        "LMSDiscreteScheduler",
        "LMSDiscreteScheduler is a linear multistep scheduler for discrete beta schedules. The scheduler is ported from and created by Katherine Crowson, and the original implementation can be found at crowsonkb/k-diffusion."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/pndm",
        "PNDMScheduler",
        "PNDMScheduler, or pseudo numerical methods for diffusion models, uses more advanced ODE integration techniques like the Runge-Kutta and linear multi-step method. The original implementation can be found at crowsonkb/k-diffusion."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/repaint",
        "RePaintScheduler",
        "RePaintScheduler is a DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks. It is designed to be used with the RePaintPipeline, and it is based on the paper RePaint: Inpainting using Denoising Diffusion Probabilistic Models by Andreas Lugmayr et al."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/score_sde_ve",
        "ScoreSdeVeScheduler",
        "ScoreSdeVeScheduler is a variance exploding stochastic differential equation (SDE) scheduler. It was introduced in the Score-Based Generative Modeling through Stochastic Differential Equations paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/score_sde_vp",
        "ScoreSdeVpScheduler",
        "ScoreSdeVpScheduler is a variance preserving stochastic differential equation (SDE) scheduler. It was introduced in the Score-Based Generative Modeling through Stochastic Differential Equations paper by Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/unipc",
        "UniPCMultistepScheduler",
        "UniPCMultistepScheduler is a training-free framework designed for fast sampling of diffusion models. It was introduced in UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models by Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/schedulers/vq_diffusion",
        "VQDiffusionScheduler",
        "VQDiffusionScheduler converts the transformer model\u2019s output into a sample for the unnoised image at the previous diffusion timestep. It was introduced in Vector Quantized Diffusion Model for Text-to-Image Synthesis by Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/internal_classes_overview",
        "Overview",
        "The APIs in this section are more experimental and prone to breaking changes. Most of them are used internally for development, but they may also be useful to you if you\u2019re interested in building a diffusion model with some custom parts or if you\u2019re interested in some of our helper utilities for working with \ud83e\udd17 Diffusers."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/attnprocessor",
        "Attention Processor",
        "An attention processor is a class for applying different types of attention mechanisms."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/activations",
        "Custom activation functions",
        "Customized activation functions for supporting various models in \ud83e\udd17 Diffusers."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/normalization",
        "Custom normalization layers",
        "Customized normalization layers for supporting various models in \ud83e\udd17 Diffusers."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/utilities",
        "Utilities",
        "Utility and helper functions for working with \ud83e\udd17 Diffusers."
    ],
    [
        "https://huggingface.co/docs/diffusers/api/image_processor",
        "VAE Image Processor",
        "The VaeImageProcessor provides a unified API for StableDiffusionPipelines to prepare image inputs for VAE encoding and post-processing outputs once they\u2019re decoded. This includes transformations such as resizing, normalization, and conversion between PIL Image, PyTorch, and NumPy arrays."
    ],
    [
        "https://huggingface.co/docs/datasets/index",
        "\ud83e\udd17 Datasets",
        "\ud83e\udd17 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks."
    ],
    [
        "https://huggingface.co/docs/datasets/quickstart",
        "Quickstart",
        "This quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate \ud83e\udd17 Datasets into their model training workflow. If you\u2019re a beginner, we recommend starting with our tutorials, where you\u2019ll get a more thorough introduction."
    ],
    [
        "https://huggingface.co/docs/datasets/installation",
        "Installation",
        "Before you start, you\u2019ll need to setup your environment and install the appropriate packages. \ud83e\udd17 Datasets is tested on Python 3.7+."
    ],
    [
        "https://huggingface.co/docs/datasets/tutorial",
        "Overview",
        "Welcome to the \ud83e\udd17 Datasets tutorials! These beginner-friendly tutorials will guide you through the fundamentals of working with \ud83e\udd17 Datasets. You\u2019ll load and prepare a dataset for training with your machine learning framework of choice. Along the way, you\u2019ll learn how to load different dataset configurations and splits, interact with and see what\u2019s inside your dataset, preprocess, and share a dataset to the Hub."
    ],
    [
        "https://huggingface.co/docs/datasets/load_hub",
        "Load a dataset from the Hub",
        "Finding high-quality datasets that are reproducible and accessible can be difficult. One of \ud83e\udd17 Datasets main goals is to provide a simple way to load a dataset of any format or type. The easiest way to get started is to discover an existing dataset on the Hugging Face Hub - a community-driven collection of datasets for tasks in NLP, computer vision, and audio - and use \ud83e\udd17 Datasets to download and generate the dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/access",
        "Know your dataset",
        "There are two types of dataset objects, a regular Dataset and then an \u2728 IterableDataset \u2728. A Dataset provides fast random access to the rows, and memory-mapping so that loading even large datasets only uses a relatively small amount of device memory. But for really, really big datasets that won\u2019t even fit on disk or in memory, an IterableDataset allows you to access and use the dataset without waiting for it to download completely!"
    ],
    [
        "https://huggingface.co/docs/datasets/use_dataset",
        "Preprocess",
        "In addition to loading datasets, \ud83e\udd17 Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework."
    ],
    [
        "https://huggingface.co/docs/datasets/metrics",
        "Evaluate predictions",
        "Metrics is deprecated in \ud83e\udd17 Datasets. To learn more about how to use metrics, take a look at the library \ud83e\udd17 Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets."
    ],
    [
        "https://huggingface.co/docs/datasets/create_dataset",
        "Create a dataset",
        "Sometimes, you may need to create a dataset if you\u2019re working with your own data. Creating a dataset with \ud83e\udd17 Datasets confers all the advantages of the library to your dataset: fast loading and processing, stream enormous datasets, memory-mapping, and more. You can easily and rapidly create a dataset with \ud83e\udd17 Datasets low-code approaches, reducing the time it takes to start training a model. In many cases, it is as easy as dragging and dropping your data files into a dataset repository on the Hub."
    ],
    [
        "https://huggingface.co/docs/datasets/upload_dataset",
        "Share a dataset to the Hub",
        "The Hub is home to an extensive collection of community-curated and popular research datasets. We encourage you to share your dataset to the Hub to help grow the ML community and accelerate progress for everyone. All contributions are welcome; adding a dataset is just a drag and drop away!"
    ],
    [
        "https://huggingface.co/docs/datasets/how_to",
        "Overview",
        "The how-to guides offer a more comprehensive overview of all the tools \ud83e\udd17 Datasets offers and how to use them. This will help you tackle messier real-world datasets where you may need to manipulate the dataset structure or content to get it ready for training."
    ],
    [
        "https://huggingface.co/docs/datasets/loading",
        "Load",
        "Your data can be stored in various places; they can be on your local machine\u2019s disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored, \ud83e\udd17 Datasets can help you load it."
    ],
    [
        "https://huggingface.co/docs/datasets/process",
        "Process",
        "\ud83e\udd17 Datasets provides many tools for modifying the structure and content of a dataset. These tools are important for tidying up a dataset, creating additional columns, converting between features and formats, and much more."
    ],
    [
        "https://huggingface.co/docs/datasets/stream",
        "Stream",
        "Dataset streaming lets you work with a dataset without downloading it. The data is streamed as you iterate over the dataset. This is especially helpful when:"
    ],
    [
        "https://huggingface.co/docs/datasets/use_with_tensorflow",
        "Use with TensorFlow",
        "This document is a quick introduction to using datasets with TensorFlow, with a particular focus on how to get tf.Tensor objects out of our datasets, and how to stream data from Hugging Face Dataset objects to Keras methods like model.fit()."
    ],
    [
        "https://huggingface.co/docs/datasets/use_with_pytorch",
        "Use with PyTorch",
        "This document is a quick introduction to using datasets with PyTorch, with a particular focus on how to get torch.Tensor objects out of our datasets, and how to use a PyTorch DataLoader and a Hugging Face Dataset with the best performance."
    ],
    [
        "https://huggingface.co/docs/datasets/use_with_jax",
        "Use with JAX",
        "This document is a quick introduction to using datasets with JAX, with a particular focus on how to get jax.Array objects out of our datasets, and how to use them to train JAX models."
    ],
    [
        "https://huggingface.co/docs/datasets/use_with_spark",
        "Use with Spark",
        "This document is a quick introduction to using \ud83e\udd17 Datasets with Spark, with a particular focus on how to load a Spark DataFrame into a Dataset object."
    ],
    [
        "https://huggingface.co/docs/datasets/cache",
        "Cache management",
        "When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows \ud83e\udd17 Datasets to avoid re-downloading or processing the entire dataset every time you use it."
    ],
    [
        "https://huggingface.co/docs/datasets/filesystems",
        "Cloud storage",
        "\ud83e\udd17 Datasets supports access to cloud storage providers through a fsspec FileSystem implementations. You can save and load datasets from any cloud storage in a Pythonic way. Take a look at the following table for some example of supported cloud storage providers:"
    ],
    [
        "https://huggingface.co/docs/datasets/faiss_es",
        "Search index",
        "FAISS and Elasticsearch enables searching for examples in a dataset. This can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task. For example, if you are working on a Open Domain Question Answering task, you may want to only return examples that are relevant to answering your question."
    ],
    [
        "https://huggingface.co/docs/datasets/how_to_metrics",
        "Metrics",
        "Metrics is deprecated in \ud83e\udd17 Datasets. To learn more about how to use metrics, take a look at the library \ud83e\udd17 Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets."
    ],
    [
        "https://huggingface.co/docs/datasets/beam",
        "Beam Datasets",
        "Some datasets are too large to be processed on a single machine. Instead, you can process them with Apache Beam, a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as Apache Flink, Apache Spark, or Google Cloud Dataflow."
    ],
    [
        "https://huggingface.co/docs/datasets/troubleshoot",
        "Troubleshooting",
        "This guide aims to provide you the tools and knowledge required to navigate some common issues. If the suggestions listed in this guide do not cover your such situation, please refer to the Asking for Help section to learn where to find help with your specific issue."
    ],
    [
        "https://huggingface.co/docs/datasets/audio_load",
        "Load audio data",
        "You can load an audio dataset using the Audio feature that automatically decodes and resamples the audio files when you access the examples. Audio decoding is based on the soundfile python package, which uses the libsndfile C library under the hood."
    ],
    [
        "https://huggingface.co/docs/datasets/audio_process",
        "Process audio data",
        "This guide shows specific methods for processing audio datasets. Learn how to:"
    ],
    [
        "https://huggingface.co/docs/datasets/audio_dataset",
        "Create an audio dataset",
        "You can share a dataset with your team or with anyone in the community by creating a dataset repository on the Hugging Face Hub:"
    ],
    [
        "https://huggingface.co/docs/datasets/image_load",
        "Load image data",
        "Image datasets have Image type columns, which contain PIL objects."
    ],
    [
        "https://huggingface.co/docs/datasets/image_process",
        "Process image data",
        "This guide shows specific methods for processing image datasets. Learn how to:"
    ],
    [
        "https://huggingface.co/docs/datasets/image_dataset",
        "Create an image dataset",
        "There are two methods for creating and sharing an image dataset. This guide will show you how to:"
    ],
    [
        "https://huggingface.co/docs/datasets/depth_estimation",
        "Depth estimation",
        "Depth estimation datasets are used to train a model to approximate the relative distance of every pixel in an image from the camera, also known as depth. The applications enabled by these datasets primarily lie in areas like visual machine perception and perception in robotics. Example applications include mapping streets for self-driving cars. This guide will show you how to apply transformations to a depth estimation dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/image_classification",
        "Image classification",
        "Image classification datasets are used to train a model to classify an entire image. There are a wide variety of applications enabled by these datasets such as identifying endangered wildlife species or screening for disease in medical images. This guide will show you how to apply transformations to an image classification dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/semantic_segmentation",
        "Semantic segmentation",
        "Semantic segmentation datasets are used to train a model to classify every pixel in an image. There are a wide variety of applications enabled by these datasets such as background removal from images, stylizing images, or scene understanding for autonomous driving. This guide will show you how to apply transformations to an image segmentation dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/object_detection",
        "Object detection",
        "Object detection models identify something in an image, and object detection datasets are used for applications such as autonomous driving and detecting natural hazards like wildfire. This guide will show you how to apply transformations to an object detection dataset following the tutorial from Albumentations."
    ],
    [
        "https://huggingface.co/docs/datasets/nlp_load",
        "Load text data",
        "This guide shows you how to load text datasets. To learn how to load any type of dataset, take a look at the general loading guide."
    ],
    [
        "https://huggingface.co/docs/datasets/nlp_process",
        "Process text data",
        "This guide shows specific methods for processing text datasets. Learn how to:"
    ],
    [
        "https://huggingface.co/docs/datasets/tabular_load",
        "Load tabular data",
        "A tabular dataset is a generic dataset used to describe any data stored in rows and columns, where the rows represent an example and the columns represent a feature (can be continuous or categorical). These datasets are commonly stored in CSV files, Pandas DataFrames, and in database tables. This guide will show you how to load and create a tabular dataset from:"
    ],
    [
        "https://huggingface.co/docs/datasets/share",
        "Share",
        "At Hugging Face, we are on a mission to democratize good Machine Learning and we believe in the value of open source. That\u2019s why we designed \ud83e\udd17 Datasets so that anyone can share a dataset with the greater ML community. There are currently thousands of datasets in over 100 languages in the Hugging Face Hub, and the Hugging Face team always welcomes new contributions!"
    ],
    [
        "https://huggingface.co/docs/datasets/dataset_card",
        "Create a dataset card",
        "Each dataset should have a dataset card to promote responsible usage and inform users of any potential biases within the dataset. This idea was inspired by the Model Cards proposed by Mitchell, 2018. Dataset cards help users understand a dataset\u2019s contents, the context for using the dataset, how it was created, and any other considerations a user should be aware of."
    ],
    [
        "https://huggingface.co/docs/datasets/repository_structure",
        "Structure your repository",
        "To host and share your dataset, create a dataset repository on the Hugging Face Hub and upload your data files."
    ],
    [
        "https://huggingface.co/docs/datasets/dataset_script",
        "Create a dataset loading script",
        "The dataset loading script is likely not needed if your dataset is in one of the following formats: CSV, JSON, JSON lines, text, images, audio or Parquet. With those formats, you should be able to load your dataset automatically with load_dataset(), as long as your dataset repository has a required structure."
    ],
    [
        "https://huggingface.co/docs/datasets/about_arrow",
        "Datasets \ud83e\udd1d Arrow",
        "Arrow enables large amounts of data to be processed and moved quickly. It is a specific data format that stores data in a columnar memory layout. This provides several significant advantages:"
    ],
    [
        "https://huggingface.co/docs/datasets/about_cache",
        "The cache",
        "The cache is one of the reasons why \ud83e\udd17 Datasets is so efficient. It stores previously downloaded and processed datasets so when you need to use them again, they are reloaded directly from the cache. This avoids having to download a dataset all over again, or reapplying processing functions. Even after you close and start another Python session, \ud83e\udd17 Datasets will reload your dataset directly from the cache!"
    ],
    [
        "https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable",
        "Dataset or IterableDataset",
        "There are two types of dataset objects, a Dataset and an IterableDataset. Whichever type of dataset you choose to use or create depends on the size of the dataset. In general, an IterableDataset is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages, while a Dataset is great for everything else. This page will compare the differences between a Dataset and an IterableDataset to help you pick the right dataset object for you."
    ],
    [
        "https://huggingface.co/docs/datasets/about_dataset_features",
        "Dataset features",
        "Features defines the internal structure of a dataset. It is used to specify the underlying serialization format. What\u2019s more interesting to you though is that Features contains high-level information about everything from the column names and types, to the ClassLabel. You can think of Features as the backbone of a dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/about_dataset_load",
        "Build and load",
        "Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With \ud83e\udd17 Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: load_dataset() to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use."
    ],
    [
        "https://huggingface.co/docs/datasets/about_map_batch",
        "Batch mapping",
        "Combining the utility of Dataset.map() with batch mode is very powerful. It allows you to speed up processing, and freely control the size of the generated dataset."
    ],
    [
        "https://huggingface.co/docs/datasets/about_metrics",
        "All about metrics",
        "Metrics is deprecated in \ud83e\udd17 Datasets. To learn more about how to use metrics, take a look at the library \ud83e\udd17 Evaluate! In addition to metrics, you can find more tools for evaluating models and datasets."
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/main_classes",
        "Main classes",
        "( description: str = <factory>citation: str = <factory>homepage: str = <factory>license: str = <factory>features: Optional = Nonepost_processed: Optional = Nonesupervised_keys: Optional = Nonetask_templates: Optional = Nonebuilder_name: Optional = Nonedataset_name: Optional = Noneconfig_name: Optional = Noneversion: Union = Nonesplits: Optional = Nonedownload_checksums: Optional = Nonedownload_size: Optional = Nonepost_processing_size: Optional = Nonedataset_size: Optional = Nonesize_in_bytes: Optional = None )"
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/builder_classes",
        "Builder classes",
        "\ud83e\udd17 Datasets relies on two main classes during the dataset building process: DatasetBuilder and BuilderConfig."
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/loading_methods",
        "Loading methods",
        "Methods for listing and loading datasets and metrics:"
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/table_classes",
        "Table Classes",
        "Each Dataset object is backed by a PyArrow Table. A Table can be loaded from either the disk (memory mapped) or in memory. Several Table types are available, and they all inherit from table.Table."
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/utilities",
        "Utilities",
        "\ud83e\udd17 Datasets strives to be transparent and explicit about how it works, but this can be quite verbose at times. We have included a series of logging methods which allow you to easily adjust the level of verbosity of the entire library. Currently the default verbosity of the library is set to WARNING."
    ],
    [
        "https://huggingface.co/docs/datasets/package_reference/task_templates",
        "Task templates",
        "The Task API is deprecated in favor of train-eval-index and will be removed in the next major release."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/index",
        "Home",
        "The huggingface_hub library allows you to interact with the Hugging Face Hub, a machine learning platform for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the hundreds of machine learning apps hosted on the Hub. You can also create and share your own models and datasets with the community. The huggingface_hub library provides a simple way to do all these things with Python."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/quick-start",
        "Quickstart",
        "The Hugging Face Hub is the go-to place for sharing machine learning models, demos, datasets, and metrics. huggingface_hub library helps you interact with the Hub without leaving your development environment. You can create and manage repositories easily, download and upload files, and get useful model and dataset metadata from the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/installation",
        "Installation",
        "Before you start, you will need to setup your environment by installing the appropriate packages."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/overview",
        "Overview",
        "In this section, you will find practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/download",
        "Download files",
        "The huggingface_hub library provides functions to download files from the repositories stored on the Hub. You can use these functions independently or integrate them into your own library, making it more convenient for your users to interact with the Hub. This guide will show you how to:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/upload",
        "Upload files",
        "Sharing your files and work is an important aspect of the Hub. The huggingface_hub offers several options for uploading your files to the Hub. You can use these functions independently or integrate them into your library, making it more convenient for your users to interact with the Hub. This guide will show you how to push files:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/cli",
        "Use the CLI",
        "The huggingface_hub Python package comes with a built-in CLI called huggingface-cli. This tool allows you to interact with the Hugging Face Hub directly from a terminal. For example, you can login to your account, create a repository, upload and download files, etc. It also comes with handy features to configure your machine or manage your cache. In this guide, we will have a look at the main features of the CLI and how to use them."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/hf_file_system",
        "HfFileSystem",
        "In addition to the HfApi, the huggingface_hub library provides HfFileSystem, a pythonic fsspec-compatible file interface to the Hugging Face Hub. The HfFileSystem builds of top of the HfApi and offers typical filesystem style operations like cp, mv, ls, du, glob, get_file, and put_file."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/repository",
        "Repository",
        "The Hugging Face Hub is a collection of git repositories. Git is a widely used tool in software development to easily version projects when working collaboratively. This guide will show you how to interact with the repositories on the Hub, especially:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/search",
        "Search",
        "In this tutorial, you will learn how to search models, datasets and spaces on the Hub using huggingface_hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/inference",
        "Inference",
        "Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/inference_endpoints",
        "Inference Endpoints",
        "Inference Endpoints provides a secure production solution to easily deploy any transformers, sentence-transformers, and diffusers models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the Hub. In this guide, we will learn how to programmatically manage Inference Endpoints with huggingface_hub. For more information about the Inference Endpoints product itself, check out its official documentation."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/community",
        "Community Tab",
        "The huggingface_hub library provides a Python interface to interact with Pull Requests and Discussions on the Hub. Visit the dedicated documentation page for a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/collections",
        "Collections",
        "A collection is a group of related items on the Hub (models, datasets, Spaces, papers) that are organized together on the same page. Collections are useful for creating your own portfolio, bookmarking content in categories, or presenting a curated list of items you want to share. Check out this guide to understand in more detail what collections are and how they look on the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/manage-cache",
        "Cache",
        "The Hugging Face Hub cache-system is designed to be the central cache shared across libraries that depend on the Hub. It has been updated in v0.8.0 to prevent re-downloading same files between revisions."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/model-cards",
        "Model Cards",
        "The huggingface_hub library provides a Python interface to create, share, and update Model Cards. Visit the dedicated documentation page for a deeper view of what Model Cards on the Hub are, and how they work under the hood."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/manage-spaces",
        "Manage your Space",
        "In this guide, we will see how to manage your Space runtime (secrets, hardware, and storage) using huggingface_hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/integrations",
        "Integrate a library",
        "The Hugging Face Hub makes hosting and sharing models with the community easy. It supports dozens of libraries in the Open Source ecosystem. We are always working on expanding this support to push collaborative Machine Learning forward. The huggingface_hub library plays a key role in this process, allowing any Python script to easily push and load files."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/guides/webhooks_server",
        "Webhooks server",
        "Webhooks are a foundation for MLOps-related features. They allow you to listen for new changes on specific repos or to all repos belonging to particular users/organizations you\u2019re interested in following. This guide will explain how to leverage huggingface_hub to create a server listening to webhooks and deploy it to a Space. It assumes you are familiar with the concept of webhooks on the Huggingface Hub. To learn more about webhooks themselves, you can read this guide first."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http",
        "Git vs HTTP paradigm",
        "The huggingface_hub library is a library for interacting with the Hugging Face Hub, which is a collections of git-based repositories (models, datasets or Spaces). There are two main ways to access the Hub using huggingface_hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/overview",
        "Overview",
        "This section contains an exhaustive and technical description of huggingface_hub classes and methods."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/login",
        "Login and logout",
        "The huggingface_hub library allows users to programmatically login and logout the machine to the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables",
        "Environment variables",
        "huggingface_hub can be configured using environment variables."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/repository",
        "Managing local and online repositories",
        "The Repository class is a helper class that wraps git and git-lfs commands. It provides tooling adapted for managing repositories which can be very large."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/hf_api",
        "Hugging Face Hub API",
        "Below is the documentation for the HfApi class, which serves as a Python wrapper for the Hugging Face Hub\u2019s API."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/file_download",
        "Downloading files",
        "( repo_id: strfilename: strsubfolder: Optional = Nonerepo_type: Optional = Nonerevision: Optional = Nonelibrary_name: Optional = Nonelibrary_version: Optional = Nonecache_dir: Union = Nonelocal_dir: Union = Nonelocal_dir_use_symlinks: Union = 'auto'user_agent: Union = Noneforce_download: bool = Falseforce_filename: Optional = Noneproxies: Optional = Noneetag_timeout: float = 10resume_download: bool = Falsetoken: Union = Nonelocal_files_only: bool = Falseheaders: Optional = Nonelegacy_cache_layout: bool = Falseendpoint: Optional = None )"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/mixins",
        "Mixins & serialization methods",
        "The huggingface_hub library offers a range of mixins that can be used as a parent class for your objects, in order to provide simple uploading and downloading functions. Check out our integration guide to learn how to integrate any ML framework with the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/inference_client",
        "Inference Client",
        "Inference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive, running on a dedicated server can be an interesting option. The huggingface_hub library provides an easy way to call a service that runs inference for hosted models. There are several services you can connect to:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/inference_endpoints",
        "Inference Endpoints",
        "Inference Endpoints provides a secure production solution to easily deploy models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the Hub. This page is a reference for huggingface_hub\u2019s integration with Inference Endpoints. For more information about the Inference Endpoints product, check out its official documentation."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/hf_file_system",
        "HfFileSystem",
        "The HfFileSystem class provides a pythonic file interface to the Hugging Face Hub based on fsspec."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/utilities",
        "Utilities",
        "The huggingface_hub package exposes a logging utility to control the logging level of the package itself. You can import it as such:"
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/community",
        "Discussions and Pull Requests",
        "Check the HfApi documentation page for the reference of methods enabling interaction with Pull Requests and Discussions on the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/cache",
        "Cache-system reference",
        "The caching system was updated in v0.8.0 to become the central cache-system shared across libraries that depend on the Hub. Read the cache-system guide for a detailed presentation of caching at HF."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/cards",
        "Repo Cards and Repo Card Data",
        "The huggingface_hub library provides a Python interface to create, share, and update Model/Dataset Cards. Visit the dedicated documentation page for a deeper view of what Model Cards on the Hub are, and how they work under the hood. You can also check out our Model Cards guide to get a feel for how you would use these utilities in your own projects."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/space_runtime",
        "Space runtime",
        "Check the HfApi documentation page for the reference of methods to manage your Space on the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/collections",
        "Collections",
        "Check out the HfApi documentation page for the reference of methods to manage your Space on the Hub."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/tensorboard",
        "TensorBoard logger",
        "TensorBoard is a visualization toolkit for machine learning experimentation. TensorBoard allows tracking and visualizing metrics such as loss and accuracy, visualizing the model graph, viewing histograms, displaying images and much more. TensorBoard is well integrated with the Hugging Face Hub. The Hub automatically detects TensorBoard traces (such as tfevents) when pushed to the Hub which starts an instance to visualize them. To get more information about TensorBoard integration on the Hub, check out this guide."
    ],
    [
        "https://huggingface.co/docs/huggingface_hub/package_reference/webhooks_server",
        "Webhooks server",
        "Webhooks are a foundation for MLOps-related features. They allow you to listen for new changes on specific repos or to all repos belonging to particular users/organizations you\u2019re interested in following. To learn more about webhooks on the Huggingface Hub, you can read the Webhooks guide."
    ],
    [
        "https://huggingface.co/docs/huggingface.js/index",
        "\ud83e\udd17 Hugging Face JS Libraries",
        ""
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/README",
        "Use Inference Endpoints",
        "A Typescript powered wrapper for the Hugging Face Inference Endpoints API. Learn more about Inference Endpoints at Hugging Face. It works with both Inference API (serverless) and Inference Endpoints (dedicated)."
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/modules",
        "API reference",
        "\u01ac AudioClassificationArgs: BaseArgs & { data: Blob | ArrayBuffer }"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/classes/HfInference",
        "HfInference",
        "TaskWithNoAccessToken"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/classes/HfInferenceEndpoint",
        "HfInferenceEndpoint",
        "TaskWithNoAccessTokenNoModel"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/classes/InferenceOutputError",
        "InferenceOutputError",
        "TypeError"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioClassificationOutputValue",
        "AudioClassificationOutputValue",
        "\u2022 label: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/AudioToAudioOutputValue",
        "AudioToAudioOutputValue",
        "\u2022 blob: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/AutomaticSpeechRecognitionOutput",
        "AutomaticSpeechRecognitionOutput",
        "\u2022 text: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/BaseArgs",
        "BaseArgs",
        "\u2022 Optional accessToken: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ConversationalOutput",
        "BaseArgs",
        "\u2022 Optional accessToken: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/DocumentQuestionAnsweringOutput",
        "DocumentQuestionAnsweringOutput",
        "\u2022 answer: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageClassificationOutputValue",
        "ImageClassificationOutputValue",
        "\u2022 label: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageSegmentationOutputValue",
        "ImageSegmentationOutputValue",
        "\u2022 label: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ImageToTextOutput",
        "ImageToTextOutput",
        "\u2022 generated_text: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ObjectDetectionOutputValue",
        "ObjectDetectionOutputValue",
        "\u2022 box: Object"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/Options",
        "Options",
        "\u2022 Optional dont_load_model: boolean"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/QuestionAnsweringOutput",
        "QuestionAnsweringOutput",
        "\u2022 answer: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/SummarizationOutput",
        "SummarizationOutput",
        "\u2022 summary_text: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TableQuestionAnsweringOutput",
        "TableQuestionAnsweringOutput",
        "\u2022 aggregator: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationOutput",
        "TextGenerationOutput",
        "Outputs for Text Generation inference"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamBestOfSequence",
        "TextGenerationStreamBestOfSequence",
        "\u2022 finish_reason: TextGenerationStreamFinishReason"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamDetails",
        "TextGenerationStreamDetails",
        "\u2022 Optional best_of_sequences: TextGenerationStreamBestOfSequence[]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamOutput",
        "TextGenerationStreamOutput",
        "\u2022 details: null | TextGenerationStreamDetails"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamPrefillToken",
        "TextGenerationStreamPrefillToken",
        "\u2022 id: number"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TextGenerationStreamToken",
        "TextGenerationStreamToken",
        "\u2022 id: number"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TokenClassificationOutputValue",
        "TokenClassificationOutputValue",
        "\u2022 end: number"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/TranslationOutputValue",
        "TranslationOutputValue",
        "\u2022 translation_text: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/VisualQuestionAnsweringOutput",
        "VisualQuestionAnsweringOutput",
        "\u2022 answer: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotClassificationOutputValue",
        "ZeroShotClassificationOutputValue",
        "\u2022 labels: string[]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/inference/interfaces/ZeroShotImageClassificationOutputValue",
        "ZeroShotImageClassificationOutputValue",
        "\u2022 label: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/README",
        "Interact with the Hub",
        "Official utilities to use the Hugging Face Hub API."
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/modules",
        "API Reference",
        "\u01ac AccessToken: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/classes/HubApiError",
        "HubApiError",
        "Error thrown when an API call to the Hugging Face Hub fails."
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/classes/InvalidApiResponseFormatError",
        "InvalidApiResponseFormatError",
        "Error"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/AuthInfo",
        "AuthInfo",
        "\u2022 Optional accessToken: Object"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitDeletedEntry",
        "CommitDeletedEntry",
        "\u2022 operation: \"delete\""
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitFile",
        "CommitFile",
        "\u2022 content: ContentSource"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitOutput",
        "CommitOutput",
        "\u2022 commit: Object"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/CommitParams",
        "CommitParams",
        "\u2022 Optional abortSignal: AbortSignal"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/Credentials",
        "Credentials",
        "\u2022 accessToken: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/DatasetEntry",
        "DatasetEntry",
        "\u2022 downloads: number"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/FileDownloadInfoOutput",
        "FileDownloadInfoOutput",
        "\u2022 downloadLink: null | string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/ListFileEntry",
        "ListFileEntry",
        "\u2022 Optional lastCommit: Object"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/ModelEntry",
        "ModelEntry",
        "\u2022 downloads: number"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/OAuthResult",
        "OAuthResult",
        "\u2022 accessToken: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/RepoId",
        "RepoId",
        "\u2022 name: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/SafetensorsIndexJson",
        "SafetensorsIndexJson",
        "\u2022 Optional dtype: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceConfig",
        "SpaceResourceConfig",
        "\u2022 Optional is_custom: boolean"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceResourceRequirement",
        "SpaceResourceRequirement",
        "\u2022 Optional cpu: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/SpaceRuntime",
        "SpaceRuntime",
        "\u2022 Optional errorMessage: string"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/TensorInfo",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIApp",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIOrg",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/hub/interfaces/WhoAmIUser",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/agents/README",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/agents/modules",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/huggingface.js/agents/classes/HfAgent",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/index",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/installation",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/pipelines",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/custom_usage",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/vanilla-js",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/react",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/next",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/browser-extension",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/electron",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/tutorials/node",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/guides/private",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/guides/node-audio-processing",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/transformers",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/pipelines",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/models",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/tokenizers",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/processors",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/configs",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/transformers.js/api/env",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/index",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/quicktour",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/detailed_parameters",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/parallelism",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/usage",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/api-inference/faq",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/index",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/security",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/supported_tasks",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/api_reference",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/autoscaling",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/pricing",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/support",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/faq",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/access",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/create_endpoint",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/test_endpoint",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/update_endpoint",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/advanced",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/private_link",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/custom_dependencies",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/custom_handler",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/custom_container",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/logs",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/metrics",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/change_organization",
        "TensorInfo",
        "\u2022 data_offsets: [number, number]"
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/guides/pause_endpoint",
        "Pause and Resume your Endpoint",
        "You can pause & resume endpoints to save cost and configurations. Please note that if your endpoint is in a failed state, you will need to create a new endpoint. To pause/resume your endpoint, navigate to the \u201coverview\u201d tab and click the button at top right corner, which will show \u201cPause endpoint\u201d to pause, or \u201cResume endpoint\u201d to reactivate the paused endpoint."
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/others/runtime",
        "Inference Endpoints Version",
        "Hugging Face Inference Endpoints comes with a default serving container which is used for all supported Transformers and Sentence-Transformers tasks and for custom inference handler and implement batching. Below you will find information about the installed packages and versions used."
    ],
    [
        "https://huggingface.co/docs/inference-endpoints/others/serialization",
        "Serialization & Deserialization for Requests",
        "Hugging Face Inference Endpount comes with a default serving container which is used for all supported Transformers and Sentence-Transformers tasks and for custom inference handler. The serving container takes care of serialization and deserialization of the request and response payloads based on the content-type and accept headers of the request. That means that when you send a request with a JSON body and a content-type: application/json header, the serving container will deserialize the JSON payload into a Python dictionary and pass it to the inference handler and if you send a request with a accept: image/png header, the serving container will seralize the response from the task/custom handler into a image."
    ],
    [
        "https://huggingface.co/docs/peft/index",
        "\ud83e\udd17 PEFT",
        "\ud83e\udd17 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model\u2019s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware."
    ],
    [
        "https://huggingface.co/docs/peft/quicktour",
        "Quicktour",
        "PEFT offers parameter-efficient methods for finetuning large pretrained models. The traditional paradigm is to finetune all of a model\u2019s parameters for each downstream task, but this is becoming exceedingly costly and impractical because of the enormous number of parameters in models today. Instead, it is more efficient to train a smaller number of prompt parameters or use a reparametrization method like low-rank adaptation (LoRA) to reduce the number of trainable parameters."
    ],
    [
        "https://huggingface.co/docs/peft/install",
        "Installation",
        "Before you start, you will need to setup your environment, install the appropriate packages, and configure \ud83e\udd17 PEFT. \ud83e\udd17 PEFT is tested on Python 3.8+."
    ],
    [
        "https://huggingface.co/docs/peft/tutorial/peft_model_config",
        "Configurations and models",
        "The sheer size of today\u2019s large pretrained models - which commonly have billions of parameters - present a significant training challenge because they require more storage space and more computational power to crunch all those calculations. You\u2019ll need access to powerful GPUs or TPUs to train these large pretrained models which is expensive, not widely accessible to everyone, not environmentally friendly, and not very practical. PEFT methods address many of these challenges. There are several types of PEFT methods (soft prompting, matrix decomposition, adapters), but they all focus on the same thing, reduce the number of trainable parameters. This makes it more accessible to train and store large models on consumer hardware."
    ],
    [
        "https://huggingface.co/docs/peft/tutorial/peft_integrations",
        "Integrations",
        "PEFT\u2019s practical benefits extends to other Hugging Face libraries like Diffusers and Transformers. One of the main benefits of PEFT is that an adapter file generated by a PEFT method is a lot smaller than the original model, which makes it super easy to manage and use multiple adapters. You can use one pretrained base model for multiple tasks by simply loading a new adapter finetuned for the task you\u2019re solving. Or you can combine multiple adapters with a text-to-image diffusion model to create new effects."
    ],
    [
        "https://huggingface.co/docs/peft/task_guides/prompt_based_methods",
        "Prompt-based methods",
        "A prompt can describe a task or provide an example of a task you want the model to learn. Instead of manually creating these prompts, soft prompting methods add learnable parameters to the input embeddings that can be optimized for a specific task while keeping the pretrained model\u2019s parameters frozen. This makes it both faster and easier to finetune large language models (LLMs) for new downstream tasks."
    ],
    [
        "https://huggingface.co/docs/peft/task_guides/lora_based_methods",
        "LoRA methods",
        "A popular way to efficiently train large models is to insert (typically in the attention blocks) smaller trainable matrices that are a low-rank decomposition of the delta weight matrix to be learnt during finetuning. The pretrained model\u2019s original weight matrix is frozen and only the smaller matrices are updated during training. This reduces the number of trainable parameters, reducing memory usage and training time which can be very expensive for large models."
    ],
    [
        "https://huggingface.co/docs/peft/task_guides/ia3",
        "IA3",
        "IA3 multiplies the model\u2019s activations (the keys and values in the self-attention and encoder-decoder attention blocks, and the intermediate activation of the position-wise feedforward network) by three learned vectors. This PEFT method introduces an even smaller number of trainable parameters than LoRA which introduces weight matrices instead of vectors. The original model\u2019s parameters are kept frozen and only these vectors are updated. As a result, it is faster, cheaper and more efficient to finetune for a new downstream task."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/quantization",
        "Quantization",
        "Quantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs). There are several ways to quantize a model including:"
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/lora",
        "LoRA",
        "LoRA is low-rank decomposition method to reduce the number of trainable parameters which speeds up finetuning large models and uses less memory. In PEFT, using LoRA is as easy as setting up a LoraConfig and wrapping it with get_peft_model() to create a trainable PeftModel."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/custom_models",
        "Custom models",
        "Some fine-tuning techniques, such as prompt tuning, are specific to language models. That means in \ud83e\udd17 PEFT, it is assumed a \ud83e\udd17 Transformers model is being used. However, other fine-tuning techniques - like LoRA - are not restricted to specific model types."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/low_level_api",
        "Adapter injection",
        "With PEFT, you can inject trainable adapters into any torch module which allows you to use adapter methods without relying on the modeling classes in PEFT. Currently, PEFT supports injecting LoRA, AdaLoRA, and IA3 into models because for these adapters, inplace modification of the model is sufficient for finetuning it."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/mixed_models",
        "Mixed adapter types",
        "Normally, it isn\u2019t possible to mix different adapter types in \ud83e\udd17 PEFT. You can create a PEFT model with two different LoRA adapters (which can have different config options), but it is not possible to combine a LoRA and LoHa adapter. With PeftMixedModel however, this works as long as the adapter types are compatible. The main purpose of allowing mixed adapter types is to combine trained adapters for inference. While it is possible to train a mixed adapter model, this has not been tested and is not recommended."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/contributing",
        "Contribute to PEFT",
        "We are happy to accept contributions to PEFT. If you plan to contribute, please read this to make the process as smooth as possible."
    ],
    [
        "https://huggingface.co/docs/peft/developer_guides/troubleshooting",
        "Troubleshooting",
        "If you encounter any issue when using PEFT, please check the following list of common issues and their solutions."
    ],
    [
        "https://huggingface.co/docs/peft/accelerate/deepspeed-zero3-offload",
        "Troubleshooting",
        "If you encounter any issue when using PEFT, please check the following list of common issues and their solutions."
    ],
    [
        "https://huggingface.co/docs/peft/accelerate/fsdp",
        "Fully Sharded Data Parallel",
        "Fully sharded data parallel (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes."
    ],
    [
        "https://huggingface.co/docs/peft/conceptual_guides/adapter",
        "Adapters",
        "Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates \u2206W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources."
    ],
    [
        "https://huggingface.co/docs/peft/conceptual_guides/prompting",
        "Soft prompts",
        "Training large pretrained language models is very time-consuming and compute-intensive. As they continue to grow in size, there is increasing interest in more efficient training methods such as prompting. Prompting primes a frozen pretrained model for a specific downstream task by including a text prompt that describes the task or even demonstrates an example of the task. With prompting, you can avoid fully training a separate model for each downstream task, and use the same frozen pretrained model instead. This is a lot easier because you can use the same model for several different tasks, and it is significantly more efficient to train and store a smaller set of prompt parameters than to train all the model\u2019s parameters."
    ],
    [
        "https://huggingface.co/docs/peft/conceptual_guides/ia3",
        "IA3",
        "This conceptual guide gives a brief overview of IA3, a parameter-efficient fine tuning technique that is intended to improve over LoRA."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/auto_class",
        "AutoPeftModel",
        "The AutoPeftModel classes loads the appropriate PEFT model for the task type by automatically inferring it from the configuration file. They are designed to quickly and easily load a PEFT model in a single line of code without having to worry about which exact model class you need or manually loading a PeftConfig."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/peft_model",
        "PEFT model",
        "PeftModel is the base model class for specifying the base Transformer model and configuration to apply a PEFT method to. The base PeftModel contains methods for loading and saving models from the Hub."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/peft_types",
        "PEFT types",
        "PeftType includes the supported adapters in PEFT, and TaskType includes PEFT-supported tasks."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/config",
        "Configuration",
        "PeftConfigMixin is the base configuration class for storing the adapter configuration of a PeftModel, and PromptLearningConfig is the base configuration class for soft prompt methods (p-tuning, prefix tuning, and prompt tuning). These base classes contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/tuners",
        "Tuner",
        "A tuner (or adapter) is a module that can be plugged into a torch.nn.Module. BaseTuner base class for other tuners and provides shared methods and attributes for preparing an adapter configuration and replacing a target module with the adapter module. BaseTunerLayer is a base class for adapter layers. It offers methods and attributes for managing adapters such as activating and disabling adapters."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/adalora",
        "AdaLoRA",
        "AdaLoRA is a method for optimizing the number of trainable parameters to assign to weight matrices and layers, unlike LoRA, which distributes parameters evenly across all modules. More parameters are budgeted for important weight matrices and layers while less important ones receive fewer parameters."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/ia3",
        "IA3",
        "Infused Adapter by Inhibiting and Amplifying Inner Activations, or IA3, is a method that adds three learned vectors to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/llama_adapter",
        "Llama-Adapter",
        "Llama-Adapter is a PEFT method specifically designed for turning Llama into an instruction-following model. The Llama model is frozen and only a set of adaptation prompts prefixed to the input instruction tokens are learned. Since randomly initialized modules inserted into the model can cause the model to lose some of its existing knowledge, Llama-Adapter uses zero-initialized attention with zero gating to progressively add the instructional prompts to the model."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/loha",
        "LoHa",
        "Low-Rank Hadamard Product (LoHa), is similar to LoRA except it approximates the large weight matrix with more low-rank matrices and combines them with the Hadamard product. This method is even more parameter-efficient than LoRA and achieves comparable performance."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/lokr",
        "LoKr",
        "Low-Rank Kronecker Product (LoKr), is a LoRA-variant method that approximates the large weight matrix with two low-rank matrices and combines them with the Kronecker product. LoKr also provides an optional third low-rank matrix to provide better control during fine-tuning."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/lora",
        "LoRA",
        "Low-Rank Adaptation (LoRA) is a PEFT method that decomposes a large matrix into two smaller low-rank matrices in the attention layers. This drastically reduces the number of parameters that need to be fine-tuned."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/adapter_utils",
        "LyCORIS",
        "LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion) are LoRA-like matrix decomposition adapters that modify the cross-attention layer of the UNet. The LoHa and LoKr methods inherit from the Lycoris classes here."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/multitask_prompt_tuning",
        "Multitask Prompt Tuning",
        "Multitask prompt tuning decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/oft",
        "OFT",
        "Orthogonal Finetuning (OFT) is a method developed for adapting text-to-image diffusion models. It works by reparameterizing the pretrained weight matrices with it\u2019s orthogonal matrix to preserve information in the pretrained model. To reduce the number of parameters, OFT introduces a block-diagonal structure in the orthogonal matrix."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/poly",
        "Polytropon",
        "Polytropon is a multitask model with a number of different LoRA adapters in it\u2019s \u201cinventory\u201d. The model learns the correct combination of adapters from the inventory with a routing function to choose the best subset of modules for a specific task. PEFT also supports Multi-Head Adapter Routing (MHR) for Polytropon which builds on and improves the routing function by combining the adapter heads more granularly. The adapter heads are separated into disjoint blocks and a different routing function is learned for each one, allowing for more expressivity."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/p_tuning",
        "P-tuning",
        "P-tuning adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/prefix_tuning",
        "Prefix tuning",
        "Prefix tuning prefixes a series of task-specific vectors to the input sequence that can be learned while keeping the pretrained model frozen. The prefix parameters are inserted in all of the model layers."
    ],
    [
        "https://huggingface.co/docs/peft/package_reference/prompt_tuning",
        "Prompt tuning",
        "Prompt tuning adds task-specific prompts to the input, and these prompt parameters are updated independently of the pretrained model parameters which are frozen."
    ],
    [
        "https://huggingface.co/docs/accelerate/index",
        "\ud83e\udd17 Accelerate",
        "\ud83e\udd17 Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable."
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/install",
        "Installation",
        "Before you start, you will need to setup your environment, install the appropriate packages, and configure \ud83e\udd17 Accelerate. \ud83e\udd17 Accelerate is tested on Python 3.8+."
    ],
    [
        "https://huggingface.co/docs/accelerate/quicktour",
        "Quicktour",
        "There are many ways to launch and run your code depending on your training environment (torchrun, DeepSpeed, etc.) and available hardware. Accelerate offers a unified interface for launching and training on different distributed setups, allowing you to focus on your PyTorch training code instead of the intricacies of adapting your code to these different setups. This allows you to easily scale your PyTorch code for training and inference on distributed setups with hardware like GPUs and TPUs. Accelerate also provides Big Model Inference to make loading and running inference with really large models that usually don\u2019t fit in memory more accessible."
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/overview",
        "Overview",
        "Welcome to the \ud83e\udd17 Accelerate tutorials! These introductory guides will help catch you up to speed on working with \ud83e\udd17 Accelerate. You\u2019ll learn how to modify your code to have it work with the API seamlessly, how to launch your script properly, and more!"
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/migration",
        "Add Accelerate to your code",
        "Each distributed training framework has their own way of doing things which can require writing a lot of custom code to adapt it to your PyTorch training code and training environment. Accelerate offers a friendly way to interface with these distributed training frameworks without having to learn the specific details of each one. Accelerate takes care of those details for you, so you can focus on the training code and scale it to any distributed training environment."
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/launch",
        "Launching distributed code",
        "In the previous tutorial, you were introduced to how to modify your current training script to use \ud83e\udd17 Accelerate. The final version of that code is shown below:"
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/notebook",
        "Launching distributed training from Jupyter Notebooks",
        "This tutorial teaches you how to fine tune a computer vision model with \ud83e\udd17 Accelerate from a Jupyter Notebook on a distributed system. You will also learn how to setup a few requirements needed for ensuring your environment is configured properly, your data has been prepared properly, and finally how to launch training."
    ],
    [
        "https://huggingface.co/docs/accelerate/basic_tutorials/troubleshooting",
        "Troubleshoot",
        "This guide provides solutions to some issues you might encounter when using Accelerate. Not all errors are covered because Accelerate is an active library that is continuously evolving and there are many different use cases and distributed training setups. If the solutions described here don\u2019t help with your specific error, please take a look at the Ask for help section to learn where and how to get help."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/explore",
        "Start Here!",
        "Please use the interactive tool below to help you get started with learning about a particular feature of \ud83e\udd17 Accelerate and how to utilize it! It will provide you with a code diff, an explanation towards what is going on, as well as provide you with some useful links to explore more within the documentation!"
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/training_zoo",
        "Example Zoo",
        "Below contains a non-exhaustive list of tutorials and scripts showcasing \ud83e\udd17 Accelerate"
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/big_modeling",
        "Big Model Inference",
        "One of the biggest advancements \ud83e\udd17 Accelerate provides is the concept of large model inference wherein you can perform inference on models that cannot fully fit on your graphics card."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/model_size_estimator",
        "Model memory estimator",
        "One very difficult aspect when exploring potential models to use on your machine is knowing just how big of a model will fit into memory with your current graphics card (such as loading the model onto CUDA)."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/quantization",
        "Model quantization",
        "\ud83e\udd17 Accelerate brings bitsandbytes quantization to your model. You can now load any pytorch model in 8-bit or 4-bit with a few lines of code."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/distributed_inference",
        "Distributed inference",
        "Distributed inference can fall into three brackets:"
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/local_sgd",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/checkpoint",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/tracking",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/mps",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/low_precision_training",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/deepspeed",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/fsdp",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/megatron_lm",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/sagemaker",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/usage_guides/ipex",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/internal_mechanism",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/big_model_inference",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/performance",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/deferring_execution",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/gradient_synchronization",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/low_precision_training",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/concept_guides/training_tpu",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/accelerator",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/state",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/cli",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/torch_wrappers",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/tracking",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/launchers",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/deepspeed",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/logging",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/big_modeling",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/kwargs",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/utilities",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/megatron_lm",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/accelerate/package_reference/fsdp",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum/index",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum/installation",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum/quicktour",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum/notebooks",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum/concept_guides/quantization",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/index",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/installation",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/quickstart",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/overview",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/notebooks",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/stable_diffusion",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/llama2-13b-chatbot",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_llama_7b",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/tutorials/sentence_transformers",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/overview",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/setup_aws_instance",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/sagemaker",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/cache_system",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/fine_tune",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/distributed_training",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/export_model",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/models",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/guides/pipelines",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/community/contributing",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/package_reference/trainer",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/package_reference/distributed",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/package_reference/export",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/package_reference/modeling",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/optimum-neuron/benchmarks/inferentia-llama2",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/tokenizers/index",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/tokenizers/quicktour",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/tokenizers/installation",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/tokenizers/pipeline",
        "Gradient accumulation",
        "Gradient accumulation is a technique where you can train on bigger batch sizes than your machine would normally be able to fit into memory. This is done by accumulating gradients over several batches, and only stepping the optimizer after a certain number of batches have been performed."
    ],
    [
        "https://huggingface.co/docs/tokenizers/components",
        "Components",
        "A Normalizer is in charge of pre-processing the input string in order to normalize it as relevant for a given use case. Some common examples of normalization are the Unicode normalization algorithms (NFD, NFKD, NFC & NFKC), lowercasing etc\u2026 The specificity of tokenizers is that we keep track of the alignment while normalizing. This is essential to allow mapping from the generated tokens back to the input text."
    ],
    [
        "https://huggingface.co/docs/tokenizers/training_from_memory",
        "Training from memory",
        "For all the examples listed below, we\u2019ll use the same Tokenizer and Trainer, built as following:"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/input-sequences",
        "Input Sequences",
        "A str that represents an input sequence"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/encode-inputs",
        "Encode Inputs",
        "Represents a textual input for encoding. Can be either:"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/tokenizer",
        "Tokenizer",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/encoding",
        "Encoding",
        "The Encoding represents the output of a Tokenizer."
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/added-tokens",
        "Added Tokens",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/models",
        "Models",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/normalizers",
        "Normalizers",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/pre-tokenizers",
        "Pre-tokenizers",
        "BertPreTokenizer"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/post-processors",
        "Post-processors",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/trainers",
        "Trainers",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/decoders",
        "Decoders",
        "Parameters"
    ],
    [
        "https://huggingface.co/docs/tokenizers/api/visualizer",
        "Visualizer",
        "( tokenizer: Tokenizerdefault_to_notebook: bool = Trueannotation_converter: typing.Union[typing.Callable[[typing.Any], tokenizers.tools.visualizer.Annotation], NoneType] = None )"
    ],
    [
        "https://huggingface.co/docs/evaluate/index",
        "\ud83e\udd17 Evaluate",
        "A library for easily evaluating machine learning models and datasets."
    ],
    [
        "https://huggingface.co/docs/evaluate/installation",
        "Installation",
        "You should install \ud83e\udd17 Evaluate in a virtual environment to keep everything neat and tidy."
    ],
    [
        "https://huggingface.co/docs/evaluate/a_quick_tour",
        "A quick tour",
        "There are different aspects of a typical machine learning pipeline that can be evaluated and for each aspect \ud83e\udd17 Evaluate provides a tool:"
    ],
    [
        "https://huggingface.co/docs/evaluate/choosing_a_metric",
        "Choosing the right metric",
        "There is no \u201cone size fits all\u201d approach to choosing an evaluation metric, but some good guidelines to keep in mind are:"
    ],
    [
        "https://huggingface.co/docs/evaluate/creating_and_sharing",
        "Adding new evaluations",
        "Also make sure your Hugging Face token is registered so you can connect to the Hugging Face Hub:"
    ],
    [
        "https://huggingface.co/docs/evaluate/base_evaluator",
        "Using the evaluator",
        "Currently supported tasks are:"
    ],
    [
        "https://huggingface.co/docs/evaluate/custom_evaluator",
        "Using the evaluator with custom pipelines",
        "First we need to train a model. We\u2019ll train a simple text classifier on the IMDb dataset, so let\u2019s start by downloading the dataset:"
    ],
    [
        "https://huggingface.co/docs/evaluate/evaluation_suite",
        "Creating an EvaluationSuite",
        "The EvaluationSuite provides a way to compose any number of (evaluator, dataset, metric) tuples as a SubTask to evaluate a model on a collection of several evaluation tasks. See the evaluator documentation for a list of currently supported tasks."
    ],
    [
        "https://huggingface.co/docs/evaluate/transformers_integrations",
        "Transformers",
        "The metrics in evaluate can be easily integrated with the Trainer. The Trainer accepts a compute_metrics keyword argument that passes a function to compute metrics. One can specify the evaluation interval with evaluation_strategy in the TrainerArguments, and based on that, the model is evaluated accordingly, and the predictions and labels passed to compute_metrics."
    ],
    [
        "https://huggingface.co/docs/evaluate/keras_integrations",
        "Keras and Tensorflow",
        "Suppose we want to keep track of model metrics while a model is training. We can use a Callback in order to calculate this metric during training, after an epoch ends."
    ],
    [
        "https://huggingface.co/docs/evaluate/sklearn_integrations",
        "scikit-learn",
        "The metrics in evaluate can be easily integrated with an Scikit-Learn estimator or pipeline."
    ],
    [
        "https://huggingface.co/docs/evaluate/types_of_evaluations",
        "Types of evaluations",
        "Here are the types of evaluations that are currently supported with a few examples for each:"
    ],
    [
        "https://huggingface.co/docs/evaluate/considerations",
        "Considerations for model evaluation",
        "Here are some things to keep in mind when evaluating your model using the \ud83e\udd17 Evaluate library:"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/main_classes",
        "Main classes",
        "( description: strcitation: strfeatures: typing.Union[datasets.features.features.Features, typing.List[datasets.features.features.Features]]inputs_description: str = <factory>homepage: str = <factory>license: str = <factory>codebase_urls: typing.List[str] = <factory>reference_urls: typing.List[str] = <factory>streamable: bool = Falseformat: typing.Optional[str] = Nonemodule_type: str = 'metric'module_name: typing.Optional[str] = Noneconfig_name: typing.Optional[str] = Noneexperiment_id: typing.Optional[str] = None )"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/loading_methods",
        "Loading methods",
        "( module_type = Noneinclude_community = Truewith_details = False )"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/saving_methods",
        "Saving methods",
        "( path_or_file**data )"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/hub_methods",
        "Hub methods",
        "( model_id: strtask_type: strdataset_type: strdataset_name: strmetric_type: strmetric_name: strmetric_value: floattask_name: str = Nonedataset_config: str = Nonedataset_split: str = Nonedataset_revision: str = Nonedataset_args: typing.Dict[str, int] = Nonemetric_config: str = Nonemetric_args: typing.Dict[str, int] = Noneoverwrite: bool = False )"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/evaluator_classes",
        "Evaluator classes",
        "The main entry point for using the evaluator:"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/visualization_methods",
        "Visualization methods",
        "( datamodel_namesinvert_range = []config = Nonefig = None )"
    ],
    [
        "https://huggingface.co/docs/evaluate/package_reference/logging_methods",
        "Logging methods",
        "To change the level of verbosity, use one of the direct setters. For instance, here is how to change the verbosity to the INFO level:"
    ],
    [
        "https://huggingface.co/docs/datasets-server/index",
        "\ud83e\udd17 Dataset viewer",
        "The dataset viewer is a lightweight web API for visualizing and exploring all types of datasets - computer vision, speech, text, and tabular - stored on the Hugging Face Hub."
    ],
    [
        "https://huggingface.co/docs/datasets-server/quick_start",
        "Quickstart",
        "In this quickstart, you\u2019ll learn how to use the dataset viewer\u2019s REST API to:"
    ],
    [
        "https://huggingface.co/docs/datasets-server/analyze_data",
        "Analyze a dataset on the Hub",
        "In the Quickstart, you were introduced to various endpoints for interacting with datasets on the Hub. One of the most useful ones is the /parquet endpoint, which allows you to get a dataset stored on the Hub and analyze it. This is a great way to explore the dataset, and get a better understanding of it\u2019s contents."
    ],
    [
        "https://huggingface.co/docs/datasets-server/valid",
        "Check dataset validity",
        "Before you download a dataset from the Hub, it is helpful to know if a specific dataset you\u2019re interested in is available. The dataset viewer provides the /is-valid endpoint to check if a specific dataset works without any errors."
    ],
    [
        "https://huggingface.co/docs/datasets-server/splits",
        "List splits and configurations",
        "Datasets typically have splits and may also have configurations. A split is a subset of the dataset, like train and test, that are used during different stages of training and evaluating a model. A configuration is a sub-dataset contained within a larger dataset. Configurations are especially common in multilingual speech datasets where there may be a different configuration for each language. If you\u2019re interested in learning more about splits and configurations, check out the conceptual guide on \u201cSplits and configurations\u201d!"
    ],
    [
        "https://huggingface.co/docs/datasets-server/info",
        "Get dataset information",
        "The dataset viewer provides an /info endpoint for exploring the general information about dataset, including such fields as description, citation, homepage, license and features."
    ],
    [
        "https://huggingface.co/docs/datasets-server/first_rows",
        "Preview a dataset",
        "The dataset viewer provides a /first-rows endpoint for visualizing the first 100 rows of a dataset. This\u2019ll give you a good idea of the data types and example data contained in a dataset."
    ],
    [
        "https://huggingface.co/docs/datasets-server/rows",
        "Download slices of rows",
        "The dataset viewer provides a /rows endpoint for visualizing any slice of rows of a dataset. This will let you walk-through and inspect the data contained in a dataset."
    ],
    [
        "https://huggingface.co/docs/datasets-server/search",
        "Search text in a dataset",
        "The dataset viewer provides a /search endpoint for searching words in a dataset."
    ],
    [
        "https://huggingface.co/docs/datasets-server/filter",
        "Filter rows in a dataset",
        "The dataset viewer provides a /filter endpoint for filtering rows in a dataset."
    ],
    [
        "https://huggingface.co/docs/datasets-server/parquet",
        "List Parquet files",
        "Datasets can be published in any format (CSV, JSONL, directories of images, etc.) to the Hub, and they are easily accessed with the \ud83e\udd17 Datasets library. For a more performant experience (especially when it comes to large datasets), the dataset viewer automatically converts every dataset to the Parquet format."
    ],
    [
        "https://huggingface.co/docs/datasets-server/size",
        "Get the number of rows and the bytes size",
        "This guide shows you how to use the dataset viewer\u2019s /size endpoint to retrieve a dataset\u2019s size programmatically. Feel free to also try it out with ReDoc."
    ],
    [
        "https://huggingface.co/docs/datasets-server/statistics",
        "Explore dataset statistics",
        "The dataset viewer provides a /statistics endpoint for fetching some basic statistics precomputed for a requested dataset. This will get you a quick insight on how the data is distributed."
    ],
    [
        "https://huggingface.co/docs/datasets-server/parquet_process",
        "Overview",
        "The dataset viewer automatically converts and publishes public datasets less than 5GB on the Hub as Parquet files. If the dataset is already in Parquet format, it will be published as is. Parquet files are column-based and they shine when you\u2019re working with big data."
    ],
    [
        "https://huggingface.co/docs/datasets-server/clickhouse",
        "ClickHouse",
        "ClickHouse is a fast and efficient column-oriented database for analytical workloads, making it easy to analyze Hub-hosted datasets with SQL. To get started quickly, use clickhouse-local to run SQL queries from the command line and avoid the need to fully install ClickHouse."
    ],
    [
        "https://huggingface.co/docs/datasets-server/duckdb",
        "DuckDB",
        "DuckDB is a database that supports reading and querying Parquet files really fast. Begin by creating a connection to DuckDB, and then install and load the httpfs extension to read and write remote files:"
    ],
    [
        "https://huggingface.co/docs/datasets-server/pandas",
        "Pandas",
        "Pandas is a popular DataFrame library for data analysis."
    ],
    [
        "https://huggingface.co/docs/datasets-server/polars",
        "Polars",
        "Polars is a fast DataFrame library written in Rust with Arrow as its foundation."
    ],
    [
        "https://huggingface.co/docs/datasets-server/configs_and_splits",
        "Splits and configurations",
        "Machine learning datasets are commonly organized in splits and they may also have configurations. These internal structures provide the scaffolding for building out a dataset, and determines how a dataset should be split and organized. Understanding a dataset\u2019s structure can help you create your own dataset, and know which subset of data you should use when during model training and evaluation."
    ],
    [
        "https://huggingface.co/docs/datasets-server/data_types",
        "Data types",
        "Datasets supported by the dataset viewer have a tabular format, meaning a data point is represented in a row and its features are contained in columns. Using the /first-rows endpoint allows you to preview the first 100 rows of a dataset and information about each feature. Within the features key, you\u2019ll notice it returns a _type field. This value describes the data type of the column, and it is also known as a dataset\u2019s Features."
    ],
    [
        "https://huggingface.co/docs/datasets-server/server",
        "Server infrastructure",
        "The dataset viewer has two main components that work together to return queries about a dataset instantly:"
    ],
    [
        "https://huggingface.co/docs/trl/index",
        "TRL",
        "TRL is a full stack library where we provide a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. The library is integrated with \ud83e\udd17 transformers."
    ],
    [
        "https://huggingface.co/docs/trl/quickstart",
        "Quickstart",
        "Fine-tuning a language model via PPO consists of roughly three steps:"
    ],
    [
        "https://huggingface.co/docs/trl/installation",
        "Installation",
        "You can install TRL either from pypi or from source:"
    ],
    [
        "https://huggingface.co/docs/trl/how_to_train",
        "PPO Training FAQ",
        "When performing classical supervised fine-tuning of language models, the loss (especially the validation loss) serves as a good indicator of the training progress. However, in Reinforcement Learning (RL), the loss becomes less informative about the model\u2019s performance, and its value may fluctuate while the actual performance improves."
    ],
    [
        "https://huggingface.co/docs/trl/use_model",
        "Use Trained Models",
        "Once you have trained a model using either the SFTTrainer, PPOTrainer, or DPOTrainer, you will have a fine-tuned model that can be used for text generation. In this section, we\u2019ll walk through the process of loading the fine-tuned model and generating text. If you need to run an inference server with the trained model, you can explore libraries such as text-generation-inference."
    ],
    [
        "https://huggingface.co/docs/trl/customization",
        "Customize the Training",
        "TRL is designed with modularity in mind so that users to be able to efficiently customize the training loop for their needs. Below are some examples on how you can apply and test different techniques."
    ],
    [
        "https://huggingface.co/docs/trl/logging",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/models",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/reward_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/sft_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/ppo_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/best_of_n",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/dpo_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/ddpo_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/iterative_sft_trainer",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/text_environments",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/example_overview",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/sentiment_tuning",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/lora_tuning_peft",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/detoxifying_a_lm",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/using_llama_models",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/learning_tools",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/trl/multi_adapter_rl",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/sagemaker/index",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/sagemaker/getting-started",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/sagemaker/train",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/sagemaker/inference",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/sagemaker/reference",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/index",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/quickstart",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/installation",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/feature_extraction",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/training_script",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/hf_hub",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/reference/models",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/reference/data",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/reference/optimizers",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/timm/reference/schedulers",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/index",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/speed",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/torch_shared_tensors",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/metadata_parsing",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/convert-weights",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/api/torch",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/api/tensorflow",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/api/paddle",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/api/flax",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/safetensors/api/numpy",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/index",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/quicktour",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/installation",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/supported_models",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/messages_api",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/consuming_tgi",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/preparing_model",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/gated_model_access",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/using_cli",
        "Understanding Logs",
        "As reinforcement learning algorithms are historically challenging to debug, it\u2019s important to pay careful attention to logging. By default, the TRL PPOTrainer saves a lot of relevant information to wandb or tensorboard."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher",
        "All TGI CLI options",
        ""
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/basic_tutorials/non_core_models",
        "Non-core Model Serving",
        "TGI supports various LLM architectures (see full list here). If you wish to serve a model that is not one of the supported models, TGI will fallback to the transformers implementation of that model. This means you will be unable to use some of the features introduced by TGI, such as tensor-parallel sharding or flash attention. However, you can still get many benefits of TGI, such as continuous batching or streaming outputs."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/streaming",
        "Streaming",
        "Token streaming is the mode in which the server returns the tokens one by one as the model generates them. This enables showing progressive generations to the user rather than waiting for the whole generation. Streaming is an essential aspect of the end-user experience as it reduces latency, one of the most critical aspects of a smooth experience."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/quantization",
        "Quantization",
        "TGI offers GPTQ and bits-and-bytes quantization to quantize large language models."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/tensor_parallelism",
        "Tensor Parallelism",
        "Tensor parallelism is a technique used to fit a large model in multiple GPUs. For example, when multiplying the input tensors with the first weight tensor, the matrix multiplication is equivalent to splitting the weight tensor column-wise, multiplying each column with the input separately, and then concatenating the separate outputs. These outputs are then transferred from the GPUs and concatenated together to get the final result, like below \ud83d\udc47"
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/paged_attention",
        "PagedAttention",
        "LLMs struggle with memory limitations during generation. In the decoding part of generation, all the attention keys and values generated for previous tokens are stored in GPU memory for reuse. This is called KV cache, and it may take up a large amount of memory for large models and long sequences."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/safetensors",
        "Safetensors",
        "Safetensors is a model serialization format for deep learning models. It is faster and safer compared to other serialization formats like pickle (which is used under the hood in many deep learning libraries)."
    ],
    [
        "https://huggingface.co/docs/text-generation-inference/conceptual/flash_attention",
        "Flash Attention",
        "Scaling the transformer architecture is heavily bottlenecked by the self-attention mechanism, which has quadratic time and memory complexity. Recent developments in accelerator hardware mainly focus on enhancing compute capacities and not memory and transferring data between hardware. This results in attention operation having a memory bottleneck. Flash Attention is an attention algorithm used to reduce this problem and scale transformer-based models more efficiently, enabling faster training and inference."
    ],
    [
        "https://huggingface.co/docs/autotrain/index",
        "\ud83e\udd17 AutoTrain",
        "\ud83e\udd17 AutoTrain is a no-code tool for training state-of-the-art models for Natural Language Processing (NLP) tasks, for Computer Vision (CV) tasks, and for Speech tasks and even for Tabular tasks. It is built on top of the awesome tools developed by the Hugging Face team, and it is designed to be easy to use."
    ],
    [
        "https://huggingface.co/docs/autotrain/getting_started",
        "Installation",
        "There is no installation required! AutoTrain Advanced runs on Hugging Face Spaces. All you need to do is create a new space with the AutoTrain Advanced template: https://huggingface.co/new-space?template=autotrain-projects/autotrain-advanced. Please make sure you keep the space private."
    ],
    [
        "https://huggingface.co/docs/autotrain/cost",
        "How much does it cost?",
        "AutoTrain provides you with best models which are deployable with just a few clicks. Unlike other services, we don\u2019t own your models. Once the training is done, you can download them and use them anywhere you want."
    ],
    [
        "https://huggingface.co/docs/autotrain/support",
        "Get help and support",
        "To get help and support for autotrain, there are 3 ways:"
    ],
    [
        "https://huggingface.co/docs/autotrain/text_classification",
        "Text Classification",
        "Training a text classification model with AutoTrain is super-easy! Get your data ready in proper format and then with just a few clicks, your state-of-the-art model will be ready to be used in production."
    ],
    [
        "https://huggingface.co/docs/autotrain/llm_finetuning",
        "LLM Finetuning",
        "With AutoTrain, you can easily finetune large language models (LLMs) on your own data!"
    ],
    [
        "https://huggingface.co/docs/autotrain/image_classification",
        "Image Classification",
        "Image classification is a supervised learning problem: define a set of target classes (objects to identify in images), and train a model to recognize them using labeled example photos. Using AutoTrain, its super-easy to train a state-of-the-art image classification model. Just upload a set of images, and AutoTrain will automatically train a model to classify them."
    ],
    [
        "https://huggingface.co/docs/autotrain/dreambooth",
        "DreamBooth",
        "DreamBooth is a method to personalize text-to-image models like Stable Diffusion given just a few (3-5) images of a subject. It allows the model to generate contextualized images of the subject in different scenes, poses, and views."
    ],
    [
        "https://huggingface.co/docs/autotrain/seq2seq",
        "Seq2Seq",
        "Seq2Seq is a task that involves converting a sequence of words into another sequence of words. It is used in machine translation, text summarization, and question answering."
    ],
    [
        "https://huggingface.co/docs/autotrain/token_classification",
        "Token Classification",
        "Token classification is the task of classifying each token in a sequence. This can be used for Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and more. Get your data ready in proper format and then with just a few clicks, your state-of-the-art model will be ready to be used in production."
    ],
    [
        "https://huggingface.co/docs/autotrain/tabular",
        "Tabular",
        "Using AutoTrain, you can train a model to classify or regress tabular data easily. All you need to do is select from a list of models and upload your dataset. Parameter tuning is done automatically."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/index",
        "Text Embeddings Inference",
        "Text Embeddings Inference (TEI) is a comprehensive toolkit designed for efficient deployment and serving of open source text embeddings models. It enables high-performance extraction for the most popular models, including FlagEmbedding, Ember, GTE, and E5."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/quick_tour",
        "Quick Tour",
        "The easiest way to get started with TEI is to use one of the official Docker containers (see Supported models and hardware to choose the right container)."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/supported_models",
        "Supported models and hardware",
        "We are continually expanding our support for other model types and plan to include them in future updates."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/local_cpu",
        "Using TEI locally with CPU",
        "You can install text-embeddings-inference locally to run it on your own machine. Here are the step-by-step instructions for installation:"
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/local_metal",
        "Using TEI locally with Metal",
        "You can install text-embeddings-inference locally to run it on your own Mac with Metal support. Here are the step-by-step instructions for installation:"
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/local_gpu",
        "Using TEI locally with GPU",
        "You can install text-embeddings-inference locally to run it on your own machine with a GPU. To make sure that your hardware is supported, check out the Supported models and hardware page."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/private_models",
        "Serving private and gated models",
        "If the model you wish to serve is behind gated access or resides in a private model repository on Hugging Face Hub, you will need to have access to the model to serve it."
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/custom_container",
        "Build custom container for TEI",
        "You can build our own CPU or CUDA TEI container using Docker. To build a CPU container, run the following command in the directory containing your custom Dockerfile:"
    ],
    [
        "https://huggingface.co/docs/text-embeddings-inference/cli_arguments",
        "CLI arguments",
        "To see all options to serve your models, run the following:"
    ],
    [
        "https://huggingface.co/docs/competitions/index",
        "\ud83e\udd17 Competitions",
        "Create a machine learning competition for your organization, friends or the world!"
    ],
    [
        "https://huggingface.co/docs/competitions/pricing",
        "Pricing",
        "Creating a competition is free. However, you will need to pay for the compute resources used to run the competition. The cost of the compute resources depends the type of competition you create."
    ],
    [
        "https://huggingface.co/docs/competitions/create_competition",
        "Create competition",
        "Creating a competition is super easy and you have full control over the data, evaluation metric and the hardware used."
    ],
    [
        "https://huggingface.co/docs/competitions/competition_repo",
        "Competition repo",
        "NOTE: Competition repo must always be kept private. Do NOT make it public!"
    ],
    [
        "https://huggingface.co/docs/competitions/competition_space",
        "Competition space",
        "A competition space is a Hugging Face Space where the actual competition takes place. It is a space where you can submit your model and get a score. It is also a space where competitors can see the leaderboard, discuss, and make submissions."
    ],
    [
        "https://huggingface.co/docs/competitions/custom_metric",
        "Custom metric",
        "In case you don\u2019t settle for the default scikit-learn metrics, you can define your own metric."
    ],
    [
        "https://huggingface.co/docs/competitions/submit",
        "Making a submission",
        "The submission format and example submissions are usually provided by the competition organizer. This page describes how to make submissions using the competition UI."
    ],
    [
        "https://huggingface.co/docs/competitions/leaderboard",
        "Leaderboard",
        "There are two types of leaderboards for all competitions:"
    ],
    [
        "https://huggingface.co/docs/competitions/teams",
        "Teams",
        "Coming soon!"
    ]
]